# MR 操作

- 通过HBase的相关JavaAPI可以实现伴随HBase操作的MapReduce过程
  - 使用MapReduce将数据从本地文件系统导入到HBase的表中
  - 从HBase中读取一些原始数据后使用MapReduce做数据分析



## 环境配置

- 查看HBase的MapReduce任务的执行
  - MR如果要执行HBase中的相关命令，需要如下的jar包配置

```bash
[ttshe@hadoop102 hbase]$ bin/hbase mapredcp

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/module/hbase/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/module/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
/opt/module/hbase/lib/zookeeper-3.4.6.jar:/opt/module/hbase/lib/netty-all-4.0.23.Final.jar:/opt/module/hbase/lib/hbase-client-1.3.1.jar:/opt/module/hbase/lib/metrics-core-2.2.0.jar:/opt/module/hbase/lib/hbase-prefix-tree-1.3.1.jar:/opt/module/hbase/lib/hbase-common-1.3.1.jar:/opt/module/hbase/lib/protobuf-java-2.5.0.jar:/opt/module/hbase/lib/guava-12.0.1.jar:/opt/module/hbase/lib/htrace-core-3.1.0-incubating.jar:/opt/module/hbase/lib/hbase-protocol-1.3.1.jar:/opt/module/hbase/lib/hbase-hadoop-compat-1.3.1.jar:/opt/module/hbase/lib/hbase-server-1.3.1.jar
```



### 临时配置

- 执行环境变量的导入
- 临时生效，在命令行执行下述操作
- 在执行MR命令前，先键入如下命令，此次生效

```bash
$ export HBASE_HOME=/opt/module/hbase
$ export HADOOP_HOME=/opt/module/hadoop-2.7.2
$ export HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase mapredcp`
```



### 永久配置

- 修改/etc/profile，添加如下配置
  - 修改完成后注意执行`source  /etc/profile`

```bash
export HBASE_HOME=/opt/module/hbase
export HADOOP_HOME=/opt/module/hadoop-2.7.2
```

- 修改`hadoop-env.sh`添加如下配置
  - 注意在for循环之后
  - 同步一下xsync

```bash
[ttshe@hadoop102 hadoop]$ pwd
/opt/module/hadoop-2.7.2/etc/hadoop
[ttshe@hadoop102 hadoop]$ vim hadoop-env.sh 

export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/module/hbase/lib/*
```



## 示例



### 官方示例



**统计Student表中有多少行数据**

```bash
[ttshe@hadoop102 hadoop]$ /opt/module/hadoop-2.7.2/bin/yarn jar /opt/module/hbase/lib/hbase-server-1.3.1.jar rowcounter student
# 两个命令效果一样
[ttshe@hadoop102 hadoop]$ hadoop jar /opt/module/hbase/lib/hbase-server-1.3.1.jar rowcounter student
...
org.apache.hadoop.hbase.mapreduce.RowCounter$RowCounterMapper$Counters
ROWS=1
...
```



**使用MapReduce将本地数据导入到HBase**

- 在本地创建一个tsv格式的文件
  - csv用`,` 分隔
  - tsv用`tab`分隔
- 文件格式如下（fruit.tsv）

```bash
1001	Apple	Red
1002	Pear	Yellow
1003	Pineapple	Yellow
```

- 创建HBase表

```bash
hbase(main):025:0> create 'fruit','info'
```

- 在HDFS中创建input_fruit文件夹并上传fruit.tsv文件

```bash
[ttshe@hadoop102 module]$ hadoop fs -mkdir /input_fruit/
[ttshe@hadoop102 module]$ hadoop fs -put /opt/module/datas/fruit.tsv /input_fruit/
```

- 导入到HBase
  - `Dimporttsv.columns` 指示了列的定义

```bash
[ttshe@hadoop102 module]$ hadoop jar /opt/module/hbase/lib/hbase-server-1.3.1.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color fruit hdfs://hadoop102:9000/input_fruit
```

- 查询HBase

```bash
hbase(main):027:0> scan 'fruit'
```



### 自定义MR1

- hbase数据 --> hbase数据

- 将fruit表中的一部分数据，通过MR迁入到fruit_mr表中
  - 创建fruit_mr表

```bash
create 'fruit_mr','info'
```

- 构建ReadDataMapper类，用于读取fruit表中的数

```java
package com.stt.demo.hbase.Ch02_mr01;

import org.apache.hadoop.hbase.Cell;
import org.apache.hadoop.hbase.CellUtil;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableMapper;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

public class ReadDataMapper extends TableMapper<ImmutableBytesWritable,Put> {

	@Override
	protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException {
		// 获取rowKey
		byte[] rowKey = key.get();
		// 定义Put
		Put put = new Put(rowKey);
		// 将fruit中name和color进行提取，将每一行数据提出出来放入Put中
		for (Cell cell : value.rawCells()) {
			// 判断列族是需要的info中的name和color，其他的过滤
			if("info".equals(Bytes.toString(CellUtil.cloneFamily(cell)))){
				if("name".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))){
					put.add(cell);
				}
				if("color".equals(Bytes.toString(CellUtil.cloneQualifier(cell)))){
					put.add(cell);
				}
			}
			put.add(cell);
		}
		// 将从fruit读取的数据的每一行写入到context中作为map的输出
		if(!put.isEmpty()){
			context.write(key,put);
		}
	}
}
```

- 构建WriteDataReducer类，用于将读取到的fruit表中的数据写入到fruit_mr表中

```java
package com.stt.demo.hbase.Ch02_mr01;

import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableReducer;
import org.apache.hadoop.io.NullWritable;

import java.io.IOException;

public class WriteDataReducer extends TableReducer<ImmutableBytesWritable,Put,NullWritable> {

	@Override
	protected void reduce(ImmutableBytesWritable key, Iterable<Put> values, Context context) throws IOException, InterruptedException {
		// 读出来的每一行数据写入到fruit_mr表中
		for (Put value : values) {
			context.write(NullWritable.get(),value);
		}
	}
}
```

- 构建DriverTool用于组装运行Job任务

```java
package com.stt.demo.hbase.Ch02_mr01;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class DriverTool extends Configuration implements Tool{

	private Configuration conf;

	@Override
	public int run(String[] args) throws Exception {
		Job job = Job.getInstance(getConf());
		// 设置当前jar包通过那个类读取
		job.setJarByClass(DriverTool.class);
		// 至少1个
		job.setNumReduceTasks(1);
		// 配置Scan
		Scan scan = new Scan().setCacheBlocks(false).setCaching(500);
		// 设置Mapper，注意导入的是mapreduce包下的，不是mapred包下的，后者是老版本
		TableMapReduceUtil.initTableMapperJob(
				"fruit",// 数据源表名
				scan,
				ReadDataMapper.class,
				ImmutableBytesWritable.class, // Mapper的输出Key
				Put.class,// Mapper的输出value
				job
		);
		// 设置reducer
		TableMapReduceUtil.initTableReducerJob(
				"fruit_mr",// 目标的表名
				WriteDataReducer.class,
				job
		);
		return job.waitForCompletion(true) ? 0 : 1;
	}
	@Override
	public void setConf(Configuration conf) {
		this.conf = conf;
	}
	@Override
	public Configuration getConf() {
		return this.conf;
	}
	public static void main(String[] args) throws Exception {
		System.exit(ToolRunner.run(new DriverTool(), args));
	}
}
```

- 打包运行任务
- 注意：将jar依赖的其他jar包可以让hadoop识别到，依赖的jar可以和执行的jar包放在一个文件夹下
  - maven打包
    - 命令：-P local clean package或-P dev clean package install
    - 将第三方jar包一同打包，需要插件：maven-shade-plugin
  - 也可以使用如下方式打包
    - 然后点击ok
    - 点击build--> build artifacts
    <img src="img/hbase/24.png" alt="1569332280275" style="zoom:100%;" />
- 执行
```bash
[ttshe@hadoop102 module]$ hadoop jar /opt/module/datas/big-data-0.0.1.jar com.stt.demo.hbase.Ch02_mr01.DriverTool
```



### 自定义MR2

- hdfs 数据->hbase 数据

- hdfs的数据是tsv，使用`tab` 进行分割

```txt
1001	Apple	Red
1002	Pear	Yellow
1003	Pineapple	Yellow
```

- mapper

```java
package com.stt.demo.hbase.Ch02_mr02;

import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;

public class ReadDataMapper extends Mapper<LongWritable,Text,ImmutableBytesWritable,Put> {
	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

		String[] split = value.toString().split("\t");
		byte[] rowKeyByte=Bytes.toBytes(split[0]);
		byte[] family = Bytes.toBytes("info");
		byte[] name = Bytes.toBytes("name");
		byte[] color = Bytes.toBytes("color");
		byte[] nameVal = Bytes.toBytes(split[1]);
		byte[] colorVal = Bytes.toBytes(split[2]);
		Put put = new Put(rowKeyByte)
				.addColumn(family,name,nameVal)
				.addColumn(family,color,colorVal);

		ImmutableBytesWritable rowKey = new ImmutableBytesWritable(rowKeyByte);
		context.write(rowKey,put);
	}
}
```

- reducer

```java
package com.stt.demo.hbase.Ch02_mr02;

import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableReducer;
import org.apache.hadoop.io.NullWritable;
import java.io.IOException;

public class WriteDataReducer extends TableReducer<ImmutableBytesWritable,Put,NullWritable> {

	@Override
	protected void reduce(ImmutableBytesWritable key, Iterable<Put> values, Context context) throws IOException, InterruptedException {
		// 读出来的每一行数据写入到fruit_mr表中
		for (Put value : values) {
			context.write(NullWritable.get(),value);
		}
	}
}
```

- driver

```java
package com.stt.demo.hbase.Ch02_mr02;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import java.io.IOException;

public class DriverTool extends Configuration implements Tool{

	private Configuration conf;

	@Override
	public int run(String[] args) throws Exception {
		getConf().set("inputpath",args[0]);

		Job job = Job.getInstance(getConf());
		// 设置当前jar包通过那个类读取
		job.setJarByClass(DriverTool.class);
		// 至少1个
		job.setNumReduceTasks(1);

		// 设置Mapper
		job.setMapperClass(ReadDataMapper.class);
		job.setMapOutputKeyClass(ImmutableBytesWritable.class);
		job.setMapOutputValueClass(Put.class);

		// 设置reducer
		TableMapReduceUtil.initTableReducerJob(
				"fruit_mr",// 目标的表名
				WriteDataReducer.class,
				job
		);

		initJobInputPath(job);

		return job.waitForCompletion(true) ? 0 : 1;
	}

	private void initJobInputPath(Job job) throws IOException {

		Configuration conf = job.getConfiguration();

		// 对输入路径进行判断是否存在，不存在抛出异常
		FileSystem fs = FileSystem.get(conf);
		String inputPathStr = conf.get("inputpath");
		Path in = new Path(inputPathStr);
		if(!fs.exists(in)){
			throw new RuntimeException(inputPathStr + " is not exists");
		}
		FileInputFormat.addInputPath(job,in);
	}

	@Override
	public void setConf(Configuration conf) {
		this.conf = conf;
	}

	@Override
	public Configuration getConf() {
		return this.conf;
	}

	public static void main(String[] args) throws Exception {
		System.exit(ToolRunner.run(new DriverTool(), args));
	}
}
```

- 打包执行

```bash
[ttshe@hadoop102 module]$ hadoop jar /opt/module/datas/big-data-0.0.4.jar com.stt.demo.hbase.Ch02_mr02.DriverTool /input_fruit/
```


