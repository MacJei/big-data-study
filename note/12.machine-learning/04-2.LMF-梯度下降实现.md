# 隐语义模型算法

- 使用梯度下降算法实现



### 0.引入依赖


```python
import numpy as np
import pandas as pd
```



### 1.数据准备


```python
# 评分矩阵R
R = np.array([
    [4,1,0,3,1],
    [0,2,0,3,4],
    [0,0,3,1,2],
    [1,3,2,0,0],
    [0,4,2,1,0],
    [0,2,1,0,3]
])
len(R[0])
```

- 结果


    5



### 2.算法实现


```python
"""
@输入参数：
R：M*N 的评分矩阵
K：隐特征向量维度
max_iter: 最大迭代次数
alpha：步长
lamda：正则化系数

@输出：
分解之后的 P，Q
P：初始化用户特征矩阵M*K
Q：初始化物品特征矩阵N*K
"""

# 给定超参数
K = 5
max_iter = 5000
alpha = 0.0001
lamda = 0.004

# 核心算法
def LFM_grad_desc(R, K=3, max_iter=1000, alpha=0.0001, lamda=0.002):
    # 数据维度的基本定义
    M = len(R)
    N = len(R[0])
    
    # P 和 Q 的初始值随机生成
    P = np.random.rand(M, K)
    Q = np.random.rand(N, K)
    # 进行转置处理
    Q = Q.T
    
    # 开始迭代
    for step in range(max_iter):
        # 对所有的用户u和物品i做遍历，对应的特征向量Pu,Qi梯度下降
        for u in range(M):
            for i in range(N):
                # 对每一个大于0的评分进行误差计算
                if R[u,i] > 0 :
                    eui = np.dot( P[u,:], Q[:,i]) - R[u, i]
                    
                    # 按照梯度下降算法更新当前的Pu、Qi
                    for k in range(K):
                        P[u, k] = P[u, k] - alpha * ( 2 * eui * Q[k, i] + 2 * lamda * P[u, k] )
                        Q[k, i] = Q[k, i] - alpha * ( 2 * eui * P[u, k] + 2 * lamda * Q[k, i] )
      
        # u、i遍历完成，所有特征向量更新完成，可以得到P、Q，可以计算预测评分矩阵                 
        predR = np.dot(P, Q)
        
        # 计算当前损失函数
        cost = 0
        for u in range(M):
            for i in range(N):
                if(R[u, i] > 0):
                    cost += ( predR[u, i] - R[u, i] ) ** 2
                    # 添加正则化项
                    for k in range(K):
                        cost += lamda * (P[u, k]** 2 + Q[k, i]**2)
                        
        if cost < 0.0001:
            break
            
    return P, Q.T, cost
```



### 3.测试


```python
P, Q, cost = LFM_grad_desc(R, K, max_iter, alpha, lamda)
print(P)
print(Q)
print(cost)

print(np.dot(P, Q.T))
R
```

    [[-0.32223581  0.06450348  1.46748954  0.20438143  1.34481943]
     [ 0.32366014  0.33251052  0.09093489  1.20206563  1.60648994]
     [ 0.30891657  0.63555631  1.11343943  0.62339112  0.48529768]
     [ 0.89990956  0.34811273  0.33667334  1.09764357  0.22801856]
     [ 1.11694996  1.20610307  0.82140467  0.57060924  0.1940241 ]
     [ 0.9801891   0.81078185 -0.11056573  0.09129137  0.84082803]]
    [[ 0.03301004  0.71636333  1.50131182  0.01439379  1.31483941]
     [ 1.26065794  1.00199261  0.74927022  1.04710261  0.02917902]
     [ 0.25740849  0.26507099  1.17563388  1.01352453  0.70014819]
     [-0.02107654 -0.16885733  0.50068694  0.75537264  1.31001222]
     [ 1.14323914  0.89013708 -0.20725389  1.1789184   1.13844997]]
    1.3946008527702858
    [[4.0098937  1.01119792 2.80810066 2.64676659 1.15684009]
     [2.51498258 2.11489448 2.62146193 2.99509038 3.89319999]
     [2.78416783 2.52744282 2.52858488 1.55029383 1.97554749]
     [1.10013995 2.89154262 1.991859   1.2186566  2.82252294]
     [2.39738688 3.8352001  2.28705858 0.86926235 3.07388712]
     [1.55404468 2.08536294 1.01846919 0.95752972 2.93007842]]
     
     array([[4, 1, 0, 3, 1],
           [0, 2, 0, 3, 4],
           [0, 0, 3, 1, 2],
           [1, 3, 2, 0, 0],
           [0, 4, 2, 1, 0],
           [0, 2, 1, 0, 3]])
