## 什么是序列化

- 序列化：将内存中的对象转换为==字节序列==（或其他数据传输协议，如json），以便存储到磁盘或者传输

- 反序列化：将收到的字节序列（或者其他格式）转换为==内存中的对象==

  

## 为什么要序列化

- 一般对象存储在内存中，一旦断电就没有了，同时该对象只能本地进程使用，不能通过网络发送给另一个进程使用

- 序列化之后，该对象可以用于传输给另一进程使用

  

## 为什么不使用Java的序列化

- java序列化是一个重量级序列化框架（Serializable），一个对象序列化之后会附带很多额外的信息，各种校验信息，header，继承体系，不便于在网络中传输，Hadoop自己开发了一套序列化机制（Writable）
- Hadoop序列化的特点
  - 紧凑：高效的使用存储空间
  - 快速：读写数据的额外开销小
  - 可扩展：随着通信协议的升级而升级
  - 互操作：支持多语言的交互



## 自定义bean序列化

### 如何编写

当基本类型的Writable对象不能满足需求时，需要自定义序列化操作，如在Hadoop内部传输一个bean对象

- 实现Writable接口
- 反序列化时，需要反射调用空参构造函数，必须要有一个空参构造器

```java
public FlowBean(){super();}
```

- 重写序列化方法

```java
@Override
public void write(DataOutput out) throws IOException {
	out.writeLong(upFlow);
	out.writeLong(downFlow);
	out.writeLong(sumFlow);
}
```

- 重写反序列化方法

```java
@Override
public void readFields(DataInput in) throws IOException {
	upFlow = in.readLong();
	downFlow = in.readLong();
	sumFlow = in.readLong();
}
```

- 注意：==反序列化的顺序和序列化的顺序完全一致==
- 可以重写toString方法，用于显示在日志文件中等
- 如果需要将自定义bean放在**Key中传输**，==需要实现 Comparable 接口==
  - MapReduce框中的==Shuffle过程要求对Key必须能排序==

```java
@Override
public int compareTo(FlowBean o) {
	// 倒序排列，从大到小
	return this.sumFlow > o.getSumFlow() ? -1 : 1;
}
```



#### 示例

- 需求：统计一个手机号耗费的总上行流量，下行流量，总流量
- 输入数据

```text
1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200
2	13846544121	192.196.100.2			264	0	200
3 	13956435636	192.196.100.3			132	1512	200
4 	13966251146	192.168.100.1			240	0	404
5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200
6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200
7 	13590439668	192.168.100.4			1116	954	200
8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200
9 	13729199489	192.168.100.6			240	0	200
10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200
11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200
12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500
13 	13560439638	192.168.100.10			918	4938	200
14 	13470253144	192.168.100.11			180	180	200
15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200
16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200
17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404
18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200
19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200
20 	13768778790	192.168.100.17			120	120	200
21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200
22 	13568436656	192.168.100.19			1116	954	200
```

- 输入数据格式

```xml
7 	13560436666	120.196.100.99		1116		 954			200
id	手机号码		网络ip			上行流量     下行流量     网络状态码
```

- 期望输出

```xml
13560436666 		1116		  954 			2070
手机号码		    上行流量        下行流量		总流量
```

- 分析

  - 需求：统计每个号码的总上行量，总下行量，总流量
  - 输入数据格式，第二列是号码，从倒数第三列依次倒数是上行量，下行量
  - map阶段：切分字段，读取上行量，下行量
    - 以号码为key，bean对象输出为value，bean要求可以被序列化传输
  - 期望输出的格式，该格式由bean的toString方法决定
  - reduce阶段，累加上行量和下行量，计算得出总行量。

  

  

##### 序列化bean

  ```java
  package com.stt.demo.mr.serialization;
  import lombok.Data;
  import org.apache.hadoop.io.Writable;
  
  import java.io.DataInput;
  import java.io.DataOutput;
  import java.io.IOException;
  
  /**
   * 用于统计流量的bean
   */
  @Data
  public class FlowBean implements Writable{
  
  	private long upFlow;
  	private long downFlow;
  	private long sumFlow;
  
  	// 反序列化时，需要反射调用空参构造函数
  	public FlowBean(){
  		super();
  	}
  
  	public FlowBean(long upFlow,long downFlow){
  		this.upFlow = upFlow;
  		this.downFlow = downFlow;
  		this.sumFlow = upFlow + downFlow;
  	}
  
  	// 写序列化方法
  	@Override
  	public void write(DataOutput out) throws IOException {
  		out.writeLong(upFlow);
  		out.writeLong(downFlow);
  		out.writeLong(sumFlow);
  	}
  
  	// 反序列化方法
  	@Override
  	public void readFields(DataInput in) throws IOException {
  		// 反序列化方法必须要和序列化方法的执行顺序保持一致
  		this.upFlow = in.readLong();
  		this.downFlow = in.readLong();
  		this.sumFlow = in.readLong();
  	}
  }
  ```



##### mapper

  ```java
  package com.stt.demo.mr.serialization;
  
  import org.apache.hadoop.io.LongWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Mapper;
  
  import java.io.IOException;
  
  public class FlowCountMapper extends Mapper<LongWritable,Text,Text,FlowBean> {
  
  	FlowBean v = new FlowBean();
  	Text k = new Text();
  
  	@Override
  	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
  		// 获取一行数据
  		String line = value.toString();
  		// 切割字段
  		String[] fields = line.split("\t");
  		// 封装对象
  		String phoneNum = fields[1];
  		// 取得上流量和下流量
  		int len = fields.length;
  		long upFlow = Long.parseLong(fields[len - 3]);
  		long downFlow = Long.parseLong(fields[len - 2]);
  
  		k.set(phoneNum);
  		v.setDownFlow(downFlow);
  		v.setUpFlow(upFlow);
  
  		// 写出
  		context.write(k,v);
  	}
  }
  ```



##### reducer

  ```java
  package com.stt.demo.mr.serialization;
  
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Reducer;
  
  import java.io.IOException;
  
  public class FlowCountReducer extends Reducer<Text,FlowBean,Text,FlowBean> {
  	@Override
  	protected void reduce(Text key, Iterable<FlowBean> values, Context context) throws IOException, InterruptedException {
  
  		long sum_upFlow = 0;
  		long sum_downFlow = 0;
  		for(FlowBean flowBean:values){
  			sum_downFlow += flowBean.getDownFlow();
  			sum_upFlow += flowBean.getUpFlow();
  		}
  
  		FlowBean result = new FlowBean(sum_upFlow,sum_downFlow);
  
  		context.write(key,result);
  	}
  }
  ```



##### driver

  ```java
  package com.stt.demo.mr.serialization;
  
  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Job;
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  import java.io.IOException;
  
  public class FlowCountDriver {
  
  	public static void main(String[] args) throws Exception{
  		// 设置输入输出参数
  		args = new String[]{"d:/input.txt","d:/output.txt"};
  		// 配置信息以及job对象
  		Configuration conf = new Configuration();
  		Job job = Job.getInstance(conf);
  		job.setJarByClass(FlowCountDriver.class);
  
  		job.setMapperClass(FlowCountMapper.class);
  		job.setReducerClass(FlowCountReducer.class);
  
  		job.setMapOutputKeyClass(Text.class);
  		job.setMapOutputValueClass(FlowBean.class);
  
  		job.setOutputKeyClass(Text.class);
  		job.setOutputValueClass(FlowBean.class);
  
  		FileInputFormat.setInputPaths(job,new Path(args[0]));
  		FileOutputFormat.setOutputPath(job,new Path(args[1]));
  
  		boolean re = job.waitForCompletion(true);
  		System.exit(re ? 0 : 1);
  	}
  }
  ```

