# OutputFormat 数据输出

- OutputFormat 是MapReduce 输出的基类
- 在reduce阶段执行之后进行



## 类型



### TextOutputFormat

- 文本输出
- MapReduce的默认输出
- 每条记录写为文本
- 键值是任意类型，因为调用`toString()`方法转换为字符串



### SequenceFileOutputFormat

- 作为后续MapReduce的任务输入
- 多job串联使用
- 格式紧凑，容易压缩



### 自定义OutputFormat

- 依据需求，自定义实现输出
  - 输出到不同文件夹
  - 输出到mysql，hbase等
    - 可以先输出到mq，然后批量插入mysql



# 自定义OutputFormat

- 自定义一个类继承FileOutputFormat
- 改写RecordWriter
  - 具体改写输出数据的write方法



## 示例

- 需求：过滤输入的log日志，包含atguigu的网站输出到e:/atguigu.log，不包含atguigu的网站输出到e:/other.log
- 数据

```text
http://www.baidu.com
http://www.google.com
http://cn.bing.com
http://www.atguigu.com
http://www.sohu.com
http://www.sina.com
http://www.sin2a.com
http://www.sin2desa.com
http://www.sindsafa.com
```

- 期望输出2个文件，一个atguigu.log，一个other.log

实现

- mapper

```java
public class FilterMapper extends Mapper<LongWritable,Text,Text,NullWritable> {
	
	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		context.write(value,NullWritable.get());
	}
}
```

- reducer

```java
public class FilterReducer extends Reducer<Text,NullWritable,Text,NullWritable> {
    @Override
    protected void reduce(Text key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {
        // 针对相同的key也进行输出
        for(NullWritable val : values){
            key.set(key.toString()+"\r\n");
            context.write(key,NullWritable.get());
        }
    }
}
```

- outputFormat

```java
package com.stt.demo.mr.Ch11_CustomizedOutputFormat;

import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class FilterOutputFormat extends FileOutputFormat<Text,NullWritable> {

	@Override
	public RecordWriter<Text, NullWritable> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException {
		return new FilterRecordWriter(context);
	}
}
```

- recordWriter

```java
package com.stt.demo.mr.Ch11_CustomizedOutputFormat;

import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;

import java.io.IOException;

import static com.stt.demo.mr.Constant.OUTPUT_PATH_PREFIX;

public class FilterRecordWriter extends RecordWriter<Text, NullWritable> {

	private FSDataOutputStream output1 = null;
	private FSDataOutputStream output2 = null;

	public FilterRecordWriter(TaskAttemptContext context) {
		try{
			FileSystem fs = FileSystem.get(context.getConfiguration());
			output1 = fs.create(new Path(OUTPUT_PATH_PREFIX+"ch11/output/atguigu.log"));
			output2 = fs.create(new Path(OUTPUT_PATH_PREFIX+"ch11/output/other.log"));
		}catch (Exception e){
			e.printStackTrace();
		}
	}

	@Override
	public void write(Text key, NullWritable value) throws IOException, InterruptedException {
		FSDataOutputStream output = key.toString().contains("atguigu") ? output1 : output2;
		output.write(key.toString().getBytes());
	}

	@Override
	public void close(TaskAttemptContext context) throws IOException, InterruptedException {
		IOUtils.closeStream(output1);
		IOUtils.closeStream(output2);
	}
}
```

- driver

```java
package com.stt.demo.mr.Ch11_CustomizedOutputFormat;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

import static com.stt.demo.mr.Constant.INPUT_PATH_PREFIX;
import static com.stt.demo.mr.Constant.OUTPUT_PATH_PREFIX;

public class FilterDriver {

	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

		args = new String[]{INPUT_PATH_PREFIX+"ch11/input.txt", OUTPUT_PATH_PREFIX+"ch11/output"};

		Job job = Job.getInstance(new Configuration());

		job.setJarByClass(FilterDriver.class);

		job.setMapperClass(FilterMapper.class);
		job.setReducerClass(FilterReducer.class);

		// 添加自定义的outputFormat
		job.setOutputFormatClass(FilterOutputFormat.class);

		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(NullWritable.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(NullWritable.class);

		FileInputFormat.setInputPaths(job,new Path(args[0]));
		// 虽然我们自定义了outputformat，但是因为自定义outputformat继承自fileoutputformat
		// 而fileoutputformat要输出一个_SUCCESS文件，所以，在这还得指定一个输出目录
		FileOutputFormat.setOutputPath(job,new Path(args[1]));

		boolean result = job.waitForCompletion(true);
		System.exit(result ? 0 : 1);
	}
}
```

