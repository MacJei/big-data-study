# 基本语法

```shell
# 写法1
bin/hadoop fs <具体命令>
# 写法2
bin/hdfs dfs <具体命令>
```



# 命令大全

```shell
[root@hadoop102 hadoop-2.7.2]# bin/hadoop fs
Usage: hadoop fs [generic options]
	[-appendToFile <localsrc> ... <dst>]
	[-cat [-ignoreCrc] <src> ...]
	[-checksum <src> ...]
	[-chgrp [-R] GROUP PATH...]
	[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
	[-chown [-R] [OWNER][:[GROUP]] PATH...]
	[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]
	[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
	[-count [-q] [-h] <path> ...]
	[-cp [-f] [-p | -p[topax]] <src> ... <dst>]
	[-createSnapshot <snapshotDir> [<snapshotName>]]
	[-deleteSnapshot <snapshotDir> <snapshotName>]
	[-df [-h] [<path> ...]]
	[-du [-s] [-h] <path> ...]
	[-expunge]
	[-find <path> ... <expression> ...]
	[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
	[-getfacl [-R] <path>]
	[-getfattr [-R] {-n name | -d} [-e en] <path>]
	[-getmerge [-nl] <src> <localdst>]
	[-help [cmd ...]]
	[-ls [-d] [-h] [-R] [<path> ...]]
	[-mkdir [-p] <path> ...]
	[-moveFromLocal <localsrc> ... <dst>]
	[-moveToLocal <src> <localdst>]
	[-mv <src> ... <dst>]
	[-put [-f] [-p] [-l] <localsrc> ... <dst>]
	[-renameSnapshot <snapshotDir> <oldName> <newName>]
	[-rm [-f] [-r|-R] [-skipTrash] <src> ...]
	[-rmdir [--ignore-fail-on-non-empty] <dir> ...]
	[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]
	[-setfattr {-n name [-v value] | -x name} <path>]
	[-setrep [-R] [-w] <rep> <path> ...]
	[-stat [format] <path> ...]
	[-tail [-f] <file>]
	[-test -[defsz] <path>]
	[-text [-ignoreCrc] <src> ...]
	[-touchz <path> ...]
	[-truncate [-w] <length> <path> ...]
	[-usage [cmd ...]]

Generic options supported are
-conf <configuration file>     specify an application configuration file
-D <property=value>            use value for given property
-fs <local|namenode:port>      specify a namenode
-jt <local|resourcemanager:port>    specify a ResourceManager
-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster
-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.
-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]
```



# 常用命令 `hadoop fs -*`

> 先启动Hadoop集群
> sbin/start-dfs.sh
> sbin/start-yarn.sh



## -help

```shell
# 输出这个命令参数
[ttshe@hadoop102 hadoop-2.7.2]# hadoop fs -help rm
-rm [-f] [-r|-R] [-skipTrash] <src> ... :
  Delete all files that match the specified file pattern. Equivalent to the Unix
  command "rm <src>"
                                                                                 
  -skipTrash  option bypasses trash, if enabled, and immediately deletes <src>   
  -f          If the file does not exist, do not display a diagnostic message or 
              modify the exit status to reflect an error.                        
  -[rR]       Recursively deletes directories
```



## -ls

```shell
# 显示目录信息
[ttshe@hadoop102 hadoop-2.7.2]# hadoop fs -ls /
Found 2 items
drwx------   - ttshe supergroup          0 2019-04-21 15:17 /tmp
drwxr-xr-x   - ttshe supergroup          0 2019-04-21 14:51 /user
[ttshe@hadoop102 hadoop-2.7.2]# hadoop fs -ls /user
Found 1 items
drwxr-xr-x   - ttshe supergroup          0 2019-04-21 15:17 /user/ttshe
```



## -mkdir

```shell
# 在HDFS 上创建目录
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir -p /dir01/dir02
# -p 表示如果文件已经存在则忽略创建，不报错
```



## -moveFromLocal

```shell
# 从本地剪切到HDFS
# 创建一个文件
[ttshe@hadoop102 hadoop-2.7.2]$ touch test.txt
# 将该文件从本地放入HDFS中
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -moveFromLocal ./test.txt /dir01/dir02
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /dir01/dir02
Found 1 items
-rw-r--r--   3 ttshe supergroup          0 2019-04-22 22:11 /dir01/dir02/test.txt
```



## -appendToFile

```shell
# 追加一个文件到已经存在的文件末尾
# 创建一个文件，并填写内容，再追加到test.txt中
[ttshe@hadoop102 hadoop-2.7.2]$ touch test02.txt
[ttshe@hadoop102 hadoop-2.7.2]$ vi test02.txt
[ttshe@hadoop102 hadoop-2.7.2]$ cat test02.txt 
hello hadoop hdfs
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -appendToFile test02.txt /dir01/dir02/test.txt
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /dir01/dir02/test.txt
hello hadoop hdfs
```



## -cat

```shell
# 显示文件内容
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -cat /dir01/dir02/test.txt
hello hadoop hdfs
```



## -chgrp，-chmod，-chown

```shell
# 修改文件所属权限
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /dir01/dir02
Found 1 items
-rw-r--r--   3 ttshe supergroup         18 2019-04-22 22:13 /dir01/dir02/test.txt
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -chmod 666 /dir01/dir02/test.txt
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -chown ttshe:ttshe /dir01/dir02/test.txt
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /dir01/dir02
Found 1 items
-rw-rw-rw-   3 ttshe ttshe         18 2019-04-22 22:13 /dir01/dir02/test.txt
```



## -copyFromLocal | -put

```shell
# 从本地文件系统拷贝到HDFS路径中
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -copyFromLocal README.txt /user/ttshe/
```

- put

```bash
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -put /opt/software/merge.txt /dir01/dir02/
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /dir01/dir02
Found 3 items
-rw-r--r--   3 ttshe supergroup       1366 2019-04-22 22:57 /dir01/dir02/README.txt
-rw-r--r--   3 ttshe supergroup       1384 2019-04-22 23:11 /dir01/dir02/merge.txt
-rw-rw-rw-   3 ttshe ttshe              18 2019-04-22 22:13 /dir01/dir02/test.txt
```

```bash
# 在guli下创建video文件夹，并将video的文件上传到/guli/video下
[ttshe@hadoop102 guli]$ hadoop fs -put video/ /guli
# 将本地video文件下的文件上传到/guli下
[ttshe@hadoop102 guli]$ hadoop fs -put video/* /guli
```



## -copyToLocal | -get

```shell
# 从HDFS 拷贝到本地
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -copyToLocal /dir01/dir02/test.txt /opt/software/
```

- get

```bash
# 等同于copyToLocal，从HDFS的文件下载到本地
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -get /dir01/dir02/README.txt /opt/software/
```



## -cp

```shell
# 从HDFS的一个路径拷贝到HDFS的另一个路径
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -cp /user/ttshe/README.txt /user/
```



## -mv

```shell
# 在HDFS中移动文件
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -mv /user/README.txt /dir01/dir02/
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /user
Found 1 items
drwxr-xr-x   - ttshe supergroup          0 2019-04-22 22:56 /user/ttshe
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -ls /dir01/dir02
Found 2 items
-rw-r--r--   3 ttshe supergroup       1366 2019-04-22 22:57 /dir01/dir02/README.txt
-rw-rw-rw-   3 ttshe ttshe              18 2019-04-22 22:13 /dir01/dir02/test.txt
```



## -getmerge

```shell
# 合并下载多个文件，如HDFS目录/dir01/dir02/下有多个文件，合并成一个文件输出到本地
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -getmerge /dir01/dir02/* /opt/software/merge.txt
```



## -tail

```shell
# 显示一个文件的末尾1kb的信息
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -tail -f /dir01/dir02/merge.txt
# -f 表示也显示增长的信息
```



## -rm

```shell
# 删除文件或文件夹
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -rm /dir01/dir02/merge.txt
19/04/22 23:14:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted /dir01/dir02/merge.txt
```

```bash
# 递归删除guli文件夹
[ttshe@hadoop102 guli]$ hadoop fs -rm -r /guli
```



## -rmdir

```shell
# 删除空目录，注意非空目录
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /dir01/dir02
rmdir: `/dir01/dir02': Directory is not empty
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -mkdir /test
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -rmdir /test
# -p  Do not fail if the directory already exists 
```



## -du 统计

```shell
# 统计文件夹的大小信息
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -du -s -h /dir01/dir02/
1.4 K  /dir01/dir02
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -du -h /dir01/dir02/
1.3 K  /dir01/dir02/README.txt
18     /dir01/dir02/test.txt
```

参数描述

```shell
  -s  Rather than showing the size of each individual file that matches the      
      pattern, shows the total (summary) size.                                   
  -h  Formats the sizes of files in a human-readable fashion rather than a number
      of bytes.                                                                  
  Note that, even without the -s option, this only shows size summaries one level
  deep into a directory.
```



## -setrep

> 这里设置的副本只是记录在NameNode的元数据中，真实的副本集的个数要以DataNode为准，DataNode的个数是3个，那么副本最多是3个，增加DataNode个数才会增加副本个数。

```shell
# 设置HDFS 中文件的副本数量
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop fs -setrep 10 /dir01/dir02/test.txt
Replication 10 set: /dir01/dir02/test.txt
```

![1](img/03.hdfs02.png)
