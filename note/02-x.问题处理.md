# Hadoop

- 不能通过网页查看HDFS文件系统

  解决方式：http://www.cnblogs.com/zlslch/p/6604189.html

  

- 执行hdfs命令报错：WARN util.NativeCodeLoader: Unable to load native-hadoop library

  ```shell
  [ttshe@hadoop101 hadoop-2.7.2]$ bin/hdfs dfs -mkdir -p /user/ttshe/input
  19/04/08 22:38:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
  # 原因是Linux的(GNU libc) 2.12 版本和Hadoop要求的`GLIBC_2.14' 版本不一致
  # 查看 Hadoop 异常信息
  [ttshe@hadoop101 native]$ pwd
  /opt/module/hadoop-2.7.2/lib/native
  [ttshe@hadoop101 native]$ ldd libhadoop.so.1.0.0 
  ./libhadoop.so.1.0.0: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by ./libhadoop.so.1.0.0)
  	linux-vdso.so.1 =>  (0x00007fff42ffc000)
  	libdl.so.2 => /lib64/libdl.so.2 (0x00007ffab97ee000)
  	libc.so.6 => /lib64/libc.so.6 (0x00007ffab9459000)
  	/lib64/ld-linux-x86-64.so.2 (0x000055f554143000)
  # 查看本地版本
  [ttshe@hadoop101 native]$ ldd --version
  ldd (GNU libc) 2.12
  # 可以在日志中配置忽略该错误
  [ttshe@hadoop101 hadoop]$ pwd
  /opt/module/hadoop-2.7.2/etc/hadoop
  [ttshe@hadoop101 hadoop]$ vim log4j.properties 
  log4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR
  ```

  https://blog.csdn.net/u010003835/article/details/81127984

- 伪分布式

  - 在不同的用户下开启不同的服务，如在root下开启了NodeManager，那么在ttshe用户下使用jps就看不到该节点，而在ttshe下开启的节点服务，在root使用jps可以看到

  

- 关于权限：在本示例中，所有hadoop的操作都要在一个非root的==普通账户==下进行，否则会有权限不一致导致程序出错的问题。所属组和所属主都要普通用户



- 防火墙没关闭、或者没有启动YARN

  - INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032

- 主机名称配置错误

- IP地址配置错误

- ssh没有配置好

- root用户和ttshe==两个用户==启动集群不统一

- 配置文件修改不细心

- 未编译源码

  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 17/05/22 15:38:58 INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032

- 不识别主机名称

  java.net.UnknownHostException: hadoop102: hadoop102

  ​        at java.net.InetAddress.getLocalHost(InetAddress.java:1475)

  ​        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:146)

  ​        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)

  ​        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)

  ​        at java.security.AccessController.doPrivileged(Native Method)

  ​	at javax.security.auth.Subject.doAs(Subject.java:415)

  - 解决办法：
    - 在/etc/hosts文件中添加192.168.1.102 hadoop102
    - 主机名称不要起hadoop  hadoop000等特殊名称	

- DataNode和NameNode进程同时只能工作一个。

- 执行命令不生效，粘贴word中命令时，遇到-和长–没区分开。导致命令失效

  - 解决办法：尽量不要粘贴word中代码。

- jps发现进程已经没有，但是重新启动集群，提示进程已经开启。原因是在linux的根目录下/tmp目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。

- jps不生效。原因：全局变量hadoop java没有生效。

  - 解决办法：需要source /etc/profile文件。

- 8088端口连接不上

  - [ttshe@hadoop102 桌面]$ cat /etc/hosts
  - 注释掉如下代码
    - #127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
    - #::1         hadoop102