

# 讨论

- 由于flume的一些缺点

  - 数据无法保存，客户如果想延迟获取做不到

  - 增加消费者,OCP
    - 要重启flume，导致现有消费者受到影响

  - 重复消费

- 设计一个存储系统，如何解决这些缺点？

![1568336143214](img/kafka/1.png)

- 该系统需要存储文件
  - store==>file
- 支持负载均衡的考虑
  - 多机部署
  - 需要负载均衡器
    - 如果Producer过多，会造成负载均衡器负载过重
    - 更改为注册中心，producer自己与存储系统Application通信
      - 每次返回Application的IP
      - producer内部做负载均衡和轮训机制
- 支持容灾的考虑
  - 如果有机器宕机，如何保证不丢失数据
  - 对数据进行备份
    - 备份数据不能存储在相同机器
    - 备份数据store-back存储在另一个Application上
    - 备份的数据如何同步？
      - producer发送数据给store和store-back？
        - 不合适，耦合性强，producer需要多关注数据的传输
      - 由Application内部进行同步
        - 由Store发送给Store-bak？
          - 不合适，store本身用于接收数据处理，负担重
          - Store-bak从Store拉取数据，合适
    - 如何保证数据的可见性？
      - 数据的可见性，store更新数据后，store-bak有也进行更新，保持同步
      - store更新了数据，而store-bak未更新数据，那么producer接收不到store发送的接收数据成功的回复，就是发送数据失败
- 关于消费者
  - 消费者获取数据是store推送还是拉取？
    - 拉取，推送会导致store完成的功能过多，负载过重，而拉取可以有效的解耦

- 增加消费者如何避免影响？
  - 增加的消费者从注册中心获取store的ip进行store的数据拉取，对现有的消费者没有影响
    - 没有重启store
- 消费者的消费模式
  - 多个消费者做负载均衡，消费同一批数据，那么消费者需要在一个consumer-group中，一个消费者拉取指定的数据，一个消费者拉取另一个指定的数据
  - 多个消费者如果不在同一个consumer-group，那么拉取的数据会有重复
- 如何重复拉取数据
  - 一个消费者如何可以再次拉取之前的数据？
    - 存储拉取历史记录，如拉取的时间点
    - 历史记录存储在消费者
      - 如存储在store中，造成耦合，负载过重



# 概念



## 消息队列

- 特点
  - 存储数据
  - 异步通信
  - 削峰处理

![1568337306571](img/kafka/2.png)



- 点对点模式
  - 一对一，消费者主动拉取数据，消息收到后消息清除
  - 点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。
- 发布/订阅模式
  - 一对多
  - 发布订阅模型则是另一个消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即使当前订阅者不可用，处于离线状态



## 为什么需要消息队列

- 解耦
  - 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。

- 冗余
  - 消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的"插入-获取-删除"范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。

- 扩展性
  - 因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可

- 灵活性 & 峰值处理能力
  - 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。

- 可恢复性
  - 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。

- ==顺序保证==
  - 在大多使用场景下，数据处理的顺序都很重要
  - 大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理
  - Kafka保证一个Partition内的消息的有序性

- 缓冲
  - 有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。

- 异步通信
  - 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们



## 关于Kafka

- 在流式计算中，一般用来缓存数据，Spark通过消费Kafka的数据进行计算

- 一个开源**消息**系统，由Scala写成
  - 是由Apache软件基金会开发的一个开源消息系统项目
  - Kafka最初是由LinkedIn公司开发，并于2011年初开源。2012年10月从Apache Incubator毕业
  - 该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台

- **一个分布式消息队列**
  - Kafka对消息保存是根据==Topic==进行归类
  - 发送消息者称为Producer
    - **不依赖ZK获取kafka的注册信息，直接与kafka通信获取所有kafka集群信息**
      - 减少与ZK建立连接的性能消耗
  - 消息接受者称为Consumer
  - kafka集群有多个kafka实例组成
    - 每个实例(server)称为broker，同agent含义相同

- 无论是kafka集群，还是consumer都依赖于**zookeeper**集群保存一些meta信息，来保证系统可用性



# 架构

![1568346220667](img/kafka/3.png)

![1568374439095](img/kafka/4.png)

- kafka是以ConsumerGroup进行消费的，没有单独的Consumer

  

## Producer 

- 消息生产者
- 向kafka broker发消息的客户端



## Consumer 

- 消息消费者
- 向kafka broker取消息的客户端

- 其中的offset需要存在在其他位置
  - 低版本0.9之前将offset保存在Zookeeper中
  - 0.9及之后保存在Kafka的`__consumer_offsets`主题中。

## Topic 

- 可以理解为一个队列



## Consumer Group

- CG
- 实现一个topic消息的广播
  - 发给所有的consumer
- 实现一个topic消息的单播
  - 发给任意一个consumer

- 一个topic可以有多个CG
- topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个partition只会把消息发给该CG中的一个consumer

- 要实现广播需要每个consumer有一个独立的CG
- 要实现单播需要所有的consumer在同一个CG
- 用CG可以将consumer进行自由的分组而不需要多次发送消息到不同的topic



## Broker 

- 一台kafka服务器就是一个broker
- 一个集群由多个broker组成
- 一个broker可以容纳多个topic



## Partition

- 为了实现扩展性，一个非常大的topic可以分布到多个broker上
- 一个topic可分为多个partition
- 每个partition是一个有序队列

- partition中的每条消息都会被分配一个有序的id（offset）
- ==kafka只保证按一个partition中的顺序将消息发给consumer==
- ==不保证一个topic的整体（多个partition间）的顺序==



## Offset

- kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找
- 例如你想找位于2049的位置，只要找到2048.kafka的文件即可
- the first offset就是00000000000.kafka



## Message

- 消息的存储格式组成
  - offset
    - 存储偏移量
    - 8个字节
  - size
    - 消息的长度
    - 4个字节
  - message
    - 存储消息的本体
    - N个字节
    - scala中
      - 称为message，组成
        - CRC32 ；4byte
        - 魔术值0,1；1byte
        - 属性；1byte
          - bit0-2；压缩类型
            - 0 不压缩
            - 1 gzip
            - 2 snappy
            - 3 lz4
          - bit 3 ；时间类型
            - 0 创建时间
            - 1 日志添加时间
          - bit 4-7；保留
        - 可选；8byte
          - 魔术值>0，增加timestap
        - key的长度；4byte
        - key的值；kbyte
        - value的长度；4byte
        - value的值；vbyte
    - java中
      - 称为record
      - org.apache.kafka.common.record
      - 等于将scala中的message翻译了一下

![1569055440147](img/kafka/18.png)

- 通过当前的offset+size计算得出下一个offset的值

  - 可以实现顺序读写

    

# 安装

- 计划

| hadoop102 | hadoop103 | hadoop104 |
| --------- | --------- | --------- |
| zk        | zk        | zk        |
| kafka     | kafka     | kafka     |

- jar包版本
  - kafka_2.11-0.11.0.2.tgz
    - 2.11是scala的版本
    - 0.11.02是kafka版本

- 解压安装，修改名称

```bash
[ttshe@hadoop102 software]$ tar -zvxf kafka_2.11-0.11.0.2.tgz -C /opt/module/
[ttshe@hadoop102 module]$ mv kafka_2.11-0.11.0.2/ kafka
```

- 在/opt/module/kafka目录下创建logs文件夹（保存数据、日志）

```bash
[ttshe@hadoop102 kafka]$ mkdir logs
```



## 配置server.properties

```bash
[ttshe@hadoop102 kafka]$ cd config/
[ttshe@hadoop102 config]$ vi server.properties
```

```bash
#broker的全局唯一编号，不能重复 ***
broker.id=0
#删除topic功能使能 *** 默认false，表示逻辑删除，true表示物理删除
delete.topic.enable=true
#处理网络请求的线程数量
num.network.threads=3
#用来处理磁盘IO的现成数量
num.io.threads=8
#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400
#接收套接字的缓冲区大小
socket.receive.buffer.bytes=102400
#请求套接字的缓冲区大小
socket.request.max.bytes=104857600
#kafka运行日志存放的路径	 ***
log.dirs=/opt/module/kafka/logs
#topic在当前broker上的分区个数
num.partitions=1
#用来恢复和清理data下数据的线程数量
num.recovery.threads.per.data.dir=1
#segment文件保留的最长时间，超时将被删除 ***
log.retention.hours=168
#配置连接Zookeeper集群地址 ***
zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181
```

- 配置环境变量

```bash
[ttshe@hadoop102 module]$ sudo vi /etc/profile
```

```bash
#KAFKA_HOME
export KAFKA_HOME=/opt/module/kafka
export PATH=$PATH:$KAFKA_HOME/bin
```

```bash
[ttshe@hadoop102 module]$ source /etc/profile
```

- 分发安装包

```bash
[ttshe@hadoop102 module]$ xsync kafka/
```

- 依次修改`server.properties`中的`broker.id`
  - 分别在hadoop103和hadoop104上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2
  - 注意：broker.id不得重复



# 启动



## ZK群起脚本

- 创建启动脚本
  - 在/home/ttshe/bin下创建zkstart.sh

```bash
#!/bin/bash
for i in ttshe@hadoop102 ttshe@hadoop103 ttshe@hadoop104
do
	echo "================           $i             ================"
	ssh $i '/opt/module/zookeeper-3.4.5/bin/zkServer.sh start'
done
```

- 创建关闭脚本zkstop.sh

```bash
#!/bin/bash
for i in ttshe@hadoop102 ttshe@hadoop103 ttshe@hadoop104
do
	echo "================           $i             ================"
	ssh $i '/opt/module/zookeeper-3.4.5/bin/zkServer.sh stop'
done
```

- 修改权限

```bash
[ttshe@hadoop101 bin]$ chmod 777 zkstart.sh 
[ttshe@hadoop101 bin]$ chmod 777 zkstop.sh 
```



## 启动Kafka集群

- 启动ZK

```bash
[ttshe@hadoop102 zookeeper-3.4.5]$ zkstart.sh
```

- 手动启动kafka集群
  - 依次在hadoop102、hadoop103、hadoop104节点上启动kafka

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-server-start.sh config/server.properties &
[ttshe@hadoop103 kafka]$ bin/kafka-server-start.sh config/server.properties &
[ttshe@hadoop104 kafka]$ bin/kafka-server-start.sh config/server.properties &
```

- 手动关闭kafka集群

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-server-stop.sh stop
[ttshe@hadoop103 kafka]$ bin/kafka-server-stop.sh stop
[ttshe@hadoop104 kafka]$ bin/kafka-server-stop.sh stop
```



## Kafka群起脚本

- 在/home/ttshe/bin下创建kkstart.sh

```bash
#!/bin/bash
for i in ttshe@hadoop102 ttshe@hadoop103 ttshe@hadoop104
do
        echo "================           $i             ================"
        ssh $i '/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties'
done
```

- 在/home/ttshe/bin下创建kkstop.sh

```bash
#!/bin/bash
for i in ttshe@hadoop102 ttshe@hadoop103 ttshe@hadoop104
do
        echo "================           $i             ================"
        ssh $i '/opt/module/kafka/bin/kafka-server-stop.sh /opt/module/kafka/config/server.properties'
done
```

- 设置权限

```bash
[ttshe@hadoop102 bin]$ chmod 777 kkstart.sh 
[ttshe@hadoop102 bin]$ chmod 777 kkstop.sh
```



# 命令行操作



## 查看帮助

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh help
```



## 创建topic

- 参数说明
  - --topic
    - 定义topic名
  - --replication-factor
    - 定义副本数
    - 含leader内的数据以及follower的数据副本总数
  - --partitions
    - 定义分区数
- 创建分区数大于broker数，但是副本数小于broker数
  - 当前broker数是3

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --topic mytopic_1 --partitions 4 --replication-factor 2

WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic "mytopic_1".
# 表示名称如果是mytopic.1 那么在kafka会转换为mytopic_1,会有重名的风险
```

- 如果副本数大于broker数，会创建失败

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --topic mytopic2 --partitions 4 --replication-factor 4

Error while executing topic command : replication factor: 4 larger than available brokers: 3
[2019-09-14 20:42:52,889] ERROR org.apache.kafka.common.errors.InvalidReplicationFactorException: replication factor: 4 larger than available brokers: 3
 (kafka.admin.TopicCommand$)
```



## 查看topic

- 如果没有topic，则显示为空

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --list
mytopic_1
```



## 查看topic详细描述

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic mytopic_1
Topic:mytopic_1	PartitionCount:4	ReplicationFactor:2	Configs:
	Topic: mytopic_1	Partition: 0	Leader: 2	Replicas: 2,0	Isr: 2,0
	Topic: mytopic_1	Partition: 1	Leader: 0	Replicas: 0,1	Isr: 0,1
	Topic: mytopic_1	Partition: 2	Leader: 1	Replicas: 1,2	Isr: 1,2
	Topic: mytopic_1	Partition: 3	Leader: 2	Replicas: 2,1	Isr: 2,1
```

- 说明
  - ReplicationFactor
    - 副本集个数
  - Partition
    - 分区编号
  - Leader
    - broker的编号
  - Replicas
    - 副本集所处于的broker的编号
    - ==值是leader编号的值==
  - ISR
    - In Sync Replication
    - 在同步的数据副本
    - 第一个表示Leader的副本所在的broker的编号
    - 第二个表示Follower的副本的broker的编号



## 发送消息

- 使用broker-list
  - 可以获取zk的kafka信息
  - 参数kafka服务的默认ip和端口
    - 默认的端口是9092
    - 在`server.properties`中进行配置
      - listeners=PLAINTEXT://:9092

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic mytopic_1

>hello kafka
>
```



## 消费消息

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --from-beginning --topic mytopic_1
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hello kafka
```

- --from-beginning

  - 把主题中以往所有的数据都读取出来
  - 根据业务场景选择是否

- 可以连接zookeeper

  - offset信息存储在zookeeper中

- 可以连接bootstrap-server

  - offset信息存储在kafka中

    

- 查看发送的数据
  - strings
    - 查看文件中可见的字符串
    - 二进制字符不显示

```bash
[ttshe@hadoop102 kafka]$ strings logs/mytopic_1-1/00000000000000000000.log 
hello kafka
```



## 删除topic

- 在server.properties中设置delete.topic.enable=true
  - 默认值false
    - 逻辑删除，只是标记删除
  - 设置为true后
    - 先标记删除
    - 然后被kafka的后台线程异步删除
- 设置完成后重启

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --delete --topic mytopic_1

Topic mytopic_1 is marked for deletion.
Note: This will have no impact if delete.topic.enable is not set to true.
```



## 查看数据log存储

- 创建topic

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --topic mytopic2 --partitions 2 --replication-factor 2
```

- 查看topic详细信息
  - 注意Replicas是Leader的编号的值

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic mytopic2
Topic:mytopic2	PartitionCount:2	ReplicationFactor:2	Configs:
	Topic: mytopic2	Partition: 0	Leader: 2	Replicas: 2,0	Isr: 2,0
	Topic: mytopic2	Partition: 1	Leader: 0	Replicas: 0,1	Isr: 0,1
```

- 查看hadoop102的kafka/logs

```bash
[ttshe@hadoop102 logs]$ tree
...
├── meta.properties
├── mytopic2-0
│   ├── 00000000000000000000.index
│   ├── 00000000000000000000.log
│   ├── 00000000000000000000.timeindex
│   └── leader-epoch-checkpoint
├── mytopic2-1
│   ├── 00000000000000000000.index
│   ├── 00000000000000000000.log
│   ├── 00000000000000000000.timeindex
│   └── leader-epoch-checkpoint
├── recovery-point-offset-checkpoint
...
```



## 当一台宕机情况

- 创建topic

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --topic mytopic3 --partitions 3 --replication-factor 2
```

- 查看topic详情

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic mytopic3

Topic:mytopic3	PartitionCount:3	ReplicationFactor:2	Configs:
	Topic: mytopic3	Partition: 0	Leader: 2	Replicas: 2,0	Isr: 2,0
	Topic: mytopic3	Partition: 1	Leader: 0	Replicas: 0,1	Isr: 0,1
	Topic: mytopic3	Partition: 2	Leader: 1	Replicas: 1,2	Isr: 1,2
```

- 当hadoop104宕机，kill掉之后查看topic详情

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic mytopic3

Topic:mytopic3	PartitionCount:3	ReplicationFactor:2	Configs:
	Topic: mytopic3	Partition: 0	Leader: 0	Replicas: 2,0	Isr: 0
	Topic: mytopic3	Partition: 1	Leader: 0	Replicas: 0,1	Isr: 0,1
	Topic: mytopic3	Partition: 2	Leader: 1	Replicas: 1,2	Isr: 1
```

- 此时再启动hadoop104的kafka

```bash
[ttshe@hadoop104 kafka]$ bin/kafka-server-start.sh config/server.properties &
```

- 再观察topic详情
  - Leader是先到先得，分区0的Leader从kafka-2变为kafka-0，当kafka-2再次恢复，也是Follower了

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic mytopic3

Topic:mytopic3	PartitionCount:3	ReplicationFactor:2	Configs:
	Topic: mytopic3	Partition: 0	Leader: 0	Replicas: 2,0	Isr: 0,2
	Topic: mytopic3	Partition: 1	Leader: 0	Replicas: 0,1	Isr: 0,1
	Topic: mytopic3	Partition: 2	Leader: 1	Replicas: 1,2	Isr: 1,2
```



## 再平衡

- 当一台机器宕机后恢复，但是原先作为的leader现在成为了follower，可能会有一个leader负责多个分区的情况产生，那么该leader的负载重，风险比较高
- 使用再平衡命令，平衡分区与leader的关系
- 先关一台kafka

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic mytopic3
Topic:mytopic3	PartitionCount:3	ReplicationFactor:2	Configs:
	Topic: mytopic3	Partition: 0	Leader: 0	Replicas: 2,0	Isr: 0
	Topic: mytopic3	Partition: 1	Leader: 0	Replicas: 0,1	Isr: 0,1
	Topic: mytopic3	Partition: 2	Leader: 1	Replicas: 1,2	Isr: 1
```

- 重启该kafka，观察

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic mytopic3
Topic:mytopic3	PartitionCount:3	ReplicationFactor:2	Configs:
	Topic: mytopic3	Partition: 0	Leader: 0	Replicas: 2,0	Isr: 0,2
	Topic: mytopic3	Partition: 1	Leader: 0	Replicas: 0,1	Isr: 0,1
	Topic: mytopic3	Partition: 2	Leader: 1	Replicas: 1,2	Isr: 1,2
```

- 再平衡后观察

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-preferred-replica-election.sh --zookeeper hadoop102:2181
Created preferred replica election path with {"version":1,"partitions":[{"topic":"mytopic_1","partition":3},{"topic":"mytopic_1","partition":0},{"topic":"mytopic2","partition":0},{"topic":"mytopic3","partition":2},{"topic":"mytopic2","partition":1},{"topic":"mytopic3","partition":0},{"topic":"mytopic3","partition":1},{"topic":"mytopic_1","partition":2},{"topic":"mytopic_1","partition":1}]}
Successfully started preferred replica election for partitions Set([mytopic_1,3], [mytopic3,1], [mytopic2,0], [mytopic3,2], [mytopic2,1], [mytopic_1,2], [mytopic_1,1], [mytopic3,0], [mytopic_1,0])
```

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --describe --topic mytopic3
Topic:mytopic3	PartitionCount:3	ReplicationFactor:2	Configs:
	Topic: mytopic3	Partition: 0	Leader: 2	Replicas: 2,0	Isr: 0,2
	Topic: mytopic3	Partition: 1	Leader: 0	Replicas: 0,1	Isr: 0,1
	Topic: mytopic3	Partition: 2	Leader: 1	Replicas: 1,2	Isr: 1,2
```



## 查看消费组

- 需要先查询zk得到消费组名称
  - Group
    - 消费组
  - Topic
    - 主题
  - offset
    - 消费者消费的消息偏移量
    - 消费的历史位置
  - logSize
    - 消息个数
  - lag
    - 延时消费个数
    - 未消费个数

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-consumer-offset-checker.sh --zookeeper hadoop102:2181 --group console-consumer-86639

[2019-09-15 10:04:06,874] WARN WARNING: ConsumerOffsetChecker is deprecated and will be dropped in releases following 0.9.0. Use ConsumerGroupCommand instead. (kafka.tools.ConsumerOffsetChecker$)
Group           Topic                          Pid Offset          logSize         Lag             Owner
console-consumer-86639 mytopic4                       0   1               1               0               console-consumer-86639_hadoop102-1568510763250-41ca8a43-0
console-consumer-86639 mytopic4                       1   0               0               0               console-consumer-86639_hadoop102-1568510763250-41ca8a43-0
```



# 工作流程

![1568473293578](img/kafka/5.png)



- producer没有连接zk是因为kafka集群连接了zk，获取到了集群信息给producer

- consumer按道理也可以不连接zk，从kafka集群获取集群信息，还为什么连接zk信息？
  - 需要在zk中写数据，写当前读取数据的记录
    - 偏移量
  - consumerA读取分区partition0，此时在zk中记录，其他的consumer就不能读取partition0了
    - 锁的概念
  - 新增的consumer从zk中获取信息，可以==再平衡==分区的分配
    - 有些consumer获取是多个分区的Leader
    - 再平衡可以重新进行Leader的规划

- 一个消费者消费一个分区，如果GC中的分区个数小于消费者个数，那么就有消费者等待

  

## 生产过程



### 写入方式

- producer采用推（push）模式将消息发布到broker

- 每条消息都被追加（append）到分区（patition）中
- ==属于顺序写磁盘==
  - 顺序写磁盘效率比随机写内存要高
  - 保障kafka吞吐率
- 磁盘缓存读取写入数据机制
  - RA
    - Read Ahead
    - 预读
      - 预先读取多行
      - block，page
  - WB
    - Write Behind
    - 后写：多个操作一起写入

#### 读数据

- 传统方式的读取系统磁盘A中的数据
  - 4个缓存，系统读缓存，应用程序读缓存，应用程序写缓存，系统写缓存

![1568477479868](img/kafka/6.png)

- kafka使用==零复制==
  - 在应用层没有复制的操作
  - 使用了磁盘缓存
  - 应用程序通知OS层将数据发送到Net，而不经过应用层缓存

![1568477517772](img/kafka/7.png)

#### 写数据

- kafka在写数据过程中
  - 顺序写磁盘
  - 不经过应用层缓存，再写入到file，
  - 通过系统缓存写入到File
  - 系统磁盘缓存写入的速度比应用层缓存写入到file的速度快
  - PageCache
    - JVM分配的内存之后剩余的内存大部分作为磁盘缓存

![1568477584305](img/kafka/8.png)



### 分区 partition

- 消息发送时都被发送到一个topic，其本质就是一个目录

- topic是由一些Partition Logs(分区日志)组成


![img](img/kafka/9.png)

![img](img/kafka/10.png)

- 每个Partition中的消息都是==有序==的
- 生产的消息被不断追加到Partition log上
- 每一个消息都被赋予了一个唯一的==offset值==



#### 分区的原因

- 方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器
- 一个topic可以有多个Partition组成，整个集群就可以适应任意大小的数据了

- 提高并发，可以以Partition为单位读写



#### 分区的原则

- 指定了patition，则直接使用

- 未指定patition但指定key
  - 通过对key的value进行hash出一个patition

- patition和key都未指定
  - 使用轮询选出一个patition

```java
DefaultPartitioner类
public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
		//获取所有的分区
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        int numPartitions = partitions.size();
        if (keyBytes == null) {
            int nextValue = nextValue(topic);
            // 获取可用的分区，没有宕机的分区
            List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);
            if (availablePartitions.size() > 0) {
            	// 进行轮询获取分区
                int part = Utils.toPositive(nextValue) % availablePartitions.size();
                return availablePartitions.get(part).partition();
            } else {
                // no partitions are available, give a non-available partition
                return Utils.toPositive(nextValue) % numPartitions;
            }
        } else {
            // hash the keyBytes to choose a partition
            // 指定分区的key
            return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
        }
    }
```



### 副本 Replication

- 同一个partition可能会有多个replication
  - 对应 `server.properties` 配置中的 `default.replication.factor=N`
- 没有replication的情况下，一旦broker 宕机，其上所有 partition 的数据都不可被消费，同时producer也不能再将数据存于其上的partition
- 引入replication之后，同一个partition可能会有多个replication
  - 需要在这些replication之间选出一个leader
  - ==producer和consumer只与这个leader交互==
  - 其它replication作为follower==从leader中复制==数据



### 写入流程

![1568507209945](img/kafka/12.png)

- producer先从zookeeper的 "/brokers/.../state"节点找到该partition的leader
  - producer直接从zk获取分区数据是不准确的，应该是==从kafka集群中获取分区数据==信息
- producer将消息发送给该leader
- leader将消息写入本地log
- followers从==leader pull消息==，写入本地log后向leader发送ACK
- leader收到所有ISR中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK



#### Producer内部构成

![1568506632196](img/kafka/11.png)

- 单独一个sender线程负责向集群写入数据
  - 这里p0和p1分区轮询发送分区数据给sender
  - sender收到数据后发送给集群
- 初始化sender通过kafka获取所有分区的信息
  - kafka通过ZK连接获取所有的分区信息
- 每个分区有一个双端队列dequeue
  - 负责发送数据
  - 如果满了就发送数据到集群
  - 如果没有满则在设置的时间间隔发送数据
  - 什么是双端队列
    - 两端都可以放入和读取数据
  - 为什么使用双端队列
    - sender可能会发送失败，数据要进行回退，放回dequeue
- 数据采集器
  - accumulator
  - 负责将数据放入dequeue中
  - 对数据进行整合，判断放入哪个partition中



#### ACK应答机制

- producer如何判断kafka集群获取到了数据
  - 基于ACK应答机制
    - 0,1,-1(all)
    - 0 表示集群无需应答，就可以发送下一条
      - 性能好，但是数据会丢失，不安全
    - 1 集群中只需要leader应答就可以发送下一条
      - 性能稍微差一点，数据可能会丢失
      - leader已经获取到数据，但是follower没有拉取到数据，此时leader宕机，切换leader就会有数据丢失
    - -1 集群中所有的备份数据都要应答才可以发送下一条数据
      - leader和follower都有应答
      - 性能最差，数据不会丢失



#### HW 和 LEO

- HW 
  - 高水位
  - 消费者在当前分区中==能够读取==的最大偏移量
  - 可读取的最大偏移量

- LEO
  - Log End Offset
  - 当前broker日志中最后存储的数据偏移量
  - 实际数据存储量

![1568508247109](img/kafka/13.png)

- 图示说明

  - 当leader有数据更新，follower拉取数据，第一次没有LEO信息

  - 第二次以后拉取数据含有follower的LEO信息

    - leader收到follower的LEO信息后，和leader的LEO比较，判断是否有更新
      - 如果follower的LEO小于Leader的LEO，返回新增数据
    - follower更新数据成功，leader增加HW

  - 如果HW还没有收到follower更新成功的通知，此时有consumer进行数据的读取，最多只能读取到HW的偏移量的数据

    - 此时有新增数据的不可见，2表示脏数据

  - 每次Fetch更新LEO，返回时更新Follower的HW

    

- 为什么会有HW和LEO不同步的情况？

  - 保证follower更新数据完成，才能最新的数据可见
  - 如果leader宕机，避免follower和leader数据不一致



## Broker 保存消息



### 存储方式

- 物理上把topic分成一个或多个patition
  - 默认应 server.properties 中的num.partitions=3配置
- 每个partition物理上对应一个文件夹
  - 存储该partition的所有消息
    - 顺序存储
  - 存储该partition的索引文件
    - 为什么有索引文件?
      - 用于文件的随机查找

```bash
[ttshe@hadoop102 logs]$ tree mytopic_1-0
mytopic_1-0
├── 00000000000000000000.index
├── 00000000000000000000.log
├── 00000000000000000000.timeindex
└── leader-epoch-checkpoint

0 directories, 4 files
```



### 存储策略

- 无论消息是否被消费，kafka都会保留所有消息
- 有两种策略可以删除旧数据
  - 基于时间
    - log.retention.hours=168
  - 基于大小
    - log.retention.bytes=1073741824

- Kafka读取特定消息的时间复杂度为O(1)
  - 与文件大小无关
  - 删除过期文件与提高 Kafka 性能无关



### Zookeeper存储结构

- producer不在zk中注册
- 消费者在zk中注册

![1568509670911](img/kafka/14.png)



- 查看ZK中kafka节点信息
  - controller_epoch
    - 表示controller切换的次数

```bash
[zk: localhost:2181(CONNECTED) 2] ls /

[isr_change_notification, admin, zookeeper, consumers, cluster, config, latest_producer_id_block, controller, brokers, controller_epoch]
```



#### 集群id信息

```bash
[zk: localhost:2181(CONNECTED) 3] get /cluster/id
{"version":"1","id":"atxlxvCbQUKuvS6osXaEdA"}
cZxid = 0x300000014
ctime = Sat Sep 14 20:20:21 CST 2019
mZxid = 0x300000014
mtime = Sat Sep 14 20:20:21 CST 2019
pZxid = 0x300000014
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 45
numChildren = 0
```



#### brokers信息

```bash
[zk: localhost:2181(CONNECTED) 8] ls /brokers 
[seqid, topics, ids]
```

```bash
[zk: localhost:2181(CONNECTED) 5] ls /brokers/ids
[2, 1, 0]
```

```bash
[zk: localhost:2181(CONNECTED) 6] get /brokers/ids/0
{"listener_security_protocol_map":{"PLAINTEXT":"PLAINTEXT"},"endpoints":["PLAINTEXT://hadoop102:9092"],"jmx_port":-1,"host":"hadoop102","timestamp":"1568474554014","port":9092,"version":4}
cZxid = 0x300000155
ctime = Sat Sep 14 23:22:34 CST 2019
mZxid = 0x300000155
mtime = Sat Sep 14 23:22:34 CST 2019
pZxid = 0x300000155
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x26d2fb2d3360021
dataLength = 188
numChildren = 0
```



#### topic信息

- 创建topic

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --topic mytopic4 --partitions 2 --replication-factor 2
```

- 查看topics

```bash
[zk: localhost:2181(CONNECTED) 10] ls /brokers/topics
[mytopic4]
```



#### partition信息

- isr中第一个是leader的编号

```bash
[zk: localhost:2181(CONNECTED) 11] ls /brokers/topics/mytopic4
[partitions]

[zk: localhost:2181(CONNECTED) 12] ls /brokers/topics/mytopic4/partitions
[1, 0]

[zk: localhost:2181(CONNECTED) 13] ls /brokers/topics/mytopic4/partitions/0      
[state]

[zk: localhost:2181(CONNECTED) 15] get /brokers/topics/mytopic4/partitions/0/state
{"controller_epoch":11,"leader":2,"version":1,"leader_epoch":0,"isr":[2,1]}
cZxid = 0x300000253
ctime = Sun Sep 15 09:18:53 CST 2019
mZxid = 0x300000253
mtime = Sun Sep 15 09:18:53 CST 2019
pZxid = 0x300000253
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 75
numChildren = 0
```



#### consumer信息

- 创建consumer和producer

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic mytopic4 --from-beginning
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
hello

[ttshe@hadoop102 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic mytopic4
>hello
```

- 查看zk上的consumers信息
  - 显示的消费组的信息

```bash
[zk: localhost:2181(CONNECTED) 17] ls /consumers
[console-consumer-86639]
[zk: localhost:2181(CONNECTED) 18] ls /consumers/console-consumer-86639
[offsets, owners, ids]
# 获取到消费组信息
[zk: localhost:2181(CONNECTED) 19] ls /consumers/console-consumer-86639/ids
[console-consumer-86639_hadoop102-1568510763250-41ca8a43]
```

```bash
[zk: localhost:2181(CONNECTED) 20] ls /consumers/console-consumer-86639/owners
[mytopic4]

[zk: localhost:2181(CONNECTED) 21] ls /consumers/console-consumer-86639/owners/mytopic4
[1, 0]

[zk: localhost:2181(CONNECTED) 22] ls /consumers/console-consumer-86639/owners/mytopic4/0
[]

#查看消费组成员分区的绑定信息
[zk: localhost:2181(CONNECTED) 23] get /consumers/console-consumer-86639/owners/mytopic4/0
console-consumer-86639_hadoop102-1568510763250-41ca8a43-0
cZxid = 0x30000026c
ctime = Sun Sep 15 09:26:03 CST 2019
mZxid = 0x30000026c
mtime = Sun Sep 15 09:26:03 CST 2019
pZxid = 0x30000026c
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x26d2fb2d3360043
dataLength = 57
numChildren = 0
```



##### 显示消费组

![1568513581700](img/kafka/15.png)

- lag延时消费个数，未消费的个数
- offset是消费者的历史消费偏移量
- 产生新的消息，查看消费情况
  - 查看消费情况有延时，已经消费了，但查看情况上开始显示没有消费，过一会查看，显示已消费





#### controller信息

- 每一个broker都有一个控制器
- 有一个主控制器
  - 在controller节点中存储
  - brokerid = 0的节点拿到了控制器
- 作用
  - 竞选操作
  - 管理topic
  - 分区发生了变化的信息处理

```bash
[zk: localhost:2181(CONNECTED) 28] get /controller
{"version":1,"brokerid":0,"timestamp":"1568474553897"}
cZxid = 0x30000014f
ctime = Sat Sep 14 23:22:33 CST 2019
mZxid = 0x30000014f
mtime = Sat Sep 14 23:22:33 CST 2019
pZxid = 0x30000014f
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x26d2fb2d3360021
dataLength = 54
numChildren = 0
```



## 消费过程



![1568522010903](img/kafka/16.png)



### consumer直接访问集群

- consumer访问zk启动
  - 历史offset记录存储在zk中
- consumer访问kafka集群启动
  - 历史offset记录存储在kafka集群中

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic mytopic4 --from-beginning
```

- 查看kafka/logs内容

```bash
[ttshe@hadoop102 kafka]$ cd logs
[ttshe@hadoop102 logs]$ ll
总用量 1704
-rw-rw-r--. 1 ttshe ttshe      4 9月  15 09:11 cleaner-offset-checkpoint
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-11
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-14
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-17
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-2
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-20
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-23
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-26
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-29
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-32
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-35
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-38
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-41
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-44
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 12:29 __consumer_offsets-47
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-5
drwxrwxr-x. 2 ttshe ttshe   4096 9月  15 10:03 __consumer_offsets-8
...

[ttshe@hadoop102 logs]$ cd __consumer_offsets-11/
[ttshe@hadoop102 __consumer_offsets-11]$ ll
总用量 0
-rw-rw-r--. 1 ttshe ttshe 10485760 9月  15 10:03 00000000000000000000.index
-rw-rw-r--. 1 ttshe ttshe        0 9月  15 10:03 00000000000000000000.log
-rw-rw-r--. 1 ttshe ttshe 10485756 9月  15 10:03 00000000000000000000.timeindex
-rw-rw-r--. 1 ttshe ttshe        0 9月  15 10:03 leader-epoch-checkpoint
```

- 默认50个offsets文件夹，交替均分在各个broker中
  - 存储consumer的偏移量



### 高级API

- 优点
  - 书写简单
  - 不需要自行去管理offset
  - 系统通过zookeeper自行管理
  - 不需要管理分区，副本等情况，系统自动管理
  - 消费者断线会自动根据上一次记录在zookeeper中的offset去接着获取数据
  - 默认设置1分钟更新一下zookeeper中存的offset
  - 使用group来区分对同一个topic 的不同程序访问分离开来
    - 不同的group记录不同的offset，这样不同程序读取同一个topic才不会因为offset互相影响

- 缺点
  - 不能自行控制offset
    - 对于某些特殊需求来说
  - 不能细化控制如分区、副本、zk等



### 低级API

-  优点
  - 能够让开发者自己控制offset，想从哪里读取就从哪里读取
  - 自行控制连接分区，对分区自定义进行负载均衡
  - 对zookeeper的依赖性降低
    - offset不一定非要靠zk存储，自行存储offset即可，比如存在文件或者内存中

- 缺点
  - 复杂
  - 需要自行控制offset，连接哪个分区，找到分区leader 等



### 消费者组

![1568522182775](img/kafka/17.png)



- 一个consumer可以读取多个分区
- 消费者是以consumer group消费者组的方式工作
- 由一个或者多个消费者组成一个组，共同消费一个topic
- 每个分区在同一时间只能由group中的一个消费者读取
- 多个group可以同时消费同一个partition
- 图中
  - 有一个由三个消费者组成的group
  - 有一个消费者读取主题中的两个分区，另外两个分别读取一个分区
  - 某个消费者读取某个分区
    - 某个消费者是某个分区的拥有者

- 消费者可以通过水平扩展的方式同时读取大量的消息
  - 如果一个消费者失败了，那么其他的group成员会自动负载均衡读取之前失败的消费者读取的分区

- 案例
  - 需求
    - 测试同一个消费者组中的消费者
    - 同一时刻只能有一个消费者消费
- 在hadoop102、hadoop103上修改/opt/module/kafka/config/consumer.properties配置文件中的group.id属性为任意组名

```bash
[atguigu@hadoop103 config]$ vi consumer.properties
group.id=ttshe-group
```

- 在hadoop102、hadoop103上分别启动消费者

```bash
[atguigu@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic mytopic4 --consumer.config config/consumer.properties
[atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic mytopic4 --consumer.config config/consumer.properties
```

- 在hadoop104上启动生产者

```bash
[atguigu@hadoop104 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic mytopic4
```

- 查看hadoop102和hadoop103的接收者
  - 同一时刻只有一个消费者接收到消息
  - 消息交替接收



### 消费方式

- consumer采用pull（拉）模式从broker中读取数据
  - push（推）模式很难适应消费速率不同的消费者
    - 因为消息发送速率是由broker决定的
    - 目标是尽可能以最快速度传递消息
    - 推很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞
    - pull模式则可以根据consumer的消费能力以适当的速率消费消息

- 对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率
- consumer可以自己控制消费方式
  - 可批量消费也可逐条消费
  - 还可选择不同的提交方式从而实现不同的传输语义

- pull模式缺点
  - 如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达
  - 为了避免这种情况
    - 在拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞
    - 且可选地等待到给定的字节数，以确保大的传输大小，积累一定的返回数据后返回



# API使用

- 环境准备
  - 启动ZK和kafka 集群
  - 创建主题api_test
  - 在kafka集群中打开一个消费者

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --topic api_test --partitions 4 --replication-factor 2
```

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --topic api_test
```



## pom

```xml
<!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients -->
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>0.11.0.0</version>
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka -->
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka_2.12</artifactId>
    <version>0.11.0.0</version>
</dependency>
```



## 生产者

- 最多只能保证在分区内有序
- 如果分区是1个，那么可以保证topic内有序



### 异步发送数据

- 新API

```java
package com.stt.demo.kafka.Ch01_producer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
import java.util.Properties;
import static org.apache.kafka.clients.producer.ProducerConfig.BOOTSTRAP_SERVERS_CONFIG;

public class HelloProducer {

	public static void main(String[] args) {
		// 创建配置对象
		Properties props = new Properties();
		// 所有的配置关键字都是在ProducerConfig中有声明
		// kafka集群配置
		props.put(BOOTSTRAP_SERVERS_CONFIG, "hadoop103:9092");
		// 等待所有副本节点的应答
		// 0 : 不需要应答
		// 1 : 需要leader应答
		// -1(all) : 需要所有副本应答
		props.put("acks", "all");
		// 消息发送最大尝试次数
		// 如果为0，表示失败了不向双端队列中存放
		props.put("retries", 0);
		// 一批消息处理大小
		props.put("batch.size", 16384);
		// 用于延时发送一批消息给kafka，超过该延时，即使batch没有满也要发送消息请求给kafka
		props.put("linger.ms", 1);
		// 发送缓存区内存大小
		props.put("buffer.memory", 33554432);
		// key序列化
		props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
		// value序列化
		props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
		// 表示生产者发送的消息的key和value的类型
		// key 用于分区时使用
		Producer<String, String> producer = new KafkaProducer<>(props);
		for (int i = 0; i < 100; i++){
			// 发送100个消息，key是i，value是hello-i
			// 封装数据
			ProducerRecord<String, String> record =
					new ProducerRecord<>("api-test", Integer.toString(i), "hello-" + Integer.toString(i));
			// 发送数据
			producer.send(record);
		}
		// 关闭资源
		producer.close();
	}
}
```

- 旧API

```java
package com.ttshe.kafka;
import java.util.Properties;
import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;

public class OldProducer {

	@SuppressWarnings("deprecation")
	public static void main(String[] args) {
		
		Properties properties = new Properties();
		properties.put("metadata.broker.list", "hadoop102:9092");
		properties.put("request.required.acks", "1");
		properties.put("serializer.class", "kafka.serializer.StringEncoder");
		
		Producer<Integer, String> producer = new Producer<Integer,String>(new ProducerConfig(properties));
		
		KeyedMessage<Integer, String> message = new KeyedMessage<Integer, String>("api_test", "hello world");
		producer.send(message );
	}
}
```



### 同步发送数据

- 同步发送数据在send之后添加get，表示获取发送结果，发送成功后才发送下一个消息

```java
// 发送数据
// 同步发送，发送成功之后继续发送下一个消息
producer.send(record).get();
```



### 含有回调结果

- 修改send函数调用，添加回调方法
- 发送完成之后接收到数据

```java
producer.send(record, new Callback() {
    @Override
    public void onCompletion(RecordMetadata metadata, Exception exception) {
        System.out.println(metadata.topic()+";"+
                           metadata.partition()+";"+metadata.offset());
    }
});
```



### 指定分区号

- 需求：将所有数据存储到topic的第0号分区上
- 自定义分区（新API）

```java
package com.stt.demo.kafka.Ch02_partitioner;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import java.util.Map;

public class MyPartitioner implements Partitioner {
	@Override
	public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
		// 返回分区号
		return 0;
	}

	@Override
	public void close() {}

	@Override
	public void configure(Map<String, ?> configs) {}
}
```

```java
package com.stt.demo.kafka.Ch02_partitioner;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
import java.util.Properties;
import static org.apache.kafka.clients.producer.ProducerConfig.BOOTSTRAP_SERVERS_CONFIG;

public class HelloProducer {

	public static void main(String[] args) {
		Properties props = new Properties();
		props.put(BOOTSTRAP_SERVERS_CONFIG, "hadoop102:9092");
		props.put("acks", "all");
		props.put("retries", 0);
		props.put("batch.size", 16384);
		props.put("linger.ms", 1);
		props.put("buffer.memory", 33554432);
		props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
		props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
		
        // 自定义分区
		props.put("partitioner.class", "com.stt.demo.kafka.Ch02_partitioner.MyPartitioner");
        
		Producer<String, String> producer = new KafkaProducer<>(props);
		for (int i = 0; i < 10; i++){
			ProducerRecord<String, String> record =
					new ProducerRecord<>("api_test", Integer.toString(i), "hello-" + Integer.toString(i));
			producer.send(record);
		}
		// 关闭资源
		producer.close();
	}
}
```

- 定义一个类实现Partitioner接口，重写里面的方法（过时API）

```java
package com.ttshe.kafka;
import java.util.Map;
import kafka.producer.Partitioner;

public class CustomPartitioner implements Partitioner {

	public CustomPartitioner() {
		super();
	}

	@Override
	public int partition(Object key, int numPartitions) {
		// 控制分区
		return 0;
	}
}
```

- 其他方式
  - 不需要指定`partitioner.class`，直接在创建`ProducerRecord`的时候指定分区号
  - 优先级比`partitioner.class`高

```java
// 指定record的分区
ProducerRecord<String, String> record =
    new ProducerRecord<>("api_test", 0,Integer.toString(i), "hello2-" + Integer.toString(i));
```

- 使用场景
  - 有复杂分区业务逻辑使用`partitioner.class`
  - 简单分区，直接指定的key，在创建`ProducerRecord`时指定分区号



## 消费者



### 高级API

```java
package com.stt.demo.kafka.Ch03_consumer;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.util.Arrays;
import java.util.Properties;

public class HighLevelConsumer {

	public static void main(String[] args) {
		Properties props = new Properties();
		// 定义kafka 服务的地址，不需要将所有broker指定上
		props.put("bootstrap.servers", "hadoop102:9092");
		// 制定consumer group
		props.put("group.id", "test2");
		// 是否自动确认offset
		props.put("enable.auto.commit", "true");
		// 自动确认offset的时间间隔
		props.put("auto.commit.interval.ms", "1000");
		// key的序列化类
		props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
		// value的序列化类
		props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
		// 定义consumer
		KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);

		// 消费者订阅的topic, 可同时订阅多个
		consumer.subscribe(Arrays.asList("api_test"));

		while (true) {
			// 读取数据，读取超时时间为100ms
			ConsumerRecords<String, String> records = consumer.poll(1000);

			for (ConsumerRecord<String, String> record : records)
				System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value());
		}
	}
}
```

- 注意，需要先开启消费端，然后再开启客户端





### 低级API

- 实现使用低级API读取指定topic，指定partition,指定offset的数据
  - offset可以存储在本地mysql，从而省去读取元数据获取offset操作

- 消费者使用低级API 的主要步骤
  - 根据指定的分区partition从主题topic元数据中找到主副本leader
  - 从主副本拉取分区的消息

- 方法描述
  - getLeader()
    - 客户端向种子节点发送主题元数据，将副本集加入备用节点
  - getData()
    - 消费者低级AP I拉取消息的主要方法

- 消费流程

  ![1569064751087](img/kafka/19.png)



```java
package com.stt.demo.kafka.Ch03_consumer;

import kafka.api.FetchRequest;
import kafka.api.FetchRequestBuilder;
import kafka.javaapi.*;
import kafka.javaapi.consumer.SimpleConsumer;
import kafka.javaapi.message.ByteBufferMessageSet;
import kafka.message.MessageAndOffset;

import java.io.UnsupportedEncodingException;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

public class LowLevelConsumer {

	/**
	 * 读取指定topic，指定partition,指定offset的数据
	 * 因为一个topic中存在多个partition，而且每一个partition中会有多个副本，所以想要获取指定的数据
	 * 必须从指定分区的主副本中获取数据，那么就必须拿到主副本的元数据信息。
	 * 1）发送主题元数据请求对象
	 * 2）得到主题元数据响应对象，其中包含主题元数据信息
	 * 3）通过主题元数据信息，获取主题的分区元数据信息
	 * 4）获取指定的分区元数据信息
	 * 5）获取分区的主副本信息
	 *
	 * 获取主副本信息后，消费者要连接对应的主副本机器，然后抓取数据
	 * 1）构建抓取数据请求对象
	 * 2）发送抓取数据请求
	 * 3）获取抓取数据响应，其中包含了获取的数据
	 * 4）由于获取的数据为字节码，还需要转换为字符串，用于业务的操作。
	 * 5）将获取的多条数据封装为集合。
 	 * @param args
	 */
	public static void main(String[] args) throws UnsupportedEncodingException {
		List<String> brokersHost = Arrays.asList("hadoop102", "hadoop103", "hadoop104");
		// 连接kafka的集群端口号
		int port = 9092;
		String topic = "api_test";
		int partition = 0;
		long offset = 0;
		// 获取指定topic指定分区的leader的ip
		String leaderHost = getLeaderHost(brokersHost,port,topic,partition);
		// 获取数据
		List<String> data = getData(leaderHost, port, topic, partition, offset);
		for (String datum : data) {
			System.out.println(datum);
		}
	}

	private static String getLeaderHost(List<String> brokersHost, int port, String topic, int partition) {
		for (String broker : brokersHost) {
			// 遍历集群，根据节点信息创建SimpleConsumer
			// 超时时间 ms单位
			// 最后参数：客户端id，用于区分SimpleConsumer
			SimpleConsumer getLeader = new SimpleConsumer(broker, port, 1000, 1024 * 4, "getLeader");
			// 根据传入的主题信息创建元数据请求
			TopicMetadataRequest request = new TopicMetadataRequest(Arrays.asList(topic));
			// 发送元数据请求得到返回值
			TopicMetadataResponse response = getLeader.send(request);

			List<TopicMetadata> topicMetadata = response.topicsMetadata();
			for (TopicMetadata topicMetadatum : topicMetadata) {
				//一个Topic由多个Partition组成
				List<PartitionMetadata> partitionMetadata = topicMetadatum.partitionsMetadata();
				// 遍历多个分区的元数据信息
				for (PartitionMetadata partitionMetadatum : partitionMetadata) {
					// 匹配传入的分区号
					if(partitionMetadatum.partitionId() == partition){
						return partitionMetadatum.leader().host();
					}
				}
			}

		}
		return null;
	}

	private static List<String> getData(String leaderHost, int port, String topic, int partition, long offset) throws UnsupportedEncodingException {
		SimpleConsumer consumer = new SimpleConsumer(leaderHost,port,1000,1024*1024,"getData");
		FetchRequest request = new FetchRequestBuilder().addFetch(topic,partition,offset,1024*1024).build();
		FetchResponse response = consumer.fetch(request);
		ByteBufferMessageSet messageAndOffsets = response.messageSet(topic, partition);
		List<String> result = new ArrayList<>();
		for (MessageAndOffset m : messageAndOffsets) {
			ByteBuffer payload = m.message().payload();
			byte[] bytes = new byte[payload.limit()];
			payload.get(bytes);
			String val = new String(bytes,"utf-8");
			result.add(val);
		}
		return result;
	}
}
```



# 生产者拦截器



## 说明

- Producer拦截器(interceptor)是在Kafka 0.10版本被引入
- 主要用于实现clients端的定制化控制逻辑

- 对于producer
  - interceptor使得用户在消息发送前
  - producer回调逻辑前有机会对消息做一些定制化需求
    - 如==修改消息==等
- producer允许指定多个interceptor按序作用于同一条消息而形成一个拦截链(interceptor chain)
- intercetpor的实现接口是`org.apache.kafka.clients.producer.ProducerInterceptor`
  - 其定义的方法
    - configure(configs)
      - 获取配置信息和初始化数据时调用
    - onSend(ProducerRecord)
      - 该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中
      - Producer确保在消息被序列化以及计算分区前调用该方法
      - ==用户可以在该方法中对消息做任何操作==
        - 最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算
    - onAcknowledgement(RecordMetadata, Exception)
      - 该方法会在消息被应答或消息发送失败时调用
      - 通常都是在producer回调逻辑触发之前
      - onAcknowledgement运行在producer的IO线程中，不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率
    - close
      - 关闭interceptor
      - 用于执行一些资源清理工作

- interceptor可能被运行在多个线程中
  - 具体实现时用户需要自行确保线程安全
- 若指定了多个interceptor，则producer将按照指定顺序调用
  - 仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中==而非向上传递==
    - 这在使用过程中要特别留意



## 示例

- 需求：实现一个简单的双interceptor组成的拦截链
  - 第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部
  - 第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数

```java
package com.stt.demo.kafka.Ch04_interceptor;

import org.apache.kafka.clients.producer.ProducerInterceptor;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;

import java.util.Map;

public class TimeInterceptor implements ProducerInterceptor<String,String> {

	@Override
	public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {
		// 创建一个新的record，把时间戳写入消息体的最前部
		return new ProducerRecord(record.topic(),record.partition(),record.timestamp(),record.key()
				,record.value().toString()+":"+System.currentTimeMillis());
	}

	@Override
	public void onAcknowledgement(RecordMetadata metadata, Exception exception) {

	}

	@Override
	public void close() {

	}

	@Override
	public void configure(Map<String, ?> configs) {

	}
}
```

```java
package com.stt.demo.kafka.Ch04_interceptor;

import org.apache.kafka.clients.producer.ProducerInterceptor;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;

import java.util.Map;

public class CounterInterceptor implements ProducerInterceptor<String,String> {

	private int errorCounter = 0;
	private int successCounter = 0;

	@Override
	public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {
		return record;
	}

	// 统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器
	@Override
	public void onAcknowledgement(RecordMetadata metadata, Exception exception) {
		if(exception != null){
			errorCounter ++;
		}else{
			successCounter ++;
		}
	}

	@Override
	public void close() {
		// 保存结果
		System.out.println("Successful sent: " + successCounter);
		System.out.println("Failed sent: " + errorCounter);
	}

	@Override
	public void configure(Map<String, ?> configs) {
	}
}
```

```java
package com.stt.demo.kafka.Ch04_interceptor;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;

import java.util.Arrays;
import java.util.Properties;


public class InterceptorProducer {

	public static void main(String[] args) {
		Properties props = new Properties();
		props.put("bootstrap.servers", "hadoop102:9092");
		props.put("acks", "all");
		props.put("retries", 0);
		props.put("batch.size", 16384);
		props.put("linger.ms", 1);
		props.put("buffer.memory", 33554432);
		props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
		props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

		// 构建拦截链
		props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, Arrays.asList(
				"com.stt.demo.kafka.Ch04_interceptor.TimeInterceptor",
				"com.stt.demo.kafka.Ch04_interceptor.CounterInterceptor"));
		Producer<String, String> producer = new KafkaProducer<>(props);
		// 发送消息
		for (int i = 0; i < 10; i++) {
			producer.send(new ProducerRecord<>("api_test", "message" + i));
		}

		//一定要关闭producer，这样才会调用interceptor的close方法
		producer.close();
	}
}
```



# Kafka Streams（了解）

- Apache Kafka开源项目的一个组成部分

- 一个功能强大，易于使用的库

- 用于在Kafka上构建高可分布式、拓展性，容错的应用程序
- ==用的不多==
  - 2个topic相互转换
  - 一般用一个topic进行转换后，返回到该topic中



## 特点

- 功能强大 

- 高扩展性，弹性，容错 

- 轻量级 

- 无需专门的集群 

- 一个库，而不是框架

- 完全集成 

- 100%的Kafka 0.10.0版本兼容

- 易于集成到现有的应用程序 

- 实时性

- 毫秒级延迟 

- 并非微批处理 

- 窗口允许乱序数据 

- 允许迟到数据



## 好处

- 开源流式处理系统有Spark Streaming和Apache Storm
  - Apache Storm发展多年，应用广泛，提供记录级别的处理能力，当前也支持SQL on Stream
  - Spark Streaming基于Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它Spark应用开发的用户而言使用门槛低
  - 目前主流的Hadoop发行版，如Cloudera和Hortonworks集成了Apache Storm和Apache Spark，使得部署更容易

既然Apache Spark与Apache Storm拥用如此多的优势，那为何还需要Kafka Stream呢？

- Spark和Storm都是流式处理框架，而Kafka Stream提供的是一个基于Kafka的流式处理类库
- 框架要求开发者按照特定的方式去开发逻辑部分，供框架调用
- 开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限
- Kafka Stream作为流式处理类库，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试

![1569079393458](img/kafka/20.png)

- 虽然Cloudera与Hortonworks方便了Storm和Spark的部署，但是这些框架的部署仍然相对复杂
  - Kafka Stream作为类库，可以非常方便的嵌入应用程序中，它对应用的打包和部署基本没有任何要求

- 就流式处理系统而言，基本都支持Kafka作为数据源
  - 如Storm具有专门的kafka-spout
  - Spark提供专门的spark-streaming-kafka模块
  - 事实上，Kafka基本上是主流的流式处理系统的标准数据源
  - 大部分流式系统中都已部署了Kafka
  - 使用Kafka Stream的成本非常低
- 使用Storm或Spark Streaming时，需要为框架本身的进程预留资源
  - 如Storm的supervisor和Spark on YARN的node manager
  - 即使对于应用实例而言，框架本身也会占用部分资源
    - 如Spark Streaming需要为shuffle和storage预留内存
  - 是Kafka作为类库不占用系统资源

- 由于Kafka本身提供数据持久化，因此Kafka Stream提供滚动部署和滚动升级以及重新计算的能力

- 由于Kafka Consumer Rebalance机制，Kafka Stream可以在线动态调整并行度



## 案例

需求：实时处理单词带有”>>>”前缀的内容。例如输入”yyy>>>xxx”，最终处理成“yyyxxx”

- 添加pom

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-streams</artifactId>
    <version>0.11.0.2</version>
</dependency>
```

- 实现

```java
package com.stt.demo.kafka.Ch05_streams;

import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.processor.Processor;
import org.apache.kafka.streams.processor.ProcessorContext;
import org.apache.kafka.streams.processor.ProcessorSupplier;
import org.apache.kafka.streams.processor.TopologyBuilder;
import java.io.UnsupportedEncodingException;
import java.util.Properties;

public class MyStreams {

	/**
	 * 需要2个topic，一个是输入，一个是输出
	 * @param args
	 */
	public static void main(String[] args) {

		String fromTopic = "fromTopic";
		String toTopic = "toTopic";
		// 参数
		Properties prop = new Properties();
		// 定义本次应用的id
		prop.put(StreamsConfig.APPLICATION_ID_CONFIG,"myFilter");
		prop.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,"hadoop102:9092");

		StreamsConfig config = new StreamsConfig(prop);

		// 构建拓扑
		TopologyBuilder topologyBuilder = new TopologyBuilder()
						.addSource("mySource",fromTopic)
						.addProcessor("myProcessor", new ProcessorSupplier<byte[],byte[]>() {
							@Override
							public Processor get() {
								return new MyProcessor();
							}
						},"mySource")
						.addSink("mySink",toTopic,"myProcessor");
		// 创建streams
		KafkaStreams kafkaStreams = new KafkaStreams(topologyBuilder,config);
		kafkaStreams.start();
	}


	static class MyProcessor implements Processor<byte[], byte[]>{

		private ProcessorContext context;

		@Override
		public void init(ProcessorContext processorContext) {
			this.context = processorContext;
		}

		@Override
		public void process(byte[] key, byte[] value) {
			try {
				String input = new String(value,"utf-8");
				input = input.replaceAll(">>>","");
				context.forward(key,input.getBytes("utf-8"));
			} catch (UnsupportedEncodingException e) {
				e.printStackTrace();
			}
		}

		@Override
		public void punctuate(long l) {

		}

		@Override
		public void close() {

		}
	}
}
```



测试

- 创建2个topic：fromTopic和toTopic

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --topic fromTopic --partitions 1 --replication-factor 1
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --topic toTopic --partitions 1 --replication-factor 1
```

- 启动main程序
  - 程序进行监听消息
- 启动消费者，从toTopic获取消息

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --from-beginning --topic toTopic
```

- 启动生产者，发送消息给fromTopic，并输入内容，如“xxx>>>yyy”，查看结果显示

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic fromTopic
```



# 扩展



## kafka与flume比较

- flume
  - cloudera公司研发
  - 适合多个生产者
  - 适合下游数据消费者不多的情况
  - 适合数据安全性要求不高的操作
    - 宕机会有数据丢失
  - 适合与Hadoop生态圈对接的操作

- kafka
  - linkedin公司研发
  - 适合数据下游消费众多的情况
  - 适合数据安全性要求较高的操作
  - 支持replication

- 常用模型
  - 线上数据 --> flume --> kafka --> flume(根据情景增删该流程) --> HDFS



## kafka与flume集成



### 配置flume

```bash
[ttshe@hadoop103 job]$ touch flume-log-2-kafka.conf
[ttshe@hadoop103 job]$ vim flume-log-2-kafka.conf 
```

```bash
# define
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# source
# 表示执行命令
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F -c +0 /opt/module/datas/flume.log
a1.sources.r1.shell = /bin/bash -c

# sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sinks.k1.kafka.topic = my-topic
a1.sinks.k1.kafka.flumeBatchSize = 20
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.kafka.producer.linger.ms = 1

# channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# bind
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```



### 测试

- 启动kafka消费者

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-console-consumer.sh --zookeeper hadoop102:2181 --from-beginning --topic my-topic
```

- 启动flume

```bash
[ttshe@hadoop102 flume]$ bin/flume-ng agent -C conf/ -n a1 -f job/flume-log-2-kafka.conf
```

- 向 /opt/module/datas/flume.log里追加数据，查看kafka消费者消费情况

```bash
[ttshe@hadoop102 datas]$ echo hlll2 >> flume.log
```



# 配置信息



## broker

| **属性**                                | **默认值**         | **描述**                                                     |
| --------------------------------------- | ------------------ | ------------------------------------------------------------ |
| broker.id                               |                    | 必填参数，broker的唯一标识                                   |
| log.dirs                                | /tmp/kafka-logs    | Kafka数据存放的目录。可以指定多个目录，中间用逗号分隔，当新partition被创建的时会被存放到当前存放partition最少的目录。 |
| port                                    | 9092               | BrokerServer接受客户端连接的端口号                           |
| zookeeper.connect                       | null               | Zookeeper的连接串，格式为：hostname1:port1,hostname2:port2,hostname3:port3。可以填一个或多个，为了提高可靠性，建议都填上。注意，此配置允许我们指定一个zookeeper路径来存放此kafka集群的所有数据，为了与其他应用集群区分开，建议在此配置中指定本集群存放目录，格式为：hostname1:port1,hostname2:port2,hostname3:port3/chroot/path 。需要注意的是，消费者的参数要和此参数一致。 |
| message.max.bytes                       | 1000000            | 服务器可以接收到的最大的消息大小。注意此参数要和consumer的maximum.message.size大小一致，否则会因为生产者生产的消息太大导致消费者无法消费。 |
| num.io.threads                          | 8                  | 服务器用来执行读写请求的IO线程数，此参数的数量至少要等于服务器上磁盘的数量。 |
| queued.max.requests                     | 500                | I/O线程可以处理请求的队列大小，若实际请求数超过此大小，网络线程将停止接收新的请求。 |
| socket.send.buffer.bytes                | 100 * 1024         | The SO_SNDBUFF buffer the server prefers for socket connections. |
| socket.receive.buffer.bytes             | 100 * 1024         | The SO_RCVBUFF buffer the server prefers for socket connections. |
| socket.request.max.bytes                | 100 * 1024 * 1024  | 服务器允许请求的最大值， 用来防止内存溢出，其值应该小于 Java heap size. |
| num.partitions                          | 1                  | 默认partition数量，如果topic在创建时没有指定partition数量，默认使用此值，建议改为5 |
| log.segment.bytes                       | 1024 * 1024 * 1024 | Segment文件的大小，超过此值将会自动新建一个segment，此值可以被topic级别的参数覆盖。 |
| log.roll.{ms,hours}                     | 24 * 7 hours       | 新建segment文件的时间，此值可以被topic级别的参数覆盖。       |
| log.retention.{ms,minutes,hours}        | 7 days             | Kafka segment log的保存周期，保存周期超过此时间日志就会被删除。此参数可以被topic级别参数覆盖。数据量大时，建议减小此值。 |
| log.retention.bytes                     | -1                 | 每个partition的最大容量，若数据量超过此值，partition数据将会被删除。注意这个参数控制的是每个partition而不是topic。此参数可以被log级别参数覆盖。 |
| log.retention.check.interval.ms         | 5 minutes          | 删除策略的检查周期                                           |
| auto.create.topics.enable               | true               | 自动创建topic参数，建议此值设置为false，严格控制topic管理，防止生产者错写topic。 |
| default.replication.factor              | 1                  | 默认副本数量，建议改为2。                                    |
| replica.lag.time.max.ms                 | 10000              | 在此窗口时间内没有收到follower的fetch请求，leader会将其从ISR(in-sync replicas)中移除。 |
| replica.lag.max.messages                | 4000               | 如果replica节点落后leader节点此值大小的消息数量，leader节点就会将其从ISR中移除。 |
| replica.socket.timeout.ms               | 30 * 1000          | replica向leader发送请求的超时时间。                          |
| replica.socket.receive.buffer.bytes     | 64 * 1024          | The socket receive buffer for network requests to the leader for replicating data. |
| replica.fetch.max.bytes                 | 1024 * 1024        | The number of byes of messages to attempt to fetch for each partition in the fetch requests the replicas send to the leader. |
| replica.fetch.wait.max.ms               | 500                | The maximum amount of time to wait time for data to arrive on the leader in the fetch requests sent by the replicas to the leader. |
| num.replica.fetchers                    | 1                  | Number of threads used to replicate messages from leaders. Increasing this value can increase the degree of I/O parallelism in the follower broker. |
| fetch.purgatory.purge.interval.requests | 1000               | The purge interval (in number of requests) of the fetch request purgatory. |
| zookeeper.session.timeout.ms            | 6000               | ZooKeeper session 超时时间。如果在此时间内server没有向zookeeper发送心跳，zookeeper就会认为此节点已挂掉。 此值太低导致节点容易被标记死亡；若太高，.会导致太迟发现节点死亡。 |
| zookeeper.connection.timeout.ms         | 6000               | 客户端连接zookeeper的超时时间。                              |
| zookeeper.sync.time.ms                  | 2000               | H ZK follower落后 ZK leader的时间。                          |
| controlled.shutdown.enable              | true               | 允许broker shutdown。如果启用，broker在关闭自己之前会把它上面的所有leaders转移到其它brokers上，建议启用，增加集群稳定性。 |
| auto.leader.rebalance.enable            | true               | If this is enabled the controller will automatically try to balance leadership for partitions among the brokers by periodically returning leadership to the “preferred” replica for each partition if it is available. |
| leader.imbalance.per.broker.percentage  | 10                 | The percentage of leader imbalance allowed per broker. The controller will rebalance leadership if this ratio goes above the configured value per broker. |
| leader.imbalance.check.interval.seconds | 300                | The frequency with which to check for leader imbalance.      |
| offset.metadata.max.bytes               | 4096               | The maximum amount of metadata to allow clients to save with their offsets. |
| connections.max.idle.ms                 | 600000             | Idle connections timeout: the server socket processor threads close the connections that idle more than this. |
| num.recovery.threads.per.data.dir       | 1                  | The number of threads per data directory to be used for log recovery at startup and flushing at shutdown. |
| unclean.leader.election.enable          | true               | Indicates whether to enable replicas not in the ISR set to be elected as leader as a last resort, even though doing so may result in data loss. |
| delete.topic.enable                     | false              | 启用deletetopic参数，建议设置为true。                        |
| offsets.topic.num.partitions            | 50                 | The number of partitions for the offset commit topic. Since changing this after deployment is currently unsupported, we recommend using a higher setting for production (e.g., 100-200). |
| offsets.topic.retention.minutes         | 1440               | Offsets that are older than this age will be marked for deletion. The actual purge will occur when the log cleaner compacts the offsets topic. |
| offsets.retention.check.interval.ms     | 600000             | The frequency at which the offset manager checks for stale offsets. |
| offsets.topic.replication.factor        | 3                  | The replication factor for the offset commit topic. A higher setting (e.g., three or four) is recommended in order to ensure higher availability. If the offsets topic is created when fewer brokers than the replication factor then the offsets topic will be created with fewer replicas. |
| offsets.topic.segment.bytes             | 104857600          | Segment size for the offsets topic. Since it uses a compacted topic, this should be kept relatively low in order to facilitate faster log compaction and loads. |
| offsets.load.buffer.size                | 5242880            | An offset load occurs when a broker becomes the offset manager for a set of consumer groups (i.e., when it becomes a leader for an offsets topic partition). This setting corresponds to the batch size (in bytes) to use when reading from the offsets segments when loading offsets into the offset manager’s cache. |
| offsets.commit.required.acks            | -1                 | The number of acknowledgements that are required before the offset commit can be accepted. This is similar to the producer’s acknowledgement setting. In general, the default should not be overridden. |
| offsets.commit.timeout.ms               | 5000               | The offset commit will be delayed until this timeout or the required number of replicas have received the offset commit. This is similar to the producer request timeout. |



## producer

| **属性**                           | **默认值**                        | **描述**                                                     |
| ---------------------------------- | --------------------------------- | ------------------------------------------------------------ |
| metadata.broker.list               |                                   | 启动时producer查询brokers的列表，可以是集群中所有brokers的一个子集。注意，这个参数只是用来获取topic的元信息用，producer会从元信息中挑选合适的broker并与之建立socket连接。格式是：host1:port1,host2:port2。 |
| request.required.acks              | 0                                 | 参见3.2节介绍                                                |
| request.timeout.ms                 | 10000                             | Broker等待ack的超时时间，若等待时间超过此值，会返回客户端错误信息。 |
| producer.type                      | sync                              | 同步异步模式。async表示异步，sync表示同步。如果设置成异步模式，可以允许生产者以batch的形式push数据，这样会极大的提高broker性能，推荐设置为异步。 |
| serializer.class                   | kafka.serializer.DefaultEncoder   | 序列号类，.默认序列化成 byte[] 。                            |
| key.serializer.class               |                                   | Key的序列化类，默认同上。                                    |
| partitioner.class                  | kafka.producer.DefaultPartitioner | Partition类，默认对key进行hash。                             |
| compression.codec                  | none                              | 指定producer消息的压缩格式，可选参数为： “none”, “gzip” and “snappy”。关于压缩参见4.1节 |
| compressed.topics                  | null                              | 启用压缩的topic名称。若上面参数选择了一个压缩格式，那么压缩仅对本参数指定的topic有效，若本参数为空，则对所有topic有效。 |
| message.send.max.retries           | 3                                 | Producer发送失败时重试次数。若网络出现问题，可能会导致不断重试。 |
| retry.backoff.ms                   | 100                               | Before each retry, the producer refreshes the metadata of relevant topics to see if a new leader has been elected. Since leader election takes a bit of time, this property specifies the amount of time that the producer waits before refreshing the metadata. |
| topic.metadata.refresh.interval.ms | 600 * 1000                        | The producer generally refreshes the topic metadata from brokers when there is a failure (partition missing, leader not available…). It will also poll regularly (default: every 10min so 600000ms). If you set this to a negative value, metadata will only get refreshed on failure. If you set this to zero, the metadata will get refreshed after each message sent (not recommended). Important note: the refresh happen only AFTER the message is sent, so if the producer never sends a message the metadata is never refreshed |
| queue.buffering.max.ms             | 5000                              | 启用异步模式时，producer缓存消息的时间。比如我们设置成1000时，它会缓存1秒的数据再一次发送出去，这样可以极大的增加broker吞吐量，但也会造成时效性的降低。 |
| queue.buffering.max.messages       | 10000                             | 采用异步模式时producer buffer 队列里最大缓存的消息数量，如果超过这个数值，producer就会阻塞或者丢掉消息。 |
| queue.enqueue.timeout.ms           | -1                                | 当达到上面参数值时producer阻塞等待的时间。如果值设置为0，buffer队列满时producer不会阻塞，消息直接被丢掉。若值设置为-1，producer会被阻塞，不会丢消息。 |
| batch.num.messages                 | 200                               | 采用异步模式时，一个batch缓存的消息数量。达到这个数量值时producer才会发送消息。 |
| send.buffer.bytes                  | 100 * 1024                        | Socket write buffer size                                     |
| client.id                          | “”                                | The client id is a user-specified string sent in each request to help trace calls. It should logically identify the application making the request. |



## consumer

| **属性**                        | **默认值**  | **描述**                                                     |
| ------------------------------- | ----------- | ------------------------------------------------------------ |
| group.id                        |             | Consumer的组ID，相同goup.id的consumer属于同一个组。          |
| zookeeper.connect               |             | Consumer的zookeeper连接串，要和broker的配置一致。            |
| consumer.id                     | null        | 如果不设置会自动生成。                                       |
| socket.timeout.ms               | 30 * 1000   | 网络请求的socket超时时间。实际超时时间由max.fetch.wait + socket.timeout.ms 确定。 |
| socket.receive.buffer.bytes     | 64 * 1024   | The socket receive buffer for network requests.              |
| fetch.message.max.bytes         | 1024 * 1024 | 查询topic-partition时允许的最大消息大小。consumer会为每个partition缓存此大小的消息到内存，因此，这个参数可以控制consumer的内存使用量。这个值应该至少比server允许的最大消息大小大，以免producer发送的消息大于consumer允许的消息。 |
| num.consumer.fetchers           | 1           | The number fetcher threads used to fetch data.               |
| auto.commit.enable              | true        | 如果此值设置为true，consumer会周期性的把当前消费的offset值保存到zookeeper。当consumer失败重启之后将会使用此值作为新开始消费的值。 |
| auto.commit.interval.ms         | 60 * 1000   | Consumer提交offset值到zookeeper的周期。                      |
| queued.max.message.chunks       | 2           | 用来被consumer消费的message chunks 数量， 每个chunk可以缓存fetch.message.max.bytes大小的数据量。 |
| auto.commit.interval.ms         | 60 * 1000   | Consumer提交offset值到zookeeper的周期。                      |
| queued.max.message.chunks       | 2           | 用来被consumer消费的message chunks 数量， 每个chunk可以缓存fetch.message.max.bytes大小的数据量。 |
| fetch.min.bytes                 | 1           | The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request. |
| fetch.wait.max.ms               | 100         | The maximum amount of time the server will block before answering the fetch request if there isn’t sufficient data to immediately satisfy fetch.min.bytes. |
| rebalance.backoff.ms            | 2000        | Backoff time between retries during rebalance.               |
| refresh.leader.backoff.ms       | 200         | Backoff time to wait before trying to determine the leader of a partition that has just lost its leader. |
| auto.offset.reset               | largest     | What to do when there is no initial offset in ZooKeeper or if an offset is out of range ;smallest : automatically reset the offset to the smallest offset; largest : automatically reset the offset to the largest offset;anything else: throw exception to the consumer |
| consumer.timeout.ms             | -1          | 若在指定时间内没有消息消费，consumer将会抛出异常。           |
| exclude.internal.topics         | true        | Whether messages from internal topics (such as offsets) should be exposed to the consumer. |
| zookeeper.session.timeout.ms    | 6000        | ZooKeeper session timeout. If the consumer fails to heartbeat to ZooKeeper for this period of time it is considered dead and a rebalance will occur. |
| zookeeper.connection.timeout.ms | 6000        | The max time that the client waits while establishing a connection to zookeeper. |
| zookeeper.sync.time.ms          | 2000        | How far a ZK follower can be behind a ZK leader              |



# 面试



## kafka的Balance如何处理

- Kafka的数据是分区存储的，以集群形式运行的Kafka，这些分区是分布在不同的Kafka服务器中
- 当消费者消费的数据分布在不同的分区时，会访问不同的服务器，这样就完成了负载均衡
- Kafka的负载均衡是通过分区机制实现的

![1569129757052](img/kafka/21.png)





## kafka的偏移量Offset存储在哪里

- Kafka0.9版本以前，offset默认保存在Zookeeper中
- 从kafka-0.9版本及以后，kafka的消费者组和offset信息存到broker服务器上
- 变动的原因
  - 之前版本Kafka其实存在一个比较大的隐患
    - 利用 Zookeeper 来存储记录每个消费者/组的消费进度
    - 虽然在使用过程当中，JVM帮助我们完成了一些优化
    - 但是消费者需要频繁的去与 Zookeeper 进行交互
  - 利用ZKClient的API操作Zookeeper频繁的Write其本身就是一个比较低效的Action
    - 对于后期水平扩展也是一个比较头疼的问题
    - 如果期间 Zookeeper 集群发生变化，那 Kafka 集群的吞吐量也跟着受影响

![1569129967337](img/kafka/22.png)

## 为什么kafka可实现高吞吐

- 分区存储
  - 提高并发

## 为什么单节点kafka的吞吐量也比其他消息队列大

- 磁盘顺序存储
  - 顺序读写
  - kafka的消息是不断追加到文件中的，这个特性使kafka可以充分利用磁盘的顺序读写性能，顺序读写不需要硬盘磁头的寻道时间，只需很少的扇区旋转时间，所以速度远快于随机读写

![1569130081658](img/kafka/23.png)

- 零拷贝
  - 参考https://blog.csdn.net/stark_summer/article/details/50144591
  - 在Linux kernel2.2 之后出现了一种叫做"零拷贝(zero-copy)"系统调用机制，就是跳过“用户缓冲区”的拷贝，建立一个磁盘空间和内存的直接映射，数据不再复制到“用户态缓冲区”，系统上下文切换减少为2次，可以提升一倍的性能

传统的网络I/O操作流程，大体上分为以下4步

1. OS从硬盘把数据读到内核区的PageCache
2. 用户进程把数据从内核区Copy到用户区。
3. 然后用户进程再把数据写入到Socket，数据流入内核区的Socket Buffer上。
4. OS再把数据从Buffer中Copy到网卡的Buffer上，这样完成一次发送


![img](img/kafka/24.png)

整个过程共经历两次Context Switch，四次System Call。同一份数据在内核Buffer与用户Buffer之间重复拷贝，效率低下。其中2、3两步没有必要，完全可以直接在内核区完成数据拷贝

![img](img/kafka/25.png)

Kafka的设计初衷是尽一切努力在内存中完成数据交换，无论是对外作为一整个消息系统，或是内部同底层操作系统的交互。如果Producer和Consumer之间生产和消费进度上配合得当，完全可以实现数据交换零I/O。这也就是为什么说Kafka使用“硬盘”并没有带来过多性能损失的原因

- 文件分段
  - kafka的队列topic被分为了多个区partition，每个partition又分为多个段segment，所以一个队列中的消息实际上是保存在N多个片段文件中，通过分段的方式，每次文件操作都是对一个小文件的操作，非常轻便，同时也增加了并行处理能力

![1569130972791](img/kafka/26.png)

![1569130999076](img/kafka/27.png)



## kafka消费过的数据如何再消费

- 修改offset
  - Kafka消息队列中消费过的数据是用offset标记的
  - 通过修改offset到以前消费过的位置，可以实现数据的重复消费
  - 通过使用不同的group来消费
    - Kafka中不同的消费者组的offset是独立的
    - 通过不同的消费者组实现数据的重复消费
    - 需要将`ConsumerConfig.AUTO_OFFSET_RESET_CONFIG`属性修改为`earliest`
      - 对offset进行重置读取
      - 如果找不到原先的offset（存储的是groupId:partition:offset），则从最开始的offset开始
        - 正常情况下每次读取消息会修改offset值
      - 如果原先有offset，然后设置该值，那么会从上次保存的offset进行读取