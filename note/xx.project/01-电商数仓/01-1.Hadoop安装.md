# 虚拟机环境准备

- 虚拟机
  - hadoop102-内存16G，其他-内存4G
  - 磁盘50G
  - 核心数2，线程数4
- 克隆虚拟机
- 修改克隆虚拟机的静态IP
  - 将eth1改为eth0，将原先的eth0删除
  - 更改ifcfg-eth0的HWADDR=eth0的ATTR的值
- 修改主机名
- 关闭防火墙
- 创建ttshe用户
- 配置ttshe用户具有root权限
- 在/opt下创建module，software文件夹
- 安装JDK
- 重启

```bash
[root@hadoop102 ~]$ vim /etc/udev/rules.d/70-persistent-net.rules
```

```bash
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?*", ATTR{address}=="00:0c:29:d1:ad:0c", ATTR{type}=="1", KERNEL=="eth*", NAME="eth0"
```

```bash
[root@hadoop102 ~]$ vim /etc/sysconfig/network-scripts/ifcfg-eth0
```

```bash
DEVICE=eth0
HWADDR=00:0c:29:d1:ad:0c
TYPE=Ethernet
UUID=403556d9-faec-4f65-8fc5-af4c24e47193
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=static
IPADDR=192.168.1.102
GATEWAY=192.168.1.2
DNS1=114.114.114.114
DNS2=8.8.8.8
```

```bash
[root@hadoop102 ~]$ vim /etc/sysconfig/network
```

```bash
NETWORKING=yes
HOSTNAME=hadoop102
```

```bash
[root@hadoop102 ~]# vim /etc/hosts
```

```bash
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.1.100 hadoop100
192.168.1.101 hadoop101
192.168.1.102 hadoop102
192.168.1.103 hadoop103
192.168.1.104 hadoop104
192.168.1.105 hadoop105
192.168.1.106 hadoop106
192.168.1.107 hadoop107
192.168.1.108 hadoop108
```

```bash
[root@hadoop102 ~]# chkconfig iptables --list
[root@hadoop102 ~]# chkconfig iptables off
```

```bash
[ttshe@hadoop102 opt]$ mkdir module
mkdir: 无法创建目录"module": 权限不够
[ttshe@hadoop102 opt]$ sudo mkdir module software
[sudo] password for ttshe: 
[ttshe@hadoop102 opt]$ ll
总用量 12
drwxr-xr-x. 2 root root 4096 4月   7 11:44 module
drwxr-xr-x. 2 root rootre 4096 10月  4 2017 rh
drwxr-xr-x. 2 root root 4096 4月   7 11:44 software
# 此时创建成功，但是所属主仍然是root，需要修改这2个文件的权限
[ttshe@hadoop102 opt]$ sudo chown ttshe:ttshe -R module/ software/
[ttshe@hadoop102 opt]$ ll
总用量 12
drwxr-xr-x. 2 ttshe ttshe 4096 4月   7 11:44 module
drwxr-xr-x. 2 root  root  4096 10月  4 2017 rh
drwxr-xr-x. 2 ttshe ttshe 4096 4月   7 11:44 software
```

```bash
[root@hadoop102 opt]# rpm -qa | grep java
[root@hadoop102 opt]# sudo rpm -e xxx
```

```bash
[root@hadoop102 opt]# vim /etc/profile
```

```bash
# JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_144
export PATH=$PATH:$JAVA_HOME/bin
```

```bash
[root@hadoop102 opt]# source /etc/profile
```

- 如果要给其他的服务器同步服务需要使用scp

```bash
sudo scp /etc/profile root@hadoop103:/etc/profile
```

- 其余2台主机按上述配置，注意变化主机名



# 安装Hadoop

- 下载地址https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/
- 用SecureCRT工具将hadoop-2.7.2.tar.gz导入到opt目录下面的software文件夹下面
- 切换到sftp连接页面，选择Linux下编译的hadoop jar包拖入
- 解压
- 添加Hadoop到环境变量

```bash
[ttshe@hadoop102 software]$ tar -zvxf hadoop-2.7.2.tar.gz -C /opt/module/

[ttshe@hadoop102 hadoop-2.7.2]$ pwd
/opt/module/hadoop-2.7.2
[ttshe@hadoop102 hadoop-2.7.2]$ sudo vim /etc/profile

# 添加 HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-2.7.2
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

[ttshe@hadoop102 hadoop-2.7.2]$ source /etc/profile
[ttshe@hadoop102 hadoop-2.7.2]$ hadoop version
```



## 分发脚本 xsync

- 在/home/ttshe目录下创建bin目录
- 在bin目录下xsync创建文件

```bash
[ttshe@hadoop102 home]$ cd ttshe
[ttshe@hadoop102 ~]$ mkdir bin
[ttshe@hadoop102 ~]$ cd bin/
[ttshe@hadoop102 bin]$ vim xsync
```

```bash
#!/bin/bash
#1 获取输入参数个数，如果没有参数，直接退出
pcount=$#
if((pcount==0)); then
echo no args;
exit;
fi

#2 获取文件名称
p1=$1
fname=`basename $p1`
echo fname=$fname

#3 获取上级目录到绝对路径
pdir=`cd -P $(dirname $p1); pwd`
echo pdir=$pdir

#4 获取当前用户名称
user=`whoami`

#5 循环
for((host=103; host<105; host++)); do
        echo ------------------- hadoop$host --------------
        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir
done
```

- 修改脚本 xsync 具有执行权限
  - 如将xsync放到/home/atguigu/bin目录下仍然不能实现全局使用，可将xsync移动到/usr/local/bin目录下

```bash
[ttshe@hadoop102 bin]$ chmod 777 xsync
```



## 配置SSH

- 生成公钥和私钥，然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）
- 将公钥拷贝到要免密登录的目标机器上

```bash
[ttshe@hadoop102 /]$ ssh-keygen -t rsa
[ttshe@hadoop102 /]$ ssh-copy-id hadoop102
[ttshe@hadoop102 /]$ ssh-copy-id hadoop103
[ttshe@hadoop102 /]$ ssh-copy-id hadoop104

[ttshe@hadoop103 /]$ ssh-keygen -t rsa
[ttshe@hadoop103 /]$ ssh-copy-id hadoop102
[ttshe@hadoop103 /]$ ssh-copy-id hadoop103
[ttshe@hadoop103 /]$ ssh-copy-id hadoop104

[ttshe@hadoop104 /]$ ssh-keygen -t rsa
[ttshe@hadoop104 /]$ ssh-copy-id hadoop102
[ttshe@hadoop104 /]$ ssh-copy-id hadoop103
[ttshe@hadoop104 /]$ ssh-copy-id hadoop104
```



## 集群配置



### 规划

|      | hadoop102              | hadoop103                        | hadoop104                       |
| ---- | ---------------------- | -------------------------------- | ------------------------------- |
| HDFS | NameNode<br />DataNode | DataNode                         | SecondaryNameNode<br />DataNode |
| YARN | NodeManager            | ResourceManager<br />NodeManager | NodeManager                     |



### core-site.xml

```bash
[ttshe@hadoop102 hadoop]$ pwd
/opt/module/hadoop-2.7.2/etc/hadoop
[ttshe@hadoop102 hadoop]$ vim core-site.xml
```

```xml
<!-- 指定HDFS中NameNode的地址 -->
<property>
		<name>fs.defaultFS</name>
      <value>hdfs://hadoop102:9000</value>
</property>

<!-- 指定Hadoop运行时产生文件的存储目录 -->
<property>
		<name>hadoop.tmp.dir</name>
		<value>/opt/module/hadoop-2.7.2/data/tmp</value>
</property>
```



### hadoop-env.sh

```bash
[ttshe@hadoop102 hadoop]$ vim hadoop-env.sh
# 修改jdk
export JAVA_HOME=/opt/module/jdk1.8.0_144
```



### hdfs-site.xml

- 注意，测试环境主机磁盘存储不够，可以设置副本集合为1

```bash
[ttshe@hadoop102 hadoop]$ vim hdfs-site.xml 
```

```xml
<property>
		<name>dfs.replication</name>
		<value>1</value>
</property>

<!-- 指定Hadoop辅助名称节点主机配置 -->
<property>
      <name>dfs.namenode.secondary.http-address</name>
      <value>hadoop104:50090</value>
</property>
```



### yarn-env.sh

```bash
[ttshe@hadoop102 hadoop]$ vim yarn-env.sh 
# 添加
export JAVA_HOME=/opt/module/jdk1.8.0_144
```



### yarn-site.xml

```bash
[ttshe@hadoop102 hadoop]$ vim yarn-site.xml
```

```xml
<!-- Reducer获取数据的方式 -->
<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
</property>

<!-- 指定YARN的ResourceManager的地址 -->
<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>hadoop103</value>
</property>

<!-- 日志聚集功能 -->
<property>
	<name>yarn.log-aggregation-enable</name>
	<value>true</value>
</property>
<!-- 日志保留时间设置7天 -->
<property>
	<name>yarn.log-aggregation.retain-seconds</name>
	<value>604800</value>
</property>
```



### mapred-env.sh

```bash
[ttshe@hadoop102 hadoop]$ vim mapred-env.sh 
# 增加
export JAVA_HOME=/opt/module/jdk1.8.0_144
```



### mapred-site.xml

```bash
[ttshe@hadoop102 hadoop]$ cp mapred-site.xml.template mapred-site.xml
[ttshe@hadoop102 hadoop]$ vim mapred-site.xml
```

```xml
<!-- 指定MR运行在Yarn上 -->
<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
</property>
<!-- 历史服务器端地址 -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop102:10020</value>
</property>
<!-- 历史服务器web端地址 -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop102:19888</value>
</property>
```

 

### slaves

- 该文件中添加的内容结尾不允许有空格，文件中不允许有空行
- slaves文件作为参数传入

```bash
[ttshe@hadoop102 hadoop]$ vim slaves
[ttshe@hadoop102 hadoop]$ cat slaves
hadoop102
hadoop103
hadoop104
```



### 分发配置

```bash
[ttshe@hadoop102 /]$ xsync /opt/module/hadoop-2.7.2/
```



## 启动集群

- 格式化namenode

```bash
[ttshe@hadoop102 hadoop-2.7.2]$ bin/hdfs namenode -format
[ttshe@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh 
[ttshe@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh
```



## 支持LZO压缩配置

- 先下载lzo的jar项目
- https://github.com/twitter/hadoop-lzo/archive/master.zip
- 下载后的文件名是hadoop-lzo-master，它是一个zip格式的压缩包，先进行解压，然后用maven编译。生成hadoop-lzo-0.4.20.jar
- 将编译好后的hadoop-lzo-0.4.20.jar 放入hadoop-2.7.2/share/hadoop/common/
- 同步hadoop-lzo-0.4.20.jar到hadoop103、hadoop104
- core-site.xml增加配置支持LZO压缩
- 同步core-site.xml到hadoop103、hadoop104
- 启动及查看集群
  - web和进程查看
    - Web查看：http://hadoop102:50070
    - 进程查看：jps查看各个节点状态。
  - 当启动发生错误的时候：
    - 查看日志：/home/atguigu/module/hadoop-2.7.2/logs
    - 如果进入安全模式，可以通过hdfs dfsadmin -safemode leave
    - 停止所有进程，删除data和log文件夹，然后hdfs namenode -format 来格式化

```bash
[ttshe@hadoop102 common]$ pwd
/opt/module/hadoop-2.7.2/share/hadoop/common
[ttshe@hadoop102 software]$ mv hadoop-lzo-0.4.20.jar /opt/module/hadoop-2.7.2/share/hadoop/common/
[ttshe@hadoop102 hadoop]$ vim core-site.xml
```

- 增加如下配置

```xml
<property>
<name>io.compression.codecs</name>
<value>
    org.apache.hadoop.io.compress.GzipCodec,
    org.apache.hadoop.io.compress.DefaultCodec,
    org.apache.hadoop.io.compress.BZip2Codec,
    org.apache.hadoop.io.compress.SnappyCodec,
    com.hadoop.compression.lzo.LzoCodec,
    com.hadoop.compression.lzo.LzopCodec
</value>
</property>
<property>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
</property>
```

```bash
[ttshe@hadoop102 hadoop]$ xsync core-site.xml
```



