# 产生数据测试

- 安装配置完成flume消费和flume生产节点
- 产生数据查看情况
- 先清除旧数据，再产生新数据

```bash
[ttshe@hadoop102 bin]$ xcall.sh rm -rf /tmp/logs/
[ttshe@hadoop102 flume]$ lg.sh
```

- 查看hadoop104上的flume的生产日志

```bash
[ttshe@hadoop104 flume]$ tail -f log.txt 
...
19/10/05 11:30:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1570245829954, queueSize: 492, queueHead: 999998
19/10/05 11:30:18 INFO file.Log: Updated checkpoint for file: /opt/module/flume/data/behavior2/log-1 position: 595356 logWriteOrderID: 1570245829954
19/10/05 11:30:18 INFO file.EventQueueBackingStoreFile: Start checkpoint for /opt/module/flume/checkpoint/behavior1/checkpoint, elements to sync = 501
19/10/05 11:30:18 INFO file.EventQueueBackingStoreFile: Updating checkpoint metadata: logWriteOrderID: 1570245829955, queueSize: 0, queueHead: 502
19/10/05 11:30:18 INFO file.Log: Updated checkpoint for file: /opt/module/flume/data/behavior1/log-1 position: 246411 logWriteOrderID: 1570245829955
19/10/05 11:30:19 INFO hdfs.HDFSCompressedDataStream: Serializer = TEXT, UseRawLocalFileSystem = false
19/10/05 11:30:19 INFO hdfs.BucketWriter: Creating /origin_data/gmall/log/topic_event/2019-10-05/logevent-.1570246219388.lzo.tmp
19/10/05 11:30:23 INFO hdfs.BucketWriter: Closing /origin_data/gmall/log/topic_start/2019-10-05/logstart-.1570246213693.lzo.tmp
19/10/05 11:30:23 INFO hdfs.BucketWriter: Renaming /origin_data/gmall/log/topic_start/2019-10-05/logstart-.1570246213693.lzo.tmp to /origin_data/gmall/log/topic_start/2019-10-05/logstart-.1570246213693.lzo
19/10/05 11:30:23 INFO hdfs.HDFSEventSink: Writer callback called
```

- ==关于启动flume失败的情况==
  - 每个节点/etc/profile需要一致，并且要执行以下source
    - 同时每个节点要执行`cat /etc/profile >> ~/.bashrc`
- 关闭集群，修改时间，再启动集群
  - 每次修改完时间，创建生产日志lg.sh
  - 如果启动失败，清除组件的元数据，重启

