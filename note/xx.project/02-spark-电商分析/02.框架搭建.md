# 父工程项目

- pom

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.stt.spark</groupId>
    <artifactId>spark-mall</artifactId>
    <packaging>pom</packaging>
    <version>1.0-SNAPSHOT</version>
    <modules>
        <module>common</module>
        <module>offline</module>
        <module>realtime</module>
        <module>mock</module>
    </modules>

    <properties>
        <spark.version>2.1.1</spark.version>
        <scala.version>2.11.8</scala.version>
        <log4j.version>1.2.17</log4j.version>
        <slf4j.version>1.7.22</slf4j.version>
    </properties>

    <dependencies>
        <!--此处放日志包，所有项目都要引用-->
        <!-- 所有子项目的日志框架 -->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>jcl-over-slf4j</artifactId>
            <version>${slf4j.version}</version>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>${slf4j.version}</version>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
            <version>${slf4j.version}</version>
        </dependency>
        <!-- 具体的日志实现 -->
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
            <version>${log4j.version}</version>
        </dependency>
    </dependencies>

    <dependencyManagement>
        <!--此处声明工具依赖，各个模块，选择使用-->
        <dependencies>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-core_2.11</artifactId>
                <version>${spark.version}</version>
                <!-- provider如果存在，那么运行时该Jar包不存在，也不会打包到最终的发布版本中，只是编译器有效 -->
                <!--<scope>provided</scope>-->
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-sql_2.11</artifactId>
                <version>${spark.version}</version>
                <!--<scope>provided</scope>-->
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-streaming_2.11</artifactId>
                <version>${spark.version}</version>
                <!--<scope>provided</scope>-->
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-mllib_2.11</artifactId>
                <version>${spark.version}</version>
                <!--<scope>provided</scope>-->
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-graphx_2.11</artifactId>
                <version>${spark.version}</version>
                <!--<scope>provided</scope>-->
            </dependency>
            <dependency>
                <groupId>org.scala-lang</groupId>
                <artifactId>scala-library</artifactId>
                <version>${scala.version}</version>
                <!--<scope>provided</scope>-->
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-hive_2.11</artifactId>
                <version>${spark.version}</version>
                <!--<scope>provided</scope>-->
            </dependency>
            <dependency>
                <groupId>org.apache.spark</groupId>
                <artifactId>spark-streaming-kafka-0-10_2.11</artifactId>
                <version>${spark.version}</version>
            </dependency>
        </dependencies>
    </dependencyManagement>

    <build>
        <!-- 声明并引入子项目共有的插件【插件就是附着到Maven各个声明周期的具体实现】 -->
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.6.1</version>
                <!-- 所有的编译都依照JDK1.8来搞 -->
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
        </plugins>

        <!-- 仅声明子项目共有的插件，如果子项目需要此插件，那么子项目需要声明 -->
        <pluginManagement>
            <plugins>
                <!-- 该插件用于将Scala代码编译成class文件 -->
                <!--直接创建scala的编译环境，不用勾选add FrameWorkSupport-->
                <plugin>
                    <groupId>net.alchim31.maven</groupId>
                    <artifactId>scala-maven-plugin</artifactId>
                    <version>3.2.2</version>
                    <executions>
                        <execution>
                            <!-- 声明绑定到maven的compile阶段 -->
                            <goals>
                                <goal>compile</goal>
                                <goal>testCompile</goal>
                            </goals>
                        </execution>
                    </executions>
                </plugin>

                <!-- 用于项目的打包插件 -->
                <plugin>
                    <groupId>org.apache.maven.plugins</groupId>
                    <artifactId>maven-assembly-plugin</artifactId>
                    <version>3.0.0</version>
                    <executions>
                        <execution>
                            <id>make-assembly</id>
                            <phase>package</phase>
                            <goals>
                                <goal>single</goal>
                            </goals>
                        </execution>
                    </executions>
                </plugin>
            </plugins>
        </pluginManagement>
    </build>
</project>
```



# 创建公共模块



## pom

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <artifactId>spark-mall</artifactId>
        <groupId>com.stt.spark</groupId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>common</artifactId>


    <dependencies>

        <!-- https://mvnrepository.com/artifact/com.alibaba/fastjson -->
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.47</version>
        </dependency>

        <!-- common-pool2使用的是面向接口的编程，它为我们提供的是一个抽象的对象池管理方式，
             根据我们业务的不同，我们需要重写或实现一些方法和接口 -->
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-pool2</artifactId>
            <version>2.4.2</version>
        </dependency>

        <dependency>
            <!-- 有助于以各种格式读取配置/偏好文件的工具 -->
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-configuration2</artifactId>
            <version>2.2</version>
        </dependency>

        <!-- commons-beanutil中包括大量和JavaBean操作有关的工具方法，
             使用它能够轻松利用Java反射机制来完毕代码中所须要的功能，而不须要具体研究反射的原理和使用 -->
        <dependency>
            <groupId>commons-beanutils</groupId>
            <artifactId>commons-beanutils</artifactId>
            <version>1.9.3</version>
        </dependency>

        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.47</version>
        </dependency>

        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>0.10.2.1</version>
        </dependency>
        
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
        </dependency>
        
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.11</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_2.11</artifactId>
        </dependency>
        <!-- https://mvnrepository.com/artifact/redis.clients/jedis -->
        <dependency>
            <groupId>redis.clients</groupId>
            <artifactId>jedis</artifactId>
            <version>2.9.0</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/com.alibaba/druid -->
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>druid</artifactId>
            <version>1.1.10</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/net.sf.json-lib/json-lib -->
        <dependency>
            <groupId>net.sf.json-lib</groupId>
            <artifactId>json-lib</artifactId>
            <version>2.4</version>
            <classifier>jdk15</classifier>
        </dependency>

        <!--json 转 scala对象-->
        <!-- https://mvnrepository.com/artifact/org.json4s/json4s-native -->
        <dependency>
            <groupId>org.json4s</groupId>
            <artifactId>json4s-native_2.11</artifactId>
            <version>3.2.11</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.json4s/json4s-jackson -->
        <dependency>
            <groupId>org.json4s</groupId>
            <artifactId>json4s-jackson_2.11</artifactId>
            <version>3.2.11</version>
        </dependency>

    </dependencies>
    <build>
        <plugins>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>

</project>
```



## 配置文件



### log4j.properties

```properties
log4j.rootLogger=error, stdout,R
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS}  %5p --- [%50t]  %-80c(line:%5L)  :  %m%n

log4j.appender.R=org.apache.log4j.RollingFileAppender
log4j.appender.R.File=../log/agent.log
log4j.appender.R.MaxFileSize=1024KB
log4j.appender.R.MaxBackupIndex=1

log4j.appender.R.layout=org.apache.log4j.PatternLayout
log4j.appender.R.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS}  %5p --- [%50t]  %-80c(line:%6L)  :  %m%n
```



### config.properties

```properties
jdbc.datasource.size=10
jdbc.driver.class=com.mysql.jdbc.Driver
jdbc.url=jdbc:mysql://hadoop102:3306/sparkmall?useUnicode=true&characterEncoding=utf8&rewriteBatchedStatements=true
jdbc.user=root
jdbc.password=123456

# Kafka配置
kafka.broker.list=hadoop102:9092,hadoop103:9092,hadoop104:9092

# Redis配置
redis.host=hadoop102
redis.port=6379

# hive 的数据库名（选配）
hive.database=sparkmall
```



### condition.properties

```properties
jdbc.datasource.size=10
jdbc.driver.class=com.mysql.jdbc.Driver
jdbc.url=jdbc:mysql://hadoop102:3306/sparkmall?useUnicode=true&characterEncoding=utf8&rewriteBatchedStatements=true
jdbc.user=root
jdbc.password=123456

# Kafka配置
kafka.broker.list=hadoop102:9092,hadoop103:9092,hadoop104:9092

# Redis配置
redis.host=hadoop102
redis.port=6379

# hive 的数据库名（选配）
hive.database=sparkmall
```



### hive-site.xml

用于idea连接虚拟机中的hive ，也可以不配置，那么spark在idea运行时，会自动在windows下建立仓库

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<configuration>
	<property>
	  <name>javax.jdo.option.ConnectionURL</name>
	  <value>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true</value>
	  <description>JDBC connect string for a JDBC metastore</description>
	</property>

	<property>
	  <name>javax.jdo.option.ConnectionDriverName</name>
	  <value>com.mysql.jdbc.Driver</value>
	  <description>Driver class name for a JDBC metastore</description>
	</property>

	<property>
	  <name>javax.jdo.option.ConnectionUserName</name>
	  <value>root</value>
	  <description>username to use against metastore database</description>
	</property>

	<property>
	  <name>javax.jdo.option.ConnectionPassword</name>
	  <value>123456</value>
	  <description>password to use against metastore database</description>
	</property>

    <property>
      <name>hive.zookeeper.quorum</name>
      <value>hadoop102,hadoop103,hadoop104</value>
      <description>The list of ZooKeeper servers to talk to. This is only needed for read/write locks.</description>
    </property>
    <property>
      <name>hive.zookeeper.client.port</name>
      <value>2181</value>
      <description>The port of ZooKeeper servers to talk to. This is only needed for read/write locks.</description>
    </property>

</configuration>
```

- 在hive中创建表

```bash
hive (default)> create database sparkmall;
```



## scala 代码



### ConfigurationUtil  

```scala
package com.stt.spark.mall.common

import java.util.ResourceBundle

object ConfigurationUtil {

  // 之前做国际化使用，现在可以用于读取properties文件
  private val rb = ResourceBundle.getBundle("config")

  /**
    * 依据key获取配置文件的value
    * @param key
    * @return
    */
  def getValueByKey(key: String): String = {
    // 当前线程环境的类加载器，一般是 应用类加载器
    // Thread.currentThread().getContextClassLoader.getResourceAsStream("config.properties")
    rb.getString(key)
  }

  def getValueByKeyFrom(config:String,key:String):String = {
    ResourceBundle.getBundle(config).getString(key)
  }

}
```



### DataModel

```scala
package com.stt.spark.mall.model

//***************** 输入表 *********************

/**
  * 用户访问动作表
  *
  * @param date               用户点击行为的日期
  * @param user_id            用户的ID
  * @param session_id         Session的ID
  * @param page_id            某个页面的ID
  * @param action_time        点击行为的时间点
  * @param search_keyword     用户搜索的关键词
  * @param click_category_id  某一个商品品类的ID
  * @param click_product_id   某一个商品的ID
  * @param order_category_ids 一次订单中所有品类的ID集合
  * @param order_product_ids  一次订单中所有商品的ID集合
  * @param pay_category_ids   一次支付中所有品类的ID集合
  * @param pay_product_ids    一次支付中所有商品的ID集合
  */
case class UserVisitAction(date: String,
                           user_id: Long,
                           session_id: String,
                           page_id: Long,
                           action_time: String,
                           search_keyword: String,
                           click_category_id: Long,
                           click_product_id: Long,
                           order_category_ids: String,
                           order_product_ids: String,
                           pay_category_ids: String,
                           pay_product_ids: String,
                           city_id:Long
                          )

/**
  * 用户信息表
  *
  * @param user_id      用户的ID
  * @param username     用户的名称
  * @param name         用户的名字
  * @param age          用户的年龄
  * @param professional 用户的职业
  * @param gender          用户的性别
  */
case class UserInfo(user_id: Long,
                    username: String,
                    name: String,
                    age: Int,
                    professional: String,
                    gender: String
                   )

/**
  * 产品表
  *
  * @param product_id   商品的ID
  * @param product_name 商品的名称
  * @param extend_info  商品额外的信息
  */
case class ProductInfo(product_id: Long,
                       product_name: String,
                       extend_info: String
                      )

case class CityInfo (city_id:Long,
                     city_name:String,
                     area:String
                    )
```



# 模拟数据模块



## pom

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <artifactId>spark-mall</artifactId>
        <groupId>com.stt.spark</groupId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>mock</artifactId>

    <dependencies>
        <dependency>
            <groupId>com.stt.spark</groupId>
            <artifactId>common</artifactId>
            <version>1.0-SNAPSHOT</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.11</artifactId>
        </dependency>

    </dependencies>
    <build>
        <plugins>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>

</project>
```



## scala代码



### MockerOffline

```scala
package com.stt.spark.mock

import java.text.SimpleDateFormat
import java.util.UUID

import com.stt.spark.mall.common.ConfigurationUtil
import com.stt.spark.mall.model.{CityInfo, ProductInfo, UserInfo, UserVisitAction}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{SparkSession, _}

import scala.collection.mutable.ListBuffer



object MockerOffline {

    val userNum = 100;
    val productNum = 100
    val sessionNum = 10000

    val pageNum = 50
    val categoryNum = 20

    val logAboutNum = 100000 //日志大致数量，用于分布时间

    val professionRandomOpt = RandomOptions(RanOpt("学生", 40), RanOpt("程序员", 30), RanOpt("经理", 20), RanOpt("老师", 10))

    val genderRandomOpt = RandomOptions(RanOpt("男", 60), RanOpt("女", 40))
    val ageFrom = 10
    val ageTo = 59

    val productExRandomOpt = RandomOptions(RanOpt("自营", 70), RanOpt("第三方", 30))

    val searchKeywordsOptions = RandomOptions(RanOpt("手机", 30), RanOpt("笔记本", 70), RanOpt("内存", 70), RanOpt("i7", 70), RanOpt("苹果", 70), RanOpt("吃鸡", 70))
    val actionsOptions = RandomOptions(RanOpt("search", 20), RanOpt("click", 60), RanOpt("order", 6), RanOpt("pay", 4), RanOpt("quit", 10))


    def main(args: Array[String]): Unit = {

        val sparkConf = new SparkConf().setAppName("Mock").setMaster("local[*]")
        val sparkSession = SparkSession.builder().config(sparkConf).enableHiveSupport().getOrCreate()

        // 模拟数据
        val userVisitActionData = this.mockUserAction()
        val userInfoData = this.mockUserInfo()
        val productInfoData = this.mockProductInfo()
        val cityInfoData = this.mockCityInfo()

        // 将模拟数据装换为RDD
        val userVisitActionRdd = sparkSession.sparkContext.makeRDD(userVisitActionData)
        val userInfoRdd = sparkSession.sparkContext.makeRDD(userInfoData)
        val productInfoRdd = sparkSession.sparkContext.makeRDD(productInfoData)
        val cityInfoRdd = sparkSession.sparkContext.makeRDD(cityInfoData)

        import sparkSession.implicits._
        val userVisitActionDF = userVisitActionRdd.toDF()
        val userInfoDF = userInfoRdd.toDF()
        val productInfoDF = productInfoRdd.toDF()
        val cityInfoDF = cityInfoRdd.toDF()


        insertHive(sparkSession, "user_visit_action", userVisitActionDF)
        insertHive(sparkSession, "user_info", userInfoDF)
        insertHive(sparkSession, "product_info", productInfoDF)
        insertHive(sparkSession, "city_info", cityInfoDF)

        sparkSession.close()
    }

    def insertHive(sparkSession: SparkSession, tableName: String, dataFrame: DataFrame): Unit = {
        sparkSession.sql("use "+ ConfigurationUtil.getValueByKey("hive.database"))
        sparkSession.sql("drop table if exists " + tableName)
        dataFrame.write.saveAsTable(tableName)
        println("保存：" + tableName + "完成")
        sparkSession.sql("select * from " + tableName).show(100)

    }


    def mockUserInfo() = {


        val rows = new ListBuffer[UserInfo]()

        for (i <- 1 to userNum) {
            val user = UserInfo(i,
                                "user_" + i,
                                "name_" + i,
                                RandomNum(ageFrom, ageTo), //年龄
                                professionRandomOpt.getRandomOpt(),
                                genderRandomOpt.getRandomOpt()
                               )
            rows += user
        }
        rows.toList
    }

    def mockUserAction() = {

        val rows = new ListBuffer[UserVisitAction]()

        val startDate = new SimpleDateFormat("yyyy-MM-dd").parse("2018-11-26")
        val endDate = new SimpleDateFormat("yyyy-MM-dd").parse("2018-11-27")
        val randomDate = RandomDate(startDate, endDate, logAboutNum)
        for (i <- 1 to sessionNum) {
            val userId = RandomNum(1, userNum)
            val sessionId = UUID.randomUUID().toString
            var isQuit = false

            while (!isQuit) {
                val action = actionsOptions.getRandomOpt()

                if (action == "quit") {
                    isQuit = true
                } else {
                    val actionDateTime = randomDate.getRandomDate()
                    val actionDateString = new SimpleDateFormat("yyyy-MM-dd").format(actionDateTime)
                    val actionDateTimeString = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(actionDateTime)

                    var searchKeyword: String = null
                    var clickCategoryId: Long = -1L
                    var clickProductId: Long = -1L
                    var orderCategoryIds: String = null
                    var orderProductIds: String = null
                    var payCategoryIds: String = null
                    var payProductIds: String = null

                    var cityId: Long = RandomNum(1, 26).toLong

                    action match {
                        case "search" => searchKeyword = searchKeywordsOptions.getRandomOpt()
                        case "click" => clickCategoryId = RandomNum(1, categoryNum)
                        clickProductId = RandomNum(1, productNum)
                        case "order" => orderCategoryIds = RandomNum.multi(1, categoryNum, RandomNum(1, 5), ",", false)
                        orderProductIds = RandomNum.multi(1, categoryNum, RandomNum(1, 5), ",", false)
                        case "pay" => payCategoryIds = RandomNum.multi(1, categoryNum, RandomNum(1, 5), ",", false)
                        payProductIds = RandomNum.multi(1, categoryNum, RandomNum(1, 5), ",", false)
                    }

                    val userVisitAction = UserVisitAction(
                        actionDateString,
                        userId.toLong,
                        sessionId,
                        RandomNum(1, pageNum).toLong,
                        actionDateTimeString,
                        searchKeyword,
                        clickCategoryId.toLong,
                        clickProductId.toLong,
                        orderCategoryIds,
                        orderProductIds,
                        payCategoryIds,
                        payProductIds,
                        cityId
                    )
                    rows += userVisitAction
                }
            }

        }
        rows.toList
    }

    def mockProductInfo() = {
        val rows = new ListBuffer[ProductInfo]()
        for (i <- 1 to productNum) {
            val productInfo = ProductInfo(
                i,
                "商品_" + i,
                productExRandomOpt.getRandomOpt()
            )
            rows += productInfo
        }
        rows.toList
    }

    def mockCityInfo() = {
        List(CityInfo(1L, "北京", "华北"), CityInfo(2L, "上海", "华东"),
             CityInfo(3L, "深圳", "华南"), CityInfo(4L, "广州", "华南"),
             CityInfo(5L, "武汉", "华中"), CityInfo(6L, "南京", "华东"),
             CityInfo(7L, "天津", "华北"), CityInfo(8L, "成都", "西南"),
             CityInfo(9L, "哈尔滨", "东北"), CityInfo(10L, "大连", "东北"),
             CityInfo(11L, "沈阳", "东北"), CityInfo(12L, "西安", "西北"),
             CityInfo(13L, "长沙", "华中"), CityInfo(14L, "重庆", "西南"),
             CityInfo(15L, "济南", "华东"), CityInfo(16L, "石家庄", "华北"),
             CityInfo(17L, "银川", "西北"), CityInfo(18L, "杭州", "华东"),
             CityInfo(19L, "保定", "华北"), CityInfo(20L, "福州", "华南"),
             CityInfo(21L, "贵阳", "西南"), CityInfo(22L, "青岛", "华东"),
             CityInfo(23L, "苏州", "华东"), CityInfo(24L, "郑州", "华北"),
             CityInfo(25L, "无锡", "华东"), CityInfo(26L, "厦门", "华南")
            )
    }
}
```



### RandomDate

```scala
package com.stt.spark.mock

import java.util.{Date, Random}

object RandomDate {

    def apply(startDate:Date,endDate:Date,step:Int): RandomDate ={
        val randomDate = new RandomDate()
        val avgStepTime = (endDate.getTime- startDate.getTime)/step
        randomDate.maxTimeStep=avgStepTime*2
        randomDate.lastDateTime=startDate.getTime
        randomDate
    }


    class RandomDate{
        var lastDateTime =0L
        var maxTimeStep=0L

        def  getRandomDate()={
            val timeStep = new Random().nextInt(maxTimeStep.toInt)
            lastDateTime = lastDateTime+timeStep

            new Date( lastDateTime)
        }
    }
}
```



### RandomNum

```scala
package com.stt.spark.mock

import java.util.Random

import scala.collection.mutable

object RandomNum {

    def apply(fromNum:Int,toNum:Int): Int =  {
        fromNum+ new Random().nextInt(toNum-fromNum+1)
    }

    //  实现方法  在fromNum和 toNum之间的 多个数组拼接的字符串 共amount个
    //  用delimiter分割  canRepeat为false则不允许重复
    def multi(fromNum:Int,toNum:Int,amount:Int,delimiter:String,canRepeat:Boolean) ={

        if(toNum - fromNum + 1 < amount){
            throw new IllegalArgumentException("toNum - fromNum + 1 < amount")
        }
        var re = if(!canRepeat) mutable.Set[Int]() else mutable.ListBuffer[Int]()

        while(re.size < amount){
            re += RandomNum(fromNum,toNum)
        }
        re.map(_.toString).reduce(_+delimiter+_)
    }

    def main(args: Array[String]): Unit = {
        println(RandomNum.multi(1,5,2,",",true))
    }

}
```



### RandomOptions

```scala
package com.stt.spark.mock

import java.util.Random

import scala.collection.mutable.ListBuffer

object RandomOptions {

    def apply[T](opts:RanOpt[T]*): RandomOptions[T] ={
        val randomOptions=  new RandomOptions[T]()
        for (opt <- opts ) {
            randomOptions.totalWeight+=opt.weight
            for ( i <- 1 to opt.weight ) {
                randomOptions.optsBuffer+=opt.value
            }
        }
        randomOptions
    }

    def main(args: Array[String]): Unit = {
        val randomName = RandomOptions(RanOpt("zhangchen",10),RanOpt("li4",30))
        for (i <- 1 to 40 ) {
            println(i+":"+randomName.getRandomOpt())
        }
    }
}

case class RanOpt[T](value:T,weight:Int){}

class RandomOptions[T](opts:RanOpt[T]*) {
    var totalWeight=0
    var optsBuffer  =new ListBuffer[T]

    def getRandomOpt(): T ={
        val randomNum= new Random().nextInt(totalWeight)
        optsBuffer(randomNum)
    }
}
```



# 问题



## Hadoop文件写入权限不足

在windows的idea中访问执行时，会向linux下的hadoop提交文件，可能会发生权限问题，原因是执行时，idea会将当前windows登陆用户做为hadoop用户来提交文件。因此会导致权限不足的问题

解决办法 ：在执行程序的Edit Configurations中 做如下设置，把VM options或Environment variables中加入

```bash
-DHADOOP_USER_NAME=xxxxxx （你的hadoop用户）
```



## hive访问失败

可能的原因是hive-site.xml误修改了元数据存储库名称

