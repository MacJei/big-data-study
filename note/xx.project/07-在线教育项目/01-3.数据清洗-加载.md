# 需求 

- 假定现在已经将数据保存到HDFS的/user/hive/warehouse/ods.db/origin_user_behavior/${day}目录中，需要用SparkCore将数据清洗，清洗需求如下
  - 手机号脱敏：187xxxx2659
  - 过滤重复行（重复条件，uid,event_key,event_time三者都相同即为重复）
  - 最终数据保存到ods.user_behavior分区表，以dt（天）为分区条件，表的文件存储格式为ORC，数据总量为xxxx条
- Hive 字段如下

```sql
uid STRING comment "用户唯一标识",
username STRING comment "用户昵称",
gender STRING comment "性别",
level TINYINT comment "1代表小学，2代表初中，3代表高中",
is_vip TINYINT comment "0代表不是会员，1代表是会员",
os STRING comment "操作系统:os,android等",
channel STRING comment "下载渠道:auto,toutiao,huawei",
net_config STRING comment "当前网络类型",
ip STRING comment "IP地址",
phone STRING comment "手机号码",
video_id INT comment "视频id",
video_length INT comment "视频时长，单位秒",
start_video_time BIGINT comment "开始看视频的时间缀，秒级",
end_video_time BIGINT comment "退出视频时的时间缀，秒级",
version STRING comment "版本",
event_key STRING comment "事件类型",
event_time BIGINT comment "事件发生时的时间缀，秒级"
```



# 整体流程（使用调度系统调度）

- SparkCore清洗数据，写入到/user/hive/warehouse/tmp.db/user_behavior_${day}目录
- 建立tmp.user_behavior_${day}临时表，并加载上面清洗后的数据
- 使用hive引擎，并用开窗函数row_number，将tmp.user_behavior_${day}表数据插入到dwd.user_behavior表中
- 删除tmp.user_behavior_${day}临时表



# 数据清洗

- 从hdfs读取数据，转换格式后输出到hdfs的临时目录中
- 验证数据的有效性，如字段个数17个，不满足的进行过滤
- 手机号脱敏：187xxxx2659



## pom

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.stt.project</groupId>
    <artifactId>UserBehavior</artifactId>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
    </properties>

    <dependencies>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.11</version>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.3.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.3.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.11</artifactId>
            <version>2.3.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.11</artifactId>
            <version>2.3.0</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java -->
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.41</version>
        </dependency>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.11</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.scalikejdbc</groupId>
            <artifactId>scalikejdbc_2.11</artifactId>
            <version>2.2.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-8_2.11</artifactId>
            <version>2.3.0</version>
        </dependency>
        <dependency>
            <groupId>net.ipip</groupId>
            <artifactId>ipdb</artifactId>
            <version>1.1.1</version>
        </dependency>

    </dependencies>

    <build>
        <sourceDirectory>src/main/</sourceDirectory>
        <testSourceDirectory>src/test/</testSourceDirectory>
        <plugins>
            <plugin>
                <!-- see http://davidb.github.com/scala-maven-plugin -->
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.1.3</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                        <configuration>
                            <args>
                                <arg>-dependencyfile</arg>
                                <arg>${project.build.directory}/.scala_dependencies</arg>
                            </args>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>1.5</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <shadedArtifactAttached>true</shadedArtifactAttached>
                            <shadedClassifierName>allinone</shadedClassifierName>
                            <artifactSet>
                                <includes>
                                    <include>*:*</include>
                                </includes>
                            </artifactSet>
                            <transformers>
                                <transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
                                    <resource>reference.conf</resource>
                                </transformer>
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <manifestEntries>
                                        <Main-Class></Main-Class>
                                    </manifestEntries>
                                </transformer>
                            </transformers>
                            <filters>
                                <filter>
                                    <artifact>*:*</artifact>
                                    <excludes>
                                        <exclude>META-INF/*.SF</exclude>
                                        <exclude>META-INF/*.DSA</exclude>
                                        <exclude>META-INF/*.RSA</exclude>
                                    </excludes>
                                </filter>
                            </filters>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>2.13</version>
                <configuration>
                    <useFile>false</useFile>
                    <disableXmlReport>true</disableXmlReport>
                    <!-- If you have classpath issue like NoDefClassError,... -->
                    <!-- useManifestOnlyJar>false</useManifestOnlyJar -->
                    <includes>
                        <include>**/*Test.*</include>
                        <include>**/*Suite.*</include>
                    </includes>
                </configuration>
            </plugin>
        </plugins>
        <pluginManagement>
            <plugins>
                <!--This plugin's configuration is used to store Eclipse m2e settings
                    only. It has no influence on the Maven build itself. -->
                <plugin>
                    <groupId>org.eclipse.m2e</groupId>
                    <artifactId>lifecycle-mapping</artifactId>
                    <version>1.0.0</version>
                    <configuration>
                        <lifecycleMappingMetadata>
                            <pluginExecutions>
                                <pluginExecution>
                                    <pluginExecutionFilter>
                                        <groupId>
                                            net.alchim31.maven
                                        </groupId>
                                        <artifactId>
                                            scala-maven-plugin
                                        </artifactId>
                                        <versionRange>
                                            [3.1.3,)
                                        </versionRange>
                                        <goals>
                                            <goal>testCompile</goal>
                                            <goal>compile</goal>
                                        </goals>
                                    </pluginExecutionFilter>
                                    <action>
                                        <ignore></ignore>
                                    </action>
                                </pluginExecution>
                            </pluginExecutions>
                        </lifecycleMappingMetadata>
                    </configuration>
                </plugin>
            </plugins>
        </pluginManagement>
    </build>
</project>
```



## scala

```scala
package com.stt.project

import org.apache.commons.lang.StringUtils
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * 清理
  * 1)/user/hive/warehouse/ods.db/origin_user_behavior/${day}目录中，需要用SparkCore将数据清洗
  * 清洗需求如下：
  * a)手机号脱敏：187xxxx2659
  * b)过滤重复行（重复条件，uid,event_key,event_time三者都相同即为重复）
  * c)最终数据保存到ods.user_behavior分区表，以dt（天）为分区条件，表的文件存储格式为ORC，数据总量为xxxx条
  */
object UserBehaviorCleaner {

  def main(args: Array[String]): Unit = {

    if(args.length !=2) {
      println("input or output miss")
      System.exit(1)
    }

    // 获取输入输出路径
    val inputPath=args(0)
    val outputPath=args(1)

    // 注意配置的有local要在yarn模式下去除
//    val conf: SparkConf = new SparkConf().setAppName(getClass.getSimpleName).setMaster("local[*]")
    val conf: SparkConf = new SparkConf().setAppName(getClass.getSimpleName)
    val sc = new SparkContext()

    // 通过输入路径获取RDD
    val eventRDD: RDD[String] = sc.textFile(inputPath)

    eventRDD.filter(event => checkEventValid(event)) // 验证数据有效性
      .map(event => maskPhone(event)) // 对手机号码进行脱敏
      .map(event => repairUsername(event)) // 修复username中带有\n导致的换行
      .coalesce(3) // 重分区，优化处理
      .saveAsTextFile(outputPath)

    sc.stop()
  }

  /**
    * 检验格式是否正确，只有17个字段才符合规则
    * @param event
    * @return
    */
  def checkEventValid(event: String): Boolean = {
    val fields: Array[String] = event.split("\t")
    return fields.length == 17
  }

  /**
    * 脱敏手机号
    * @param event
    * @return
    */
  def maskPhone(event: String): String = {
    val fields: Array[String] = event.split("\t")
    var phoneNum: String = fields(9)
    // 号码有效时进行数据处理
    if(phoneNum != "" && !"Null".equals(phoneNum)){
      fields(9) = new StringBuffer(phoneNum.substring(0,3))
              .append("xxx")
              .append(phoneNum.substring(7,11))
              .toString
    }
    fields.mkString("\t")
  }

  /**
    * username为用户自定义的，里面有要能存在"\n"，导致写入到HDFS时换行
    * @param event
    * @return
    */
  def repairUsername(event:String):String ={
    val fields: Array[String] = event.split("\t")
    val username: String = fields(1)
    if(!StringUtils.isEmpty(username)){
      fields(1)=username.replace("\n","")
    }
    fields.mkString("\t")
  }
}
```

- 本地执行时，需要配置参数
  - -DHADOOP_USER_NAME=ttshe
  - -Dspark.master=local[2]



## 执行

- ==打包，注意打包的classes里面要有内容==

- 上传jar包（命名ub.jar）
- 创建目录，上传文件（使用本地生成模拟数据）

```bash
[ttshe@hadoop102 soft]$ hadoop fs -mkdir -p /user/hive/warehouse/ods.db/origin_user_behavior
[ttshe@hadoop102 soft]$ hadoop fs -put 20190402/ /user/hive/warehouse/ods.db/origin_user_behavior
```

- 提交命令
  - 注意执行内存调节
- 格式，${day}用于脚本入参执行

```bash
spark-submit --master yarn --deploy-mode cluster \
--num-executors 8 \
--executor-cores 4 \
--executor-memory 12G \
--class 主类 xxx.jar \
hdfs://ip:port/user/hive/warehouse/ods.db/origin_user_behavior/${day} \
hdfs://ip:port/user/hive/warehouse/tmp.db/user_behavior_${day} 
```

- 命令示例

```bash
bin/spark-submit --master yarn --deploy-mode cluster \
--num-executors 1 \
--executor-cores 1 \
--executor-memory 2G \
--class com.stt.project.UserBehaviorCleaner /opt/soft/ub.jar \
hdfs://hadoop102:9000/user/hive/warehouse/ods.db/origin_user_behavior/20190402 \
hdfs://hadoop102:9000/user/hive/warehouse/tmp.db/user_behavior_20190402
```

- 依次生成7天的数据并清洗和加载，为之后的7日留存指标做准备



# 数据加载

- 可在清洗阶段可以使用spark做数据的去重操作，如使用sparksession，创建一个view进行去重
- 那么单独使用一个数据加载阶段进行去重的好处
  - 解耦，将清洗阶段和加载阶段进行解耦
  - 如果都使用spark进行清洗和加载，那么如果数据过大，去重加载需要将大量数据放入内存中，一旦程序异常，清洗阶段的数据也会丢失，需要重新执行清洗流程
  - 使用hive加载去重重新放入新的表中，还可以进行orc格式存储转换
    - spark-sql是计算密集型任务
    - hive是IO密集型任务
    - 当前的需求是IO密集型任务



## 加载到临时表 [spark-core]

```sql
create database tmp;
use tmp;
```

- 临时表使用日期，表示该临时表创建的日期，后期可以判断是否要删除该临时表依据时间

```sql
create table if not exists tmp.user_behavior_${day}(
uid STRING comment "用户唯一标识",
username STRING comment "用户昵称",
gender STRING comment "性别",
level TINYINT comment "1代表小学，2代表初中，3代表高中",
is_vip TINYINT comment "0代表不是会员，1代表是会员",
os STRING comment "操作系统:os,android等",
channel STRING comment  "下载渠道:auto,toutiao,huawei",
net_config STRING comment "当前网络类型",
ip STRING comment "IP地址",
phone STRING comment "手机号码",
video_id INT comment "视频id",
video_length INT comment "视频时长，单位秒",
start_video_time BIGINT comment "开始看视频的时间缀，秒级",
end_video_time BIGINT comment "退出视频时的时间缀，秒级",
version STRING comment "版本",
event_key STRING comment  "事件类型",
event_time BIGINT comment  "事件发生时的时间缀，秒级")
row format delimited fields terminated by "\t" 
location "/user/hive/warehouse/tmp.db/user_behavior_${day}";
```

- 示例

```sql
create table if not exists tmp.user_behavior_20190408(
uid STRING comment "用户唯一标识",
username STRING comment "用户昵称",
gender STRING comment "性别",
level TINYINT comment "1代表小学，2代表初中，3代表高中",
is_vip TINYINT comment "0代表不是会员，1代表是会员",
os STRING comment "操作系统:os,android等",
channel STRING comment  "下载渠道:auto,toutiao,huawei",
net_config STRING comment "当前网络类型",
ip STRING comment "IP地址",
phone STRING comment "手机号码",
video_id INT comment "视频id",
video_length INT comment "视频时长，单位秒",
start_video_time BIGINT comment "开始看视频的时间缀，秒级",
end_video_time BIGINT comment "退出视频时的时间缀，秒级",
version STRING comment "版本",
event_key STRING comment  "事件类型",
event_time BIGINT comment  "事件发生时的时间缀，秒级")
row format delimited fields terminated by "\t" 
location "/user/hive/warehouse/tmp.db/user_behavior_20190408";
```



## 外部表 [orc格式]

- schema与上面相同

```sql
create database dwd;
use dwd;
```

- 创建orc目标表

```sql
create external table if not exists dwd.user_behavior(
uid STRING comment "用户唯一标识",
username STRING comment "用户昵称",
gender STRING comment "性别",
level TINYINT comment "1代表小学，2代表初中，3代表高中",
is_vip TINYINT comment "0代表不是会员，1代表是会员",
os STRING comment "操作系统:os,android等",
channel STRING comment  "下载渠道:auto,toutiao,huawei",
net_config STRING comment "当前网络类型",
ip STRING comment "IP地址",
phone STRING comment "手机号码",
video_id INT comment "视频id",
video_length INT comment "视频时长，单位秒",
start_video_time BIGINT comment "开始看视频的时间缀，秒级",
end_video_time BIGINT comment "退出视频时的时间缀，秒级",
version STRING comment "版本",
event_key STRING comment  "事件类型",
event_time BIGINT comment  "事件发生时的时间缀，秒级"
) 
partitioned by(dt INT)  
row format delimited fields terminated by "\t" stored as ORC;
```



### event_key值

- startApp 打开App
- closeApp 关闭App
- registerAccount 注册用户
- startVideo 开始看视频
- endVideo 结束看视频
- startHomework 开始作业
- completeHomework 完成作业
- shareVideo 分享视频
- enterOrderPage 进入订单详情页
- completeOrder 支付完成订单，成为vip

说明：每个event_key代表一种行为



## 导入数据sql

- 将tmp.user_behavior_${tmp}的数据导入到ORC表中，使用开窗函数实现去重业务

```sql
insert overwrite table dwd.user_behavior partition(dt=${day})
select 
    uid,
    username,
    gender,
    level,
    is_vip,
    os,
    channel,
    net_config,
    ip,
    phone,
    video_id,
    video_length,
    start_video_time,
    end_video_time,
    version,
    event_key,
    event_time  
from (
    select 
        uid,
        username,
        gender,
        level,
        is_vip,
        os,
        channel,
        net_config,
        ip,
        phone,
        video_id,
        video_length,
        start_video_time,
        end_video_time,
        version,
        event_key,
        event_time,
		row_number() OVER (PARTITION BY uid,event_key,event_time ORDER BY event_time) u_rank 
	from tmp.user_behavior_${day} 
) temp 
where u_rank = 1
```

- 示例

```sql
insert overwrite table dwd.user_behavior partition(dt=20190408)
select 
uid,
username,
gender,
level,
is_vip,
os,
channel,
net_config,
ip,
phone,
video_id,
video_length,
start_video_time,
end_video_time,
version,
event_key,
event_time  
from (
select 
uid,
username,
gender,
level,
is_vip,
os,
channel,
net_config,
ip,
phone,
video_id,
video_length,
start_video_time,
end_video_time,
version,
event_key,
event_time,
row_number() OVER (PARTITION BY uid,event_key,event_time ORDER BY event_time) u_rank 
from tmp.user_behavior_20190408
) temp where u_rank = 1;
```



## 分析

- level和is_vip使用TINYINT，而不是使用INT
  - 优化，能节省内存空间

- 分区字段dt为什么要存储int型，如20190408，而不是字符串的'2019-04-08'
  - hive是弱化索引的，使用分区代替粗粒度索引
  - 类型是整型比字符串类型在查询索引会快
  - 在实际生产中，使用0和1代替boolean类型

-  说明：event_key为endVideo时，会发送start_video_time和end_video_time字段

