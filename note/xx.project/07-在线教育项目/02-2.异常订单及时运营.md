# 需求2：异常订单用户及时运营

- 保证数据精确消费一次

- 每隔30S，统计近60S的用户行为数据，当出现进入定单页（eventKey=enterOrderPage）>=3次，但是没有成功完成订单时(去业务数据库的实时备份表vip_users查询用户是否为VIP)，即将用户uid持久化到Mysql中
  
  - 使用实时备份表，是由于大数据不会对真实生产表进行处理，即使是查询也不会
    - 大数据查询可能会查询大量数据，导致数据压力过大，一般单独给大数据一个备份表进行操作
  - 实际生产中的指标是每隔2分钟，统计10分钟的用户行为数据
  
  

# 测试数据

```text
61024	61024	M	1	0	ios	toutiao	wifi	42.86.6.0	18701461024	0	0	0	0	2.0	enterOrderPage	1554652800
```



# 准备工作



## 结果表

```sql
CREATE TABLE `unpayment_record` (
    `uid` varchar(128) NOT NULL COMMENT '用户id',
    `phone` varchar(16) NOT NULL COMMENT '用户手机号',
    `callback_status` tinyint(4) DEFAULT NULL COMMENT '回访状态',
    constraint unpayment_record_unique unique(uid) # 创建唯一索引
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```



## offset表

```sql
CREATE TABLE `unpayment_topic_offset` (  
  `topic` varchar(128) DEFAULT NULL,  
  `part_id` int(11) DEFAULT NULL,  
  `offset` bigint(20) DEFAULT NULL  
) ENGINE=InnoDB DEFAULT CHARSET=utf8

# 创建唯一组合键
CREATE UNIQUE INDEX unpayment_topic_offset_unique ON unpayment_topic_offset (topic,part_id);

# 初始化数据
insert into unpayment_topic_offset(topic, part_id, offset) values('user-behavior',0,0);  
insert into unpayment_topic_offset(topic, part_id, offset) values('user-behavior',1,0);  
insert into unpayment_topic_offset(topic, part_id, offset) values('user-behavior',2,0);
```



## 模拟业务表

```sql
CREATE TABLE `vip_user` (
    `id` int(11) NOT NULL AUTO_INCREMENT,
    `uid` varchar(16) DEFAULT NULL,
    PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```



# 分析

- 窗口是否需要精确一次消费？
  - 基本很那做到，因为窗口有叠加，在交界处如果宕机，会有数据丢失，无法实现消息重发
  - 但是kafka的消费的offset依然需要记录在数据库中，下次启动时使用，防止重复消费
- 窗口是否有必要精确一次消费？
  - 由于窗口是一个趋势的各个时段的展示，在某些业务的数据准确性上不需要
- 窗口操作是否需要检查点操作？
  - 检查点首先是作为故障恢复时使用的，保存了元数据信息以及rdd信息，会对系统的性能有消耗
  - 如果窗口操作含有检查点，使用updateByKey则每次操作的结果会有上一个检查点的rdd数据
  - 而且对于窗口而言，有些业务需求需要显示时段的大概趋势，那么从检查点恢复时，已经没有时效意义了
  - 故在窗口操作下可以去除检查点操作，无状态性



# 实现

- 使用窗口函数

```scala
package com.stt.project.ch04

import java.util.Properties

import com.stt.project.ch03.VipIncrementAnalysis2._
import com.stt.project.ch03.VipIncrementAnalysis3.initJDBC
import kafka.message.MessageAndMetadata
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.TopicPartition
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.InputDStream
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.{Seconds, StreamingContext}
import scalikejdbc.{DB, _}

import util.control.Breaks._

/**
  * 每隔30S，统计近60S的用户行为数据，当出现进入定单页（eventKey=enterOrderPage）>=3次，
  * 但是没有成功完成订单时(去业务数据库的实时备份表vip_users查询用户是否为VIP)，即将用户uid持久化到Mysql中
  */
object UnPaymentAnalysis {

  val prop = new Properties()
  prop.load(getClass.getClassLoader.getResourceAsStream("config.properties"))

  initJDBC()

  def main(args: Array[String]): Unit = {

    val ssc = createStreamContext()

    // 启动流式计算
    ssc.start()
    ssc.awaitTermination()
  }

  def createStreamContext(): StreamingContext = {
    val conf = new SparkConf()
      .set("spark.streaming.stopGracefullyOnShutdown", "true") // 优雅关闭配置
      .set("spark.streaming.backpressure.enabled", "true") // 背压配置
      .setAppName(getClass.getSimpleName)

    val ssc = new StreamingContext(conf, Seconds(getProcessInterval))

    val msgDStream: InputDStream[ConsumerRecord[String, String]] = getDStreamFromKafka(ssc)
    // 业务处理
    unPaymentAnalysisHandler(msgDStream)
    ssc
  }

  def getDStreamFromKafka(ssc: StreamingContext): InputDStream[ConsumerRecord[String, String]] = {

    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> prop.getProperty("brokers"),
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> getClass.getSimpleName,
      "auto.offset.reset" -> "latest", // latest从最新的开始读取 smallest从最早的读取
      "enable.auto.commit" -> (false: java.lang.Boolean)
    )

    val fromOffsets: Map[TopicPartition, Long] =
      readOffsetFromDB().map(r => (new TopicPartition(r._1, r._2), r._3)).toMap

    // 消息处理匿名函数
    val messageHandler = (mmd: MessageAndMetadata[String, String]) => (mmd.topic, mmd.message())

    // 使用kafka的Direct模式，拉取的方式
    // 注意需要声明类型
    KafkaUtils.createDirectStream[String, String](
      ssc,
      PreferConsistent,
      ConsumerStrategies.Assign[String, String](fromOffsets.keys, kafkaParams, fromOffsets)
    )
  }

  def unPaymentAnalysisHandler(msgDStream: InputDStream[ConsumerRecord[String, String]]) = {

    // 进行过滤处理，小于3个的以及存在于数据库的
    val filterUnnormalOrderUser = (event:((String,String),Int))=>{

      if(event._2 >=3){
        // 查询数据库
        val result: List[Int] = DB.readOnly {
          implicit session => {
            sql"""
                select id
                from vip_user
                where uid = ${event._1._1}
              """.map(r => r.get[Int](1)).list().apply()
          }
        }
        // 如果结果为空，代表用户还不是vip，所以需要做后续运营
        if(result.isEmpty){
          true
        }else{
          false
        }
      }else{
        false
      }
    }

    // 定义偏移量
    var offsetRanges: Array[OffsetRange] = Array.empty[OffsetRange]

    // 使用transform 算子的原因是，不会对分区进行修改，此时rdd的分区和kafka的分区保持一致
    msgDStream
      .transform(rdd => {
        offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
        rdd
      })
      .map(m => m.value()) // 将value取出
      .filter(completeOrderData)
      .map(conversionUIDAndOne) // 数据转换，返回((uid,phone),1)格式的数据
      .reduceByKeyAndWindow((a:Int,b:Int) => a+b,Seconds(getProcessInterval*4), Seconds(getProcessInterval*2)) // 窗口大小60s，滑动距离30s
      .filter(filterUnnormalOrderUser)
      .map{ case ((uid: String, phone: String), sum: Int) => (uid, phone,sum)} // 转换格式
      .foreachRDD(rdd => {

        // 将所有的rdd的结果汇总到driver，当数据量小的时候
        val results: Array[(String, String,Int)] = rdd.collect()

          // 开始事务
          DB.localTx {
            implicit session => {
              for (o <- results) breakable {
                // 对分区数据进行更新
                sql"replace into unpayment_record(uid,phone) values (${o._1},${o._2})"
                  .executeUpdate().apply()
                println(o)
              }

              // 保存offset
              for (offset <- offsetRanges) {
                //println(offset.topic, offset.partition, offset.fromOffset, offset.untilOffset)
                sql"update unpayment_topic_offset set offset=${offset.untilOffset} where topic=${offset.topic} and part_id=${offset.partition}"
                  .update().apply()
              }
              // 更新offset 如果更新失败了，消息重新计算，那么需要做幂等去重
              msgDStream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)
            }
          }
      })

    //对msg进行过滤处理
    def completeOrderData(msg: String): Boolean = {
      val fields = msg.split("\t")
      // 切分后长度要是17
      if (fields.length == 17) {
        return "enterOrderPage".equals(fields(15))
      }
      return false
    }

    // 数据转换，返回((uid,phone),1)格式的数据
    def conversionUIDAndOne(msg: String): ((String, String), Int) = {
      val fields = msg.split("\t")
      val uid = fields(0)
      val phone = fields(9)
      ((uid, phone), 1)
    }
  }
}
```

- 测试参数
  - VM
    - -Dspark.master=local[2] -Dspark.testing.memory=1023741824 -DHADOOP_USER_NAME=ttshe