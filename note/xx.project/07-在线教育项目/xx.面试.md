# 面试



## yarn cluster和yarn client区别

- Driver运行的位置不同
- yarn-client端运行的瓶颈是网络IO，客户端网卡流量打满，网络IO负载过高
- yarn-cluster的driver运行在nodemanager上



## Textfile、Parquet、ORC格式选择

- 亮点：使用ORC后，空间节省90%，查询提升3-5倍

- 如何测试
  - 使用相同的2份数据，一份textFile，一份orc
  - 跑10个最常用的sql，取得平均数评判



## 外部表与内部表在企业中怎么使用

- 数仓中的表都使用外部表
- 分析而生成的中间结果表，都使用内部表，并且这类表一般以日期结尾，这样可以清晰意识到这是个中间表，还能知道是哪天创建的。这类表当天使用完后就会在脚本的最后将其删除



## union 和 union all 的区别

- union 会对结果去重
- union all 不会对结果去重
  - 使用union all ，从业务角度而言，2张表不会有重复数据，可使用union all 提升性能



## hive 的 job的个数判定

- sql中select的个数或者看from的个数
  - hive会做优化，最多是select的个数



# 离线项目真实场景分析

- executor memory和executor cores如何设置?

  - 默认值在spark-env.sh脚本中有写明

  - memory默认1G

  - driver memory 默认1G

  - cores默认1个

  - 在20台机器：2160G（`108*20`），720线程cpu（`36*20`）

    ![](img/8.png) 

    ![](img/9.png) 

    - 每台128G内存，40核cpu，8T硬盘
    - 36线程给计算任务，4线程给其他任务
    - 108G可以分配任务

- 集群情况说明
  - 20台 128G内存，40线程CPU，4T硬盘 * 4
  - 每台服务器分给Yarn管理的资源为 36线程，108G内存（因为CPU数量尽量与内存量是一个整数比，此处为1：3）
  - 所以Yarn能够管理的总资源为2160G内存和720线程CPU

- 数据情况：用户行为数据每日新增200G，约2亿条数据

- 使用SparkCore清洗时，使用On Yarn模式，如何设置job资源
  - 首先确认--executor-cores，官方建议2-5个，超过5个时不会有性能的明显提升，但是却占用了更多的cpu资源，此处我们设置为4
  - 再确认--executor-memory，因为上面说了比例为1：3，所以此处设置为12G（4核*3G）
  - 最后确认--num-executors，而此项又需要根据提交任务所在队列的最大资源和多少应用程序并行执行，比如Yarn中只有4个队列，各占25%，每个队列需要并发执行4个任务，所以任务可以获取45线程CPU和135G内存，上面2点已经确认了每个executor需要的cpu和内存，直接相除，向下取整即可，所以executor数量在此场景下设置为10最合适
  - executor
    - cpu：4
    - memory：12G
    - num
      - 资源池2160G，cpu720
      - yarn中4个队列，每个队列4个任务并行执行，共16个任务同时执行
        - 每个任务分配的cpu 核数 720/16=45，内存45*3=135G
        - 每个任务可以执行的executor个数为 45/4 约 11 向下取整 10

- 使用上面的资源提升任务时，比如需求是清洗200G数据，大概耗时5-10分钟