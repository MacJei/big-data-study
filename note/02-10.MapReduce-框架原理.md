# MapReduce 框架原理

- InputFormat阶段

- Shuffle（拖拽？，改组）阶段

- OutputFormat阶段

  

注意：需要理解逻辑切片和实际存储Block的关系



![1558248444836](img/hadoop/04.mr03.png)





# InputFormat 阶段

- 继承结构
  ![1558276392855](img/hadoop/04.mr07.png)



- 在InputFormat阶段的数据格式：

  - 基于行的日志文件

  - 二进制文件

  - 数据库表

    

- 通过InputFormat接口的不同的实现类来实现

  - TextInputFormat
  - KeyValueTextInputFormat
  - NLineInputFormat
  - CombineTextInputFormat
  - 自定义InputFormat



## 切片与MapTask并行度

> MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度

- 思考
  - 1G的数据
    - 启动8个MapTask
    - 可以提高集群的并发处理能力
  - 1k的数据
    - 启动8个MapTask，会提高集群性能么？
    - MapTask并行任务是否越多越好呢？
    - 那些因素影响了MapTask的并行度？



### MapTask并行度决定机制

- 数据块

  - Block是HDFS物理上把数据分成一块一块

- 数据切片

  - 只是在逻辑上对输入进行分片

  - 并不会在磁盘上将其切分成片进行存储

    

- 假设情况1

  - 300M的数据，按照100M进行切片
  - 数据块的大小是128M
  - 每个DataNode在收到MapTask的数据调用请求时，会有跨DataNode的情况出现
    - DataNode之间的数据IO交互，降低了Job在Map阶段的性能
    - 对DataNode上数据的访问需要进行计算起始位置，然后访问数据
    - 逻辑复杂，出现问题不利于排查。

![1558249362238](img/hadoop/04.mr04.png)



- 假设情况2
  - 300M的数据，按照128M进行切片处理
  - 每个分片分配到一个MapTask进行处理，MapTask读取到一个DataNode，刚好是一个分片
  - 没有多余的判断分片大小是否等于设定的大小，直接读取处理即可，简单安全
  - 如果再来一个数据，数据大小是100M
    - 最后一个DataNode存储的是100MBlock
    - 最后2个分别是44M和100M的Block
      - 不会合并再划分Block

![1558249659117](img/hadoop/04.mr05.png)



### 小结

- 一个Job的Map阶段并行读有客户端在提交Job时的==切片数决定==
- 每一个Split切片分配一个MapTask并行实例处理
- 在默认情况下，切片大小=BlockSize
- 切片时不考虑数据集整体，而是逐个针对每个文件单独切片



## Job提交流程源码分析

从`job.waitForCompletion`打上断点，进行分析。

```java
// 0.提交执行 Job.java 1307行
submit();
	// setUseNewApi() 为了旧版本的兼容处理，将旧的Api转换为新的Api
	// 1.建立连接
	connect();
	// (1) 创建提交Job的代理
		return new Cluster(getConfiguration());
		// 判断协议：是使用local,yarn,还是远程
			initialize(jobTrackAddr, conf);
	// 2.提交job
	return submitter.submitJobInternal(Job.this, cluster);
	// 验证输出的job的路径是否已经有文件，有文件表示结果已经生成退出
		checkSpecs(job);
	// (1) 创建给集群提交数据的Stag路径，这里的路径是file:/tmp/hadoop-Administrator/mapred/staging/Administrator186995491/.staging
		Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf)
	// (2) 获取jobId，创建job路径
    	JobID jobId = submitClient.getNewJobID()
    // (3) 拷贝jar包到集群
    	copyAndConfigureFiles(job, submitJobDir);
			// 上传配置文件和要执行的jar到tmp对应的job文件夹下（本地运行时没有jar包上传）
        	rUploader.uploadFiles(job, jobSubmitDir);
	// (4) 计算切片信息，生成切片规划文件到tmp文件夹
	// 切片文件job.split内部信息：SPL /org.apache.hadoop.mapreduce.lib.input.FileSplitfile:/d:/hello.txt  
		 int maps = writeSplits(job, submitJobDir);
			 	maps = writeNewSplits(job, jobSubmitDir);
					List<InputSplit> splits = input.getSplits(job); //*******
	// (5) 生成job.xml配置信息，包含当前job运行的所有hadoop的配置信息（这里修改的原因是：代码中可能会对hadoop的一些配置进行修改，那么每个不同的job使用的配置是不同的，那么就需要给每个job一个配置，代码的配置优先级最高，其次是自定义配置，最后是系统默认配置，这些配置合在一起就是该job的配置信息）
		writeConf(conf, submitJobFile);
	// (6) 提交job，返回提交状态（执行Map阶段，执行Reduce阶段返回结果）
		status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials());
	// 提交完成后，会删除tmp的staging内的job的临时文件
```



### 分析图

![1558257974732](img/hadoop/04.mr06.png)



## FileInputFormat



### 切片源码分析

> 在提交过程中首先会生成切片信息，在input.getSplites(job)中获取，该流程如下
>
> 注意：集群模式Block大小是128M，1.x版本的集群模式是64M，本地模式是32M
>
> 切片的大小是逻辑划分
> 数据块的大小是物理划分

- 程序先找到存储数据的目录
- 开始遍历处理，规划切片目录下的每一个文件
- 遍历第一个文件ss.txt
  - 获取文件大小
    - `fs.sizeOf(ss.txt)`
  - 计算切片大小
    - `computeSplitSize(Math.max(minSize,Math.min(maxSize,blockSize))) = blockSize = 128M`
  - ==默认情况下，切片大小=blockSize==
  - 开始切
    - 第一个切片ss.txt--0:128M
    - 第二个切片ss.txt--128M:256M
    - 第三个切片ss.txt--256M:300M
    - ==每次切片，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分为一块切片==
  - 将切片信息写到一个切片规划中
  - 整个切片的核心过程在getSplit()方法中完成
  - **==InputSplit记录了切片的元数据信息==**
    - 如起始位置
    - 长度
    - 所在节点的列表等
- ==提交切片规划文件到YARN上的MrAppMaster可以依据切片规划文件计算开始MapTask个数==



### 切片机制

> 简单的按照文件的内容长度进行切片
> 切片大小默认等于Block大小
> 切片时不考虑数据集整体，而是逐个针对每个文件单独切片（所以不能有大量的小文件）

- 案例分析
  - 输入数据2个文件：file1.txt--320M，file2.txt--10M
  - 经过FileInputFormat的切片机制运算后，形成如下切片信息
    - file1.txt.split1-- 0:128
    - file1.txt.split2-- 128:256
    - file1.txt.split3-- 256:320
    - file2.txt.split1-- 0:10  



### 切片参数配置

- 源码中计算切片大小的公式

  - `Math.max(minSize,Math.min(maxSize,blockSize));`
    - `mapreduce.input.fileinputformat.split.minsize `
      - 默认值为==1==
    - `mapreduce.input.fileinputformat.split.maxsize`
      - 默认值`Long.MAXValue`
    - 默认切片大小` blockSize`

- 切片大小设置

  - maxsize：若设置比blockSize小，会让切片变小，等于配置的这个参数的值
  - minsize：若设置比blockSize大，则可以让切片比blockSize大

- 获取切片信息API

  - 获取切片的文件名称

    ```java
    String name =  inputSplit.getPath().getName();
    ```

  - 根据文件类型获取切片信息

    ```java
    FileSplit fileSplit = (FileSplit)context.getInputSplit();
    ```

    

## CombineTextInputFormat

> 框架默认的TextFileInputFormat切片机制是对任务按文件规划切片，==无论文件多小，都是一个单独的切片==，都会生成一个MapTask进行处理，这样如果有大量文件，就会产生大量的MapTask，效率极其低下

源码参考F:\大数据-尚硅谷\归档\01.hadoop\resource\04_扩展资料\合并小文件切片逻辑&测试数据.zip



### 应用场景

- 用于小文件过多的场景
- 将多个小文件从==逻辑上==规划到一个切片中
  - 交给一个MapTask进行处理
  - 逻辑上一个切片，一个MapTask



### 参数设置

- 虚拟存储切片的最大值最好和实际的小文件大小情况来设置
- 设置虚拟存储切片的最大值
  - `CombineTextInputFormat.setmaxInputSplitSize(job,4194304);`
    - 4MB大小
    - 默认128MB



### 切片机制

生成切片过程

- 虚拟存储

- 切片

  

![1558853677886](img/hadoop/04.mr08.png)

- 存储过程
  - 将输入目录下所有文件的大小，每个和设置的setMaxInputSplitSize比较，定为s
    - 如果>2s，以s切割一块
    - ==当剩余数据t，s<=t<2s，将t/2进行切分(防止出现切片太小)==
    - 如：t=8.02，s=4，那么切分结果t1=4,t2=2.01,t3=2.01
- 切片过程
  - 判断虚拟存储文件的大小，设为k，如果k >= s，则单独作为一个切片
  - 如果k<s，那么k与下一个切片k1合并形成一个切片



### 操作实例

准备4个文件，大小一次为1.7M，5.1M，3.4M，6.5M

在原先的 wordCount代码中的Driver中添加如下代码

```java
// 默认使用TextFileInputFormat
job.setInputFormatClass(CombineTextInputFormat.class);
// 大小设置为4M
CombineTextInputFormat.setMaxInputSplitSize(job,4194304);
```

在控制台中显示3个切片执行

```java
15:27:37.147 [main] INFO org.apache.hadoop.mapreduce.JobSubmitter - number of splits:3
```





## TextInputFormat

默认的FileInputFormat实现类，按行读取每条记录

- key是存储改行在整个文件中的==起始字节的偏移量==
- key是LongWritable类型
- value是这行的内容，不包括任何行终止字符（换行和回车）
- value是Text类型





## KeyValueTextInputFormat

- 每一行是一条记录，被分隔符分隔为key，value
- 在驱动类中设置分隔符，分隔key，value
- 默认分隔符是\t



示例

- 输入数据

```shell
banzhang ni hao
xihuan hadoop banzhang
banzhang ni hao
xihuan hadoop banzhang
```

- 期望结果
  - banzhang是key，ni hao是value

```shell
banzhang	2
xihuan	2
```

- 分析

  - 使用KeyValueTextInputFormat
  - 分隔符为空格
  - Map阶段，设置key为匹配的值，value是1
  - Reduce阶段进行value汇总，写出
  - Driver设置分隔符，设置输入格式

  

实现

mapper

```java
package com.stt.demo.mr.Ch04_KeyValueTextInputFormat;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class KVTextMapper extends Mapper<Text,Text,Text,LongWritable> {

	LongWritable v = new LongWritable(1);

	@Override
	protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {
		context.write(key,v);
	}
}
```

​	reducer

```java
package com.stt.demo.mr.Ch04_KeyValueTextInputFormat;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class KVTextReducer extends Reducer<Text,LongWritable,Text,LongWritable> {

    LongWritable v = new LongWritable();

    @Override
    protected void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
        long sum = 0;
        for (LongWritable value : values) {
            sum += value.get();
        }
        v.set(sum);
        context.write(key,v);
    }
}
```

​	driver

```java
package com.stt.demo.mr.Ch04_KeyValueTextInputFormat;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader;
import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class KVTextDriver {

	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

		args = new String[]{"d:/study/big-data/code/data/hadoop/mr/ch04/input.txt",
		"d:/study/big-data/code/data/hadoop/mr/ch04/output.txt"};

		Configuration config = new Configuration();
		// 设置分隔符
		config.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR," ");
		// 设置job
		Job job = Job.getInstance(config);
		job.setJarByClass(KVTextDriver.class);
		job.setMapperClass(KVTextMapper.class);
		job.setReducerClass(KVTextReducer.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(LongWritable.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(LongWritable.class);
		// 设置输入格式
		job.setInputFormatClass(KeyValueTextInputFormat.class);

		FileInputFormat.setInputPaths(job,new Path(args[0]));
		FileOutputFormat.setOutputPath(job,new Path(args[1]));
		job.waitForCompletion(true);
	}
}
```



## NLineInputFormat

每个map进程处理的InputSplit==不再按照Block块划分处理==，而是按照NLineInputFormat的行数N进行处理

- 输入文件的总行数/N = 切片数
- 如果没有整除，切片数 = 商 + 1
- map进程中的key和value与TextInputFormat保存一致

案例

​	对每个单词进行个数统计，按照行数进行划分切片，3行一个切片

- 输入数据

```shell
banzhang ni hao
xihuan hadoop banzhang
banzhang ni hao
xihuan hadoop banzhang
banzhang ni hao
xihuan hadoop banzhang
banzhang ni hao
xihuan hadoop banzhang
banzhang ni hao
xihuan hadoop banzhang banzhang ni hao
xihuan hadoop banzhang
```



- 实现

mapper

```java
package com.stt.demo.mr.Ch05_NLineInputFormat;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class NLineMapper extends Mapper<LongWritable,Text,Text,LongWritable>{
	Text k = new Text();
	LongWritable v = new LongWritable(1);
	
	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		String[] words = value.toString().split(" ");
		for (String word : words) {
			k.set(word);
			context.write(k,v);
		}
	}
}
```

​	reducer

```java
package com.stt.demo.mr.Ch05_NLineInputFormat;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class NLineReducer extends Reducer<Text,LongWritable,Text,LongWritable> {

	LongWritable v = new LongWritable();

	@Override
	protected void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
		long sum = 0L;
		for (LongWritable value : values) {
			sum += value.get();
		}
		v.set(sum);
		context.write(key,v);
	}
}
```

​	driver

```java
package com.stt.demo.mr.Ch05_NLineInputFormat;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class NLineDriver {

	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
		args = new String[]{"d:/study/big-data/code/data/hadoop/mr/ch05/input.txt",
				"d:/study/big-data/code/data/hadoop/mr/ch05/output.txt"};
		Configuration config = new Configuration();

		Job job = Job.getInstance(config);

		// 设置3行一个切片
		NLineInputFormat.setNumLinesPerSplit(job,3);
        // 设置实行的InputFormat
		job.setInputFormatClass(NLineInputFormat.class);

		job.setJarByClass(NLineDriver.class);
		job.setMapperClass(NLineMapper.class);
		job.setReducerClass(NLineReducer.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(LongWritable.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(LongWritable.class);

		FileInputFormat.setInputPaths(job,new Path(args[0]));
		FileOutputFormat.setOutputPath(job,new Path(args[1]));

		job.waitForCompletion(true);
	}
}
```

- 在控制台上有输出

```shell
16:12:24.446 [main] INFO org.apache.hadoop.mapreduce.JobSubmitter - number of splits:4
```



## 自定义InputFormat

背景：

- hadoop自带的InputFormat不能满足所有应用场景，需要使用自定义InputFormat来实现
- 针对大量小数据方案
  - Har，将大量小数据在上传hdfs时打包成一个har包
  - CombineTextInputFormat，针对多个小文件，生成一个切片分配一个mapTask进行处理
  - 自定义InputFormat



### 步骤

- 继承FileInputFormat
- 改写RecordReader，实现一次读取一个完整文件封装为KV
- 在输出时使用SequenceFileOutPutFormat输出合并文件



案例：

- 需求
  - 处理大量小文件
  - 将多个小文件合并成一个SequenceFile文件
    - hadoop用于存储二进制形式的KV对形式的文件
    - 存储多个文件，存储形式：文件路径+名称为key，内容为value
- 分析
  - 自定义一个类继承FileInputFormat
    - 重写isSplitable方法，返回false表示不可分割
    - 重写createRecordReader()，创建自定义的RecordReader对象，并初始化
  - 改写RecordReader，实现一次读取一个完整的文件封装为KV
    - 采用IO流一次读取一个文件输出到value中，由于设置了不可切片，最终将所有文件都封装到了value中
    - 获取文件路径信息+名称，并设置key
  - 设置Driver
    - 设置输入的inputFormat，设置输出的outputFormat

### 实现



#### 自定义InputFormat

```java
package com.stt.demo.mr.Ch06_CustomizedInputFormat;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.JobContext;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;

import java.io.IOException;

public class WholeFileInputFormat extends FileInputFormat<Text,BytesWritable> {

	@Override
	protected boolean isSplitable(JobContext context, Path filename) {
		// 不进行分片操作
		return false;
	}

	@Override
	public RecordReader<Text, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {
		// 使用自定义的RecordReader
		WholeRecordReader recordReader = new WholeRecordReader();
		recordReader.initialize(split,context);
		return recordReader;
	}
}
```



#### 自定义RecordReader类

```java
package com.stt.demo.mr.Ch06_CustomizedInputFormat;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;

import java.io.IOException;

public class WholeRecordReader extends RecordReader<Text,BytesWritable> {

	private Configuration configuration;
	private FileSplit fileSplit;

	private boolean isProgress = true;

	private BytesWritable v = new BytesWritable();
	private Text k = new Text();

	@Override
	public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException {
		this.fileSplit = (FileSplit) split;
		configuration = context.getConfiguration();
	}

	@Override
	public boolean nextKeyValue() throws IOException, InterruptedException {
		// 标记位，表示是否读取完成，注意查看Mapper中的run方法的while
		if(isProgress){

			// 获取文件系统
			FileSystem fs = FileSystem.get(configuration);
			// 获取切片的输入流
			FSDataInputStream fsDataInputStream = fs.open(fileSplit.getPath());
			// 定义缓存区
			byte[] buf = new byte[(int) fileSplit.getLength()];
			// 拷贝数据切片的所有数据到buf
	IOUtils.readFully(fsDataInputStream,buf,0,buf.length);
			// 封装value
			v.set(buf,0,buf.length);
			// 封装key，获取文件的路径和名称
			k.set(fileSplit.getPath().toString());

			IOUtils.closeStream(fsDataInputStream);
			isProgress = false;
			return true;
		}
		return false;
	}

	@Override
	public Text getCurrentKey() throws IOException, InterruptedException {
		return k;
	}

	@Override
	public BytesWritable getCurrentValue() throws IOException, InterruptedException {
		return v;
	}

	@Override
	public float getProgress() throws IOException, InterruptedException {
		return 0;
	}

	@Override
	public void close() throws IOException {
		// 关闭一些资源
	}
}
```

​	

#### 实现mapper,reducer,driver

```java
package com.stt.demo.mr.Ch06_CustomizedInputFormat;

import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class SequenceFileMapper extends Mapper<Text,BytesWritable,Text,BytesWritable> {

	@Override
	protected void map(Text key, BytesWritable value, Context context) throws IOException, InterruptedException {
		context.write(key,value);
	}
}
```

```java
package com.stt.demo.mr.Ch06_CustomizedInputFormat;

import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class SequenceFileReducer extends Reducer<Text,BytesWritable,Text,BytesWritable> {

	@Override
	protected void reduce(Text key, Iterable<BytesWritable> values, Context context) throws IOException, InterruptedException {
		// values的值只有一个
		context.write(key,values.iterator().next());
	}
}
```

```java
package com.stt.demo.mr.Ch06_CustomizedInputFormat;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;

public class SequenceFileDriver {

	public static void main(String[] args) throws Exception {
		args = new String[]{"d:/study/big-data/code/data/hadoop/mr/ch06",
				"d:/study/big-data/code/data/hadoop/mr/ch06/output"};

		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf);

		job.setJarByClass(SequenceFileDriver.class);
		job.setMapperClass(SequenceFileMapper.class);
		job.setReducerClass(SequenceFileReducer.class);

		// 定义输入的InputFormat
		job.setInputFormatClass(WholeFileInputFormat.class);
		// 定义输出的OutputFormat
		job.setOutputFormatClass(SequenceFileOutputFormat.class);

		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(BytesWritable.class);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(BytesWritable.class);

		FileInputFormat.setInputPaths(job,new Path(args[0]));
		FileOutputFormat.setOutputPath(job,new Path(args[1]));

		boolean re = job.waitForCompletion(true);
		System.exit(re ? 0:1);
	}
}
```



结果可以看到是key是路径，value是该文本的值，将多个文件存储在一个文件中，减少了namenode的内存压力

```shell
SEQorg.apache.hadoop.io.Text"org.apache.hadoop.io.BytesWritable      � w��h���	����I�   n   98file:/d:/study/big-data/code/data/hadoop/mr/ch06/one.txt   1yongpeng weidong weinan
sanfeng luozong xiaoming   p   ;:file:/d:/study/big-data/code/data/hadoop/mr/ch06/three.txt   1shuaige changmo zhenqiang 
dongli lingu xuanxuan   �   98....
```



### 源码分析

- 注意，本地调试MapTask是串行执行
- 注意，没有改变切片个数，切片个数以输入文件个数为准

```java
// 0
boolean re = job.waitForCompletion(true);
// 1
WholeFileInputFormat
	RecordReader
// 2
MapTask
// 3
WholeRecordReader
	// 读取第一个文件
    nextKeyValue
// 4
Mapper
	run
// 5
SequenceFileMapper
	map
	// nextKeyValue == false 结束map
// 读取下一个切片 ，从1开始到5重复
// 6 依据切片个数重复执行
SequenceFileReducer
	reduce
```





# MapReduce 工作流程

## Map 流程

![mr9](img/hadoop/04.mr09.png)

- 重点
  - 提交信息给yarn
    - job.split 切片信息
    - wc.jar 执行业务程序jar包
    - job.xml 所有的job的配置
  - yarn将信息给Mr appmaster节点
    - 计算MapTask 的数量
    - 开始执行MapTask任务
  - MapTask中
    - `content.write(k,v)`会将数据切片信息，将kv读取放入==内存环形缓存区==
      - 当环形缓存区到80%时，会存储到本地磁盘形成一个==分区==
      - 缓存区默认大小100M
      - 缓存区分隔2块
        - 左边存储元数据
          - index
          - partition，分区
          - keystart
          - valstart
        - 右边存储数据
          - key
          - value
  - combiner 合并
    - 等价于在MapTask阶段执行一次Reduce操作



## Reduce 流程

![mr10](img/hadoop/04.mr10.png)



- 重点
  - GroupingComparator
    - 在reduce之前进行分组处理



## 流程详解

上面的流程是整个MapReduce最全工作流程，但是Shuffle过程只是从第7步开始到第16步结束，具体Shuffle过程详解，如下：

- MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中

- 从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件

- 多个溢出文件会被合并成大的溢出文件

- 在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序

- ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据

- ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）

- 合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法）



## 注意

- Shuffle中的缓冲区大小会影响到MapReduce程序的执行效率

- 原则上：缓冲区越大，磁盘io的次数越少，执行速度就越快

- 缓冲区的大小可以通过参数调整
  - 参数 io.sort.mb
  - 默认100M
- 归并排序（猜测）
  - 考虑到内存，如果多个文件的数据都放在内存中进行归并，肯定存储不下
  - 那么归并的可能只能是索引的归并
    - 2个文件，一个原始数据文件，一个索引文件
    - 通过索引文件可以访问原始文件的数据，通过索引，IO流可以读取一段数据，那么就是该索引对应的值
    - 通过该值进行归并算法的排序，而排序移动的是索引文件。



## 源码解析流程

MapTask

```java
context.write(k, NullWritable.get());
	output.write(key, value);
		collector.collect(key, value,partitioner.getPartition(key, value, partitions));
			HashPartitioner();
		collect()
			close()
				collect.flush()
				sortAndSpill() // 排序和溢写
					sort()   QuickSort // 进行快排，注意：排序时操作顺序的是环形缓存区的左侧索引的位置，而存储数据的右侧位置不变，每次比较时，从元数据获取右侧数据的值比较，然后更改元数据索引的位置，最终输出按照左侧索引的位置，依次从右侧取出数据。减少队列中数据的顺序更换的空间，提升效率
           				 
				mergeParts(); // 合并分区，生成file.out文件给ReduceTask使用
				// file.out  所有的分区数据存储在out中
				// file.out.index 分区的索引，通过索引在file.out中获取相应的分区数据
		collector.close();
```



# Shuffe 机制（重点）

> 数据组合排序的过程

**Map方法之后**

![1566357688004](img/hadoop/04.mr11.png)

- 重点
  - partition分区
    - 默认依据key的hashcode进行分区
    - 每个key-value计算出分区编号partition，作为元数据存储在环形缓冲区内
  - WritableComparable 排序
    - 当从环形缓存区溢出，写出到本地磁盘，对每个分区进行==**快速排序**==
    - 多个写在磁盘上的分区进行==**归并排序**==
    - 最终形成排序完成的分区





**Reduce方法之前**

![1566357816391](img/hadoop/04.mr12.png)



- 重点

  - ReduceTask 去==拉取==相应的分区数据

    - 先存储在内存中
      - 溢出，写在磁盘上
    - 最后归并排序

  - 归并排序

    - Reduce处理数据之间需要进行分区汇总

    - 将同一分区的数据分块写在磁盘上

    - 每写一次磁盘，进行一次==归并排序==

    - 最后对磁盘上所有的该分区文件进行一次归并排序

      

## Partition 分区



### 分区概念

- 将统计的==结果按照要求输出==一个或者多个文件（分区）
- 需求扩展
  - 将结果按照手机归属地的省份不同输出到不同的文件中



### 默认执行分区

- 默认分区按照key的hashCode对ReduceTasks个数取模
- 用户无法控制key可以存到哪个分区中

- 默认实现代码

  - 需要在Driver中添加`job.setNumReduceTasks(n);`
    - n默认为1，直接返回0号分区，不执行 `HashPartitioner` 
    - 设置为>1的值，执行默认`HashPartitioner` 类对象方法
  - MapTask中调用

  ```java
  @Override
  public void write(K key, V value) throws IOException, InterruptedException {
      collector.collect(key, value,
                        partitioner.getPartition(key, value, partitions));
  }
  ```

  - 执行`partitioner.getPartition` 方法获取分区编号

  ```java
  package org.apache.hadoop.mapreduce.lib.partition;
  
  import org.apache.hadoop.classification.InterfaceAudience;
  import org.apache.hadoop.classification.InterfaceStability;
  import org.apache.hadoop.mapreduce.Partitioner;
  
  /** Partition keys by their {@link Object#hashCode()}. */
  @InterfaceAudience.Public
  @InterfaceStability.Stable
  public class HashPartitioner<K, V> extends Partitioner<K, V> {
  
    /** Use {@link Object#hashCode()} to partition. */
    public int getPartition(K key, V value,
                            int numReduceTasks) {
      return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
    }
  }
  ```



### 自定义执行分区



#### 步骤

- 自定义类继承Partitioner，重写getPartition方法
- 在job驱动类中，设置自定义的Partitioner
- 自定义Partition后，要根据自定义Partition的逻辑设置相应数量的ReduceTask



#### 示例

- 需求

  - 在自定义序列化Bean的示例中，对手机号码输出到不同的文件中，进行分区
- 期望输出的数据

  - 136,137,138,139开头的分别放在独立的文件中，其他开头的放在一个文件中

- 代码
  - 自定义实现类

```java
package com.stt.demo.mr.Ch07_partition;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Partitioner;

/**
   * Partitioner 的 key 和 value 是 MapTask阶段 输出的结果类型
   */
public class NumberPartitioner extends Partitioner<Text,FlowBean> {
    @Override
    public int getPartition(Text key, FlowBean value, int numPartitions) {
        String phone = key.toString();
        // 默认输出到0分区
        if(StringUtils.isEmpty(phone)){return 0;}
        if(phone.startsWith("136")){return 1;}
        if(phone.startsWith("137")){return 2;}
        if(phone.startsWith("138")){return 3;}
        if(phone.startsWith("139")){return 4;}
        return 0;
    }
}
```

  - job中的配置，设置NumReduceTask

```java
package com.stt.demo.mr.Ch07_partition;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;

import static com.stt.demo.mr.Constant.*;

public class FlowCountDriver {

    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
        args = new String[]{
            INPUT_PATH_PREFIX+"ch07/input.txt", 	OUTPUT_PATH_PREFIX+"ch07/output"};

        // 配置信息以及job对象
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);
        job.setJarByClass(FlowCountDriver.class);

        job.setMapperClass(FlowCountMapper.class);
        job.setReducerClass(FlowCountReducer.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(FlowBean.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(FlowBean.class);

        FileInputFormat.setInputPaths(job,new Path(args[0]));
        FileOutputFormat.setOutputPath(job,new Path(args[1]));
        // 设置自定义 分区操作类
        job.setPartitionerClass(NumberPartitioner.class);
        // 设置分区的个数
        job.setNumReduceTasks(5);

        boolean re = job.waitForCompletion(true);
        System.exit(re ? 0 : 1);
    }
}
```



#### 小结

- ReduceTask数量 > getPartition的结果数
  - 如设置是setNumReduceTasks(5)，但是在代码中getPartition的最大值是4，那么会建立以空的输出文件part-000xx
- ReduceTask数量 = 1
  - getPartition 的返回结果不生效，最终输出一个part-00000文件
- getPartition > ReduceTask数量 > 1
  - 抛出==IO异常==，文件不存在
- 分区号必须从0开始
- setNumReduceTasks定义分区个数后，会生成设定个数的分区供写入数据



## WritableComparable 排序

- MapTask 和 ReduceTask 都会对数据==按照Key进行排序==
- 属于Hadoop框架的默认行为
- 所有应用程序都会被排序
- 默认排序按照==字典顺序排序==
- 排序算法是==快速排序==
- MapTask 过程进行的排序
  - 环形缓存区到达阈值时，对缓存区内的数据进行一次==快速排序==
  - 写到磁盘
  - 所有数据处理都写在磁盘后，对磁盘所有文件进行==归并排序==

- ReduceTask 过程进行的排序
  - 从MapTask 上远程拷贝相应的数据
    - 文件大小超过阈值，写到磁盘
      - 如果磁盘的文件的数目达到一定个数，进行一次==归并排序==，合成一个文件
    - 文件大小没有超过阈值，写到内存
      - 在内存中的数据继续读取各个MapTaskde 同一分区数据
      - 进行==合并==，超过阈值或设定数据大小，输出到磁盘
  - 所有数据拷贝完成，对磁盘的所有数据统一进行==归并排序==



### 分类

- 部分排序
  - MapReduce 根据输入记录的key对数据集合进行排序
  - 保证输出的每个文件内部有序
- 全排序
  - 本质上是一个部分排序，不过ReduceTask是1
  - 最终结果为一个文件，内部有序
  - 处理大型文件效率低
  - 没有用到MapReduce的并行架构
- 辅助排序
  - GroupingComparator分组
  - 在Reduce端对key进行分组
  - 在接收key为bean对象时，让一个或多个字段相同的key进入到一个reduce方法中
- 二次排序
  - 在定义排序过程中，对compareTo方法中对多个属性进行依次大小比对



### 自定义排序

- 使用bean作为key进行对象传输
- 该bean实现WritableComparable 接口，重写compareTo方法
  - 等价于Writable是序列化接口，在此基础上实现排序功能



#### 示例

- 基于自定义序列化的数据结果进行自定义排序
  - 按照手机总流量大小进行分区排序（全排序就是分区设置为1）
- 要求Mapper阶段的key为bean对象，value为Text类型（手机号）



- 实现FlowBean

```java
package com.stt.demo.mr.Ch08_WritableComparable;

import lombok.Data;
import org.apache.hadoop.io.WritableComparable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.Objects;

/**
 * 用于统计流量的bean
 */
@Data
public class FlowBean implements WritableComparable<FlowBean>{

	private long upFlow;
	private long downFlow;
	private long sumFlow;

	// 反序列化时，需要反射调用空参构造函数
	public FlowBean(){
		super();
	}

	public FlowBean(long upFlow, long downFlow){
		this.upFlow = upFlow;
		this.downFlow = downFlow;
		this.sumFlow = upFlow + downFlow;
	}

	// 写序列化方法
	@Override
	public void write(DataOutput out) throws IOException {
		out.writeLong(upFlow);
		out.writeLong(downFlow);
		out.writeLong(sumFlow);
	}

	// 反序列化方法
	@Override
	public void readFields(DataInput in) throws IOException {
		// 反序列化方法必须要和序列化方法的执行顺序保持一致
		this.upFlow = in.readLong();
		this.downFlow = in.readLong();
		this.sumFlow = in.readLong();
	}

	public String toString(){
		return upFlow+"\t"+downFlow+"\t"+sumFlow;
	}

	@Override
	public int compareTo(FlowBean o) {
		// 按照流量总大小倒叙排列
		if(Objects.equals(sumFlow,o.getSumFlow())){
			return 0;
		}
		return sumFlow > o.getSumFlow() ? -1 : 1;
	}
}
```

- 实现mapper

```java
package com.stt.demo.mr.Ch08_WritableComparable;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * 注意这里的key是FlowBean ,value是Text类型的手机号
 */
public class FlowCountMapper extends Mapper<LongWritable,Text, FlowBean,Text> {

	FlowBean k = new FlowBean();
	Text v = new Text();

	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		// 获取一行数据
		String line = value.toString();
		// 切割字段
		String[] fields = line.split("\t");
		// 封装对象
		String phoneNum = fields[0];
		// 取得上流量和下流量
		long upFlow = Long.parseLong(fields[1]);
		long downFlow = Long.parseLong(fields[2]);
		long sumFlow = Long.parseLong(fields[3]);

		v.set(phoneNum);
		k.setDownFlow(downFlow);
		k.setUpFlow(upFlow);
		k.setSumFlow(sumFlow);

		context.write(k,v);
	}
}
```

- reducer

```java
package com.stt.demo.mr.Ch08_WritableComparable;


import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class FlowCountReducer extends Reducer<FlowBean,Text,Text,FlowBean> {
	@Override
	protected void reduce(FlowBean key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
		// 已经排好序，循环输出，避免总流量相同情况
		for(Text v : values){
			context.write(v,key);
		}
	}
}
```

- partitioner

```java
package com.stt.demo.mr.Ch08_WritableComparable;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Partitioner;

/**
 * Partitioner 的 key 和 value 是 MapTask阶段 输出的结果类型
 */
public class NumberPartitioner extends Partitioner<FlowBean,Text> {
	@Override
	public int getPartition(FlowBean key, Text value, int numPartitions) {
		String phone = value.toString();
		if(StringUtils.isEmpty(phone)){
			// 默认输出到0分区
			return 0;
		}
		if(phone.startsWith("136")){
			return 1;
		}
		if(phone.startsWith("137")){
			return 2;
		}
		if(phone.startsWith("138")){
			return 3;
		}
		if(phone.startsWith("139")){
			return 4;
		}
		return 0;
	}
}
```

- driver

```java
package com.stt.demo.mr.Ch08_WritableComparable;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

import static com.stt.demo.mr.Constant.INPUT_PATH_PREFIX;
import static com.stt.demo.mr.Constant.OUTPUT_PATH_PREFIX;

public class FlowCountDriver {

	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

		args = new String[]{INPUT_PATH_PREFIX+"ch08/input.txt", OUTPUT_PATH_PREFIX+"ch08/output"};

		// 配置信息以及job对象
		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf);
		job.setJarByClass(FlowCountDriver.class);

		job.setMapperClass(FlowCountMapper.class);
		job.setReducerClass(FlowCountReducer.class);

		job.setMapOutputKeyClass(FlowBean.class);
		job.setMapOutputValueClass(Text.class);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(FlowBean.class);

		FileInputFormat.setInputPaths(job,new Path(args[0]));
		FileOutputFormat.setOutputPath(job,new Path(args[1]));
		// 设置自定义 分区操作类
		job.setPartitionerClass(NumberPartitioner.class);
		// 设置分区的个数
		job.setNumReduceTasks(5);

		boolean re = job.waitForCompletion(true);
		System.exit(re ? 0 : 1);
	}

}
```



## Combiner 合并

- MR程序中Mapper和Reducer之外的一种组件
- Combiner组件的父类是Reducer
  - 等于是在MapTask阶段执行一次Reducer操作，对数据进行初步合并
- 与Reducer的区别
  - Combiner在每个MapTask所在的节点处运行
  - Reducer在接收全局的所有Mapper的输出结果
- 作用
  - 对每个MapTask的输出进行局部汇总
  - 减少网络IO传输
- 使用前提
  - 不能影响整体的业务逻辑
    - 如不能使用在求平均数的场景
  - Combiner的输出与Reducer的输入KV要对应



### 自定义Combiner实现

> 以基本的wordCount为例

- 自定义一个Combiner 继承Reducer，重写Reduce方法

```java
package com.stt.demo.mr.Ch09_Combiner;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class WordCombiner extends Reducer<Text,IntWritable,Text,IntWritable>  {
	
	IntWritable val = new IntWritable();
	
	@Override
	protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
		int sum = 0;
		for(IntWritable v : values){
			sum += v.get();
		}
		val.set(sum);
		context.write(key,val);
	}
}
```

- 在job中进行登记

```java
// 登记combiner
job.setCombinerClass(WordCombiner.class);
```

- 观察执行结果

![1566380040496](img/hadoop/04.mr13.png)





## GroupingComparator 分组

- 辅助排序
- 可以用于实现topN的功能
- 在reduce之前对key进行分组
  - 通过key的bean的某些字段判断进行输出



### 实现

- 自定义类继承`WritableComparator`
- 重写`compare`方法，创建构造器

```java
@Override
public int compare(WritableComparable a, WritableComparable b) {
		// 比较的业务逻辑
		return result;
}

protected OrderGroupingComparator() {
    // 注意，这里必须传参true
    super(OrderBean.class, true);
}
```



### 示例

- 需求
  - 取出每个订单中成交金额最大的

| 订单id  | 商品id | 成交金额 |
| ------- | ------ | -------- |
| 0000001 | Pdt_01 | 222.8    |
|         | Pdt_02 | 33.8     |
| 0000002 | Pdt_03 | 522.8    |
|         | Pdt_04 | 122.4    |
|         | Pdt_05 | 722.4    |
| 0000003 | Pdt_06 | 232.8    |
|         | Pdt_02 | 33.8     |

- 输入数据

```text
0000001	Pdt_01	222.8
0000002	Pdt_05	722.4
0000001	Pdt_02	33.8
0000003	Pdt_06	232.8
0000003	Pdt_02	33.8
0000002	Pdt_03	522.8
0000002	Pdt_04	122.4
0000002	Pdt_03	522.8
0000002	Pdt_04	122.4
0000002	Pdt_03	522.8
0000002	Pdt_04	122.4
```

- 分析
  - 利用“订单id和成交金额”作为key
    - 将Map阶段读取到的所有订单数据按照id升序排序
    - 二次排序：id相同再按照金额降序排序
    - 发送到Reduce
  - 在Reduce端利用groupingComparator将订单id相同的kv聚合成组，取第一个即是该订单中最贵商品

  ![1566396213153](img/hadoop/04.mr14.png)

- 代码
- 实现OrderBean

```java
package com.stt.demo.mr.Ch10_GroupingComparator;

import lombok.Data;
import org.apache.hadoop.io.WritableComparable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.Objects;

@Data
public class OrderBean implements WritableComparable<OrderBean>{

	private int orderId;
	private double price;

	@Override
	public void write(DataOutput out) throws IOException {
		out.writeInt(orderId);
		out.writeDouble(price);
	}

	@Override
	public void readFields(DataInput in) throws IOException {
		orderId = in.readInt();
		price = in.readDouble();
	}

	public String toString(){
		return orderId + "\t" + price;
	}

	@Override
	public int compareTo(OrderBean o) {
		if(Objects.equals(orderId,o.getOrderId())){
			if(Objects.equals(price,o.getPrice())){
				return 0;
			}
			// 按照价格降序
			return price > o.getPrice() ? -1 : 1;
		}
		// 按照orderId默认升序
		return orderId > o.getOrderId() ? 1 : -1;
	}
}
```

- mapper

```java
package com.stt.demo.mr.Ch10_GroupingComparator;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class OrderMapper extends Mapper<LongWritable,Text,OrderBean,NullWritable> {

	OrderBean k = new OrderBean();

	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {

		//0000001	Pdt_01	222.8
		String[] lines = value.toString().split("\\s+");

		k.setOrderId(Integer.parseInt(lines[0]));
		k.setPrice(Double.parseDouble(lines[2]));

		context.write(k,NullWritable.get());
	}
}
```

- groupingComparator

```java
package com.stt.demo.mr.Ch10_GroupingComparator;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

import java.util.Objects;

public class OrderGroupingComparator extends WritableComparator {

	// 这里必须要传递true，用于实例化，否则报异常
	protected OrderGroupingComparator(){
		super(OrderBean.class,true);
	}

	@Override
	public int compare(WritableComparable a, WritableComparable b) {
		// 在相同的id处进行分组处理
		// 由于在MapTask阶段进行了二次排序
		// 到此处compare的是排序后的key对象
		OrderBean aOrder = (OrderBean) a;
		OrderBean bOrder = (OrderBean) b;

		if(Objects.equals(aOrder.getOrderId(),bOrder.getOrderId())){
			return 0;
		}
		return aOrder.getOrderId() > bOrder.getOrderId() ? 1 : -1;
	}
}
```

- reducer

```java
package com.stt.demo.mr.Ch10_GroupingComparator;

import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class OrderReducer extends Reducer<OrderBean,NullWritable,OrderBean,NullWritable> {

	@Override
	protected void reduce(OrderBean key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {
		context.write(key,NullWritable.get());
	}
}
```

- driver

```java
// 添加OrderGroupingComparator
job.setGroupingComparatorClass(OrderGroupingComparator.class);
```



### 源码分析

- 在`OrderGroupingComparator` 的 `compare` 方法上打上断点
- 在`OrderReducer` 的 `reduce` 上打上断点
- 执行后，step out 得到调用方法

```java
Reducer
	run()
    	while(content.nextKey())
            reduce(...) // reduce执行

ReduceContextImpl：nextKey()  
    while(nextKeyIsSame){ // 
		nextKeyValue() // 如果下一个相同则nextKeyIsSame = true，跳过，继续比较
    }
	return nextKeyValue() // nextKeyIsSame == false , 
   
nextKeyValue()
   WritableComparator:compare // 比较2个orderBean的id是否相同
       nextKeyIsSame = true ? false; // 给nextKeyIsSame设置值
   return true; // 始终返回true
```

- `nextKeyValue` 方法解析
  - 始终返回true
  - 预先到达下一个键值对
  - 比较当前key和下一个key，设置`nextKeyIsSame`的值
  - 每次调用切换到下一个键值对
    - key的切换：`key = keyDeserializer.deserialize(key)`
    - value的切换：`value = valueDeserializer.deserialize(value)`
    - 切换之前对 `buffer` 进行了 `reset` 操作

```java

public boolean nextKeyValue() throws IOException, InterruptedException {    
 ...
    DataInputBuffer nextKey = input.getKey();
    currentRawKey.set(nextKey.getData(), nextKey.getPosition(), 
                      nextKey.getLength() - nextKey.getPosition());
    buffer.reset(currentRawKey.getBytes(), 0, currentRawKey.getLength());
    // 对key进行反序列化操作，原因是节点之间远程通信传递数据，同时切换到下一个key
    key = keyDeserializer.deserialize(key);
    DataInputBuffer nextVal = input.getValue();
    buffer.reset(nextVal.getData(), nextVal.getPosition(), nextVal.getLength()
                 - nextVal.getPosition());
    value = valueDeserializer.deserialize(value);

    currentKeyLength = nextKey.getLength() - nextKey.getPosition();
    currentValueLength = nextVal.getLength() - nextVal.getPosition();

    if (isMarked) {
        backupStore.write(nextKey, nextVal);
    }

    hasMore = input.next();
    if (hasMore) {
        nextKey = input.getKey();
        nextKeyIsSame = 
            // 调用比较器比较
            comparator.compare(currentRawKey.getBytes(), 0, 
                       currentRawKey.getLength(),
                       nextKey.getData(),
                       nextKey.getPosition(),
                       nextKey.getLength() - nextKey.getPosition()) == 0;
    } else {
        nextKeyIsSame = false;
    }
    inputValueCounter.increment(1);
    return true;
}
```



### 扩展topN

- 需求扩展，如果获取分组的最大前2项
- 修改OrderReducer
  - 此时打印了该分组的所有key-value值
  - 为什么会打印所有?

```java
package com.stt.demo.mr.Ch10_GroupingComparator;

import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class OrderReducer extends Reducer<OrderBean,NullWritable,OrderBean,NullWritable> {

	@Override
	protected void reduce(OrderBean key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {
		for(NullWritable v : values){
			context.write(key,NullWritable.get());
		}
	}
}
```



#### 分析

- 如何做到，因为`OrderGroupingComparator`处理后每次只会有一个分组的头部的key进入reduce方法

- 查看源码

  - 关注reduce内的循环，会调用迭代器的next()方法

  ```java
  // 注意循环
  for(NullWritable v : values)
  ```

  - reduce方法的执行，在Reducer中被执行

  ```java
  // Reducer：：run(Context context)
  while (context.nextKey()) {
      reduce(context.getCurrentKey(), context.getValues(), context);
  
  // 注意，values的传值是通过 context.getValues()
      // 查看context对象
  public abstract class Context 
      implements ReduceContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {
  }
      // 对应的实现ReduceContextImpl，查看getValues()方法
      // 是一个自定义的迭代器实现
  public Iterable<VALUEIN> getValues() {
      return iterable;
  }
      // 该迭代器有声明
  private ValueIterable iterable = new ValueIterable();
      
      // 找到ValueIterable类的实现
  protected class ValueIterable implements Iterable<VALUEIN> {
      private ValueIterator iterator = new ValueIterator();
      @Override
      public Iterator<VALUEIN> iterator() {
        return iterator;
      } 
  }
  	// 分析ValueIterator 对象的实现方法
  protected class ValueIterator implements ReduceContext.ValueIterator<VALUEIN>
      // 查看next方法
  public VALUEIN next() {
  ...
      try {
          // 重点：这里进行了nextKeyValue 的操作，key和value被更新了
          nextKeyValue();
          return value;
      } catch (IOException ie) {
       ...
      }
  }
  ```



#### 实现

- 修改 reducer 代码

```java
protected void reduce(OrderBean key, Iterable<NullWritable> values, Context context) {  // 获取前2名
    int topN = 2;
    for(NullWritable v : values){
        if(topN <= 0){
            break;
        }
        context.write(key,NullWritable.get());
        topN --;
    }
}
```



# MapTask 机制

![1566450823002](img/hadoop/04.mr15.png)

- Read阶段

  - MapTask通过用户编写的RecordReader，从输入InputFormat中解析出一个个key/value。

- Map阶段
  - 将解析出的key/value交给用户编写map()函数处理
  - 产生一系列新的key/value
- Collect收集阶段
  - 在编写map()函数中
  - 当数据处理完成后，一般会调用OutputCollector.collect()输出结果
    - 在该函数内部，将生成的key/value分区（调用Partitioner）写入环形内存缓冲区

- Spill阶段，即“溢写”
  - 当环形缓冲区满后，将数据写到本地磁盘上，生成一个临时文件
    - 将数据写入本地磁盘之前，先对数据进行一次本地排序
    - 在必要时对数据进行合并、压缩等操作

  - 步骤1

    - 利用快速排序算法对缓存区内的数据进行排序

      - 先按照分区编号Partition进行排序

      - 然后按照key进行排序

    - 经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。

  - 步骤2

    - 按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中
    - 如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。

  - 步骤3

    - 将分区数据的**元信息**写到内存索引数据结构SpillRecord中
    - 每个分区的元信息包括
      - 在临时文件中的偏移量
      - 压缩前数据大小
      - 压缩后数据大小
    - 当前内存索引大小超过1MB则将内存索引写到文件output/spillN.out.index中

- Combine阶段
  - 所有数据处理完成后
  - MapTask对所有临时文件进行一次合并
    - 以分区为单位进行合并
    - 采用多轮递归合并的方式
    - 每轮合并io.sort.factor（默认10）个文件
    - 将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件
  - 确保每个MapTask最终只会生成一个数据文件
    - 避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销
  - 保存到文件output/file.out中
  - 生成相应的索引文件output/file.out.index




# ReduceTask 机制



![1566452750986](img/hadoop/04.mr16.png)



- Copy阶段
  - 从各个MapTask上远程拷贝一片数据
  - 针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。

- Merge阶段
  - 在远程拷贝数据的同时
  - ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并
    - 防止内存使用过多或磁盘上文件过多

- Sort阶段
  - 按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据
  - 为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略
  - 由于各个MapTask已经实现对自己的处理结果进行了局部排序
    - ReduceTask只需对所有数据进行一次归并排序即可。

- Reduce阶段

  - reduce()函数将计算结果写到HDFS上

    

## 设置ReduceTask并行度

- ReduceTask 并行度同样影响整个Job的执行并发度和执行效率
- 与MapTask 并发数由切片数决定不同
- ReduceTask 数量可直接手动设置

```java
// 默认值是1，手动设置为4
job.setNumReduceTasks(4);
```



- 实验：测试ReduceTask多少合适
  - 实验环境：1个Master节点，16个Slave节点：CPU:8GHZ，内存: 2G
  - 实验结论
    - 并行度提高，反而会降低效率

| MapTask =16 |      |      |      |      |      |      |      |      |      |      |
| ----------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| ReduceTask  | 1    | 5    | 10   | 15   | 16   | 20   | 25   | 30   | 45   | 60   |
| 总时间      | 892  | 146  | 110  | 92   | 88   | 100  | 128  | 101  | 145  | 104  |

- 注意事项
  - ReduceTask=0
    - 没有Reduce阶段，输出文件个数与Map个数一致
    - 实际生产推荐，如果可以在map阶段解决的话
    - 少了Shuffe阶段和reduce阶段
      - 其中Shuffe阶段需要大量的排序，IO操作，耗时严重
  - ReduceTask 的默认值 1，输出文件就一个
  - 如果数据分布不均衡，在reduce阶段产生数据倾斜
  - ReduceTask的数量不是任意设置的
    - 考虑业务逻辑需求
    - 计算全局汇总结果，只能是1个ReduceTask
  - 具体多少ReduceTask需要依照集群性能而定
  - 如果分区不是1，但是ReduceTask = 1，那么不会执行分区过程
    - 在源码中对`ReduceNum == 1`进行了判断
    - 大于1才会执行分区过程



# OutputFormat 数据输出

- OutputFormat 是MapReduce 输出的基类
- 在reduce阶段执行之后进行

- 类型
  - TextOutputFormat
    - 文本输出
    - MapReduce的默认输出
    - 每条记录写为文本
    - 键值是任意类型，因为调用`toString()`方法转换为字符串
  - SequenceFileOutputFormat
    - 作为后续MapReduce的任务输入
    - 多job串联使用
    - 格式紧凑，容易压缩
  - 自定义OutputFormat
    - 依据需求，自定义实现输出
      - 输出到不同文件夹
      - 输出到mysql，hbase等
        - 可以先输出到mq，然后批量插入mysql



## 自定义OutputFormat

- 自定义一个类继承FileOutputFormat
- 改写RecordWriter
  - 具体改写输出数据的write方法



### 示例

- 需求：过滤输入的log日志，包含atguigu的网站输出到e:/atguigu.log，不包含atguigu的网站输出到e:/other.log
- 数据

```text
http://www.baidu.com
http://www.google.com
http://cn.bing.com
http://www.atguigu.com
http://www.sohu.com
http://www.sina.com
http://www.sin2a.com
http://www.sin2desa.com
http://www.sindsafa.com
```

- 期望输出2个文件，一个atguigu.log，一个other.log

实现

- mapper

```java
public class FilterMapper extends Mapper<LongWritable,Text,Text,NullWritable> {
	
	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		context.write(value,NullWritable.get());
	}
}
```

- reducer

```java
public class FilterReducer extends Reducer<Text,NullWritable,Text,NullWritable> {
    @Override
    protected void reduce(Text key, Iterable<NullWritable> values, Context context) throws IOException, InterruptedException {
        // 针对相同的key也进行输出
        for(NullWritable val : values){
            key.set(key.toString()+"\r\n");
            context.write(key,NullWritable.get());
        }
    }
}
```

- outputFormat

```java
package com.stt.demo.mr.Ch11_CustomizedOutputFormat;

import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class FilterOutputFormat extends FileOutputFormat<Text,NullWritable> {

	@Override
	public RecordWriter<Text, NullWritable> getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException {
		return new FilterRecordWriter(context);
	}
}
```

- recordWriter

```java
package com.stt.demo.mr.Ch11_CustomizedOutputFormat;

import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;

import java.io.IOException;

import static com.stt.demo.mr.Constant.OUTPUT_PATH_PREFIX;

public class FilterRecordWriter extends RecordWriter<Text, NullWritable> {

	private FSDataOutputStream output1 = null;
	private FSDataOutputStream output2 = null;

	public FilterRecordWriter(TaskAttemptContext context) {
		try{
			FileSystem fs = FileSystem.get(context.getConfiguration());
			output1 = fs.create(new Path(OUTPUT_PATH_PREFIX+"ch11/output/atguigu.log"));
			output2 = fs.create(new Path(OUTPUT_PATH_PREFIX+"ch11/output/other.log"));
		}catch (Exception e){
			e.printStackTrace();
		}
	}

	@Override
	public void write(Text key, NullWritable value) throws IOException, InterruptedException {
		FSDataOutputStream output = key.toString().contains("atguigu") ? output1 : output2;
		output.write(key.toString().getBytes());
	}

	@Override
	public void close(TaskAttemptContext context) throws IOException, InterruptedException {
		IOUtils.closeStream(output1);
		IOUtils.closeStream(output2);
	}
}
```

- driver

```java
package com.stt.demo.mr.Ch11_CustomizedOutputFormat;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

import static com.stt.demo.mr.Constant.INPUT_PATH_PREFIX;
import static com.stt.demo.mr.Constant.OUTPUT_PATH_PREFIX;

public class FilterDriver {

	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

		args = new String[]{INPUT_PATH_PREFIX+"ch11/input.txt", OUTPUT_PATH_PREFIX+"ch11/output"};

		Job job = Job.getInstance(new Configuration());

		job.setJarByClass(FilterDriver.class);

		job.setMapperClass(FilterMapper.class);
		job.setReducerClass(FilterReducer.class);

		// 添加自定义的outputFormat
		job.setOutputFormatClass(FilterOutputFormat.class);

		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(NullWritable.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(NullWritable.class);

		FileInputFormat.setInputPaths(job,new Path(args[0]));
		// 虽然我们自定义了outputformat，但是因为我们的outputformat继承自fileoutputformat
		// 而fileoutputformat要输出一个_SUCCESS文件，所以，在这还得指定一个输出目录
		FileOutputFormat.setOutputPath(job,new Path(args[1]));

		boolean result = job.waitForCompletion(true);
		System.exit(result ? 0 : 1);
	}
}
```



# Join 多种应用（重点）



## ReduceJoin

- 在reduce阶段进行join处理
- 原理
  - 将2个文件的公共部分（连接字段）作为key
  - Map端
    - 来自不同表或者文件的key/value
    - **打上标签区别不同的记录**
      - 比如来源，文件名称等，用于区分
    - 连接字段使用key
    - 其余部分使用value
    - 最后输出
  - Reduce端
    - 对每个分组记录进行过滤，得到不同标签的数据
    - 对不同标签的数据进行合并

### 示例



**需求**

- order 订单表

| **id**   | **pid** | **amount** |
| -------- | ------- | ---------- |
| **1001** | **01**  | **1**      |
| **1002** | **02**  | **2**      |
| **1003** | **03**  | **3**      |
| **1004** | **01**  | **4**      |
| **1005** | **02**  | **5**      |
| **1006** | **03**  | **6**      |
- product 商品表

| pid  | pname |
| ---- | ----- |
| 01   | 小米  |
| 02   | 华为  |
| 03   | 格力  |

- 将order表中的pid替换为pname



**分析**

- 通过将关联条件作为Map输出的key
- 将2表满足join条件的数据发送到同一个ReduceTask处理
- 在Reduce中进行数据的串联

![1566474358836](img/hadoop/04.mr17.png)



**代码**

- bean

```java
package com.stt.demo.mr.Ch12_ReduceJoin;

import lombok.Data;
import lombok.NoArgsConstructor;
import org.apache.hadoop.io.Writable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

@Data
@NoArgsConstructor
public class TableBean implements Writable {

    private String orderId; // 订单id
    private String pId;      // 产品id
    private int amount;       // 产品数量
    private String pName;     // 产品名称
    private String flag;      // 表的标记

    @Override
    public void write(DataOutput out) throws IOException {

        out.writeUTF(orderId);
        out.writeUTF(pId);
        out.writeInt(amount);
        out.writeUTF(pName);
        out.writeUTF(flag);
    }

    @Override
    public void readFields(DataInput in) throws IOException {

        this.orderId = in.readUTF();
        this.pId = in.readUTF();
        this.amount = in.readInt();
        this.pName = in.readUTF();
        this.flag = in.readUTF();
    }

    public String toString(){
        return orderId + "\t" + pName + "\t" + amount;
    }
}
```

- mapper

```java
package com.stt.demo.mr.Ch12_ReduceJoin;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;

import java.io.IOException;

public class TableMapper extends Mapper<LongWritable,Text,Text,TableBean> {

    private String fileName;

    TableBean v = new TableBean();
    Text k = new Text();

    @Override
    protected void setup(Context context) throws IOException, InterruptedException {
        // 获取切片的名称
        fileName = ((FileSplit)context.getInputSplit()).getPath().getName();
    }

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] fields = value.toString().split("\\s+");

        if(fileName.startsWith("order")){
            // 1001	01	1
            v.setOrderId(fields[0]);
            v.setPId(fields[1]);
            v.setAmount(Integer.parseInt(fields[2]));
            v.setFlag("order");
            v.setPName("");
        }else if(fileName.startsWith("pd")){
            // 01	小米
            v.setPId(fields[0]);
            v.setPName(fields[1]);
            v.setFlag("pd");
            v.setAmount(0);
            v.setOrderId("");
        }
        k.set(v.getPId());
        context.write(k,v);
    }
}
```

- reducer
  - 重点关注迭代器源码，此处是Hadoop重写了迭代器

```java
package com.stt.demo.mr.Ch12_ReduceJoin;

import org.apache.commons.beanutils.BeanUtils;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
import java.lang.reflect.InvocationTargetException;
import java.util.ArrayList;
import java.util.List;

public class TableReducer extends Reducer<Text,TableBean,TableBean,NullWritable> {

    @Override
    protected void reduce(Text key, Iterable<TableBean> values, Context context) throws IOException, InterruptedException {

        List<TableBean> orders = new ArrayList<>();
        TableBean pdBean = null;

        // 需要注意此处迭代器源码
        // 这里使用深拷贝的原因
        //      由于此处values的迭代器是Hadoop改写的，每次val都是最新的值，而非不同的引用，为了节省内存空间
        //      因此如果直接orders.add(val); 那么会导致orders里面的所有bean都是同一个对象（最后一个对象）
        for(TableBean val : values){
            if("order".equals(val.getFlag())){
                orders.add(copy(val));
            }else if("pd".equals(val.getFlag())){
                pdBean = copy(val);
            }
        }
        for(TableBean val : orders){
            if(pdBean != null){
                val.setPName(pdBean.getPName());
            }
            context.write(val,NullWritable.get());
        }
    }

    private TableBean copy(TableBean source){
        TableBean tmp = new TableBean();
        try {
            // 最好不要使用spring框架的提供的方法
            // 因为会打包普通的jar，而hadoop里面可能没有spring的类库
            // 深度拷贝
            BeanUtils.copyProperties(tmp,source);
        } catch (IllegalAccessException e) {
            e.printStackTrace();
        } catch (InvocationTargetException e) {
            e.printStackTrace();
        }
        return tmp;
    }
}
```

- driver

```java
public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {

    args = new String[]{INPUT_PATH_PREFIX+"ch12/input", OUTPUT_PATH_PREFIX+"ch12/output"};

    Job job = Job.getInstance(new Configuration());

    job.setJarByClass(TableDriver.class);

    job.setMapperClass(TableMapper.class);
    job.setReducerClass(TableReducer.class);

    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(TableBean.class);
    job.setOutputKeyClass(TableBean.class);
    job.setOutputValueClass(NullWritable.class);

    FileInputFormat.setInputPaths(job,new Path(args[0]));
    FileOutputFormat.setOutputPath(job,new Path(args[1]));

    boolean result = job.waitForCompletion(true);
    System.exit(result ? 0 : 1);
}
}
```



### 缺点

- 都放在reduce处理，大量相同的pid数据在到一个reduce中执行
- reduce端处理压力大
- 会导致数据倾斜
- map节点的运算负载很低，资源利用率不高
- 为了解决这个问题一般使用mapJoin的方式



## MapJoin

- 在map阶段进行join处理
- 适用于一张小表，一张大表的场景
- 在map端缓存多张表，提前处理业务逻辑
  - 增加map端的业务，减少reduce端数据压力，减少数据倾斜
- 具体实现
  - 采用DistributedCache
  - 在mapper的setup阶段，将文件读取到缓存集合中
  - 在驱动函数中加载缓存



### 示例

需求和ReduceJoin保持一致

**代码**

- driver 中添加缓存路径

```java
package com.stt.demo.mr.Ch13_MapJoin;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

import static com.stt.demo.mr.Constant.*;

public class DistributedCacheDriver  {

	public static void main(String[] args) throws Exception {

		args = new String[]{INPUT+"ch13/order.txt", OUTPUT+"ch13/output"};

		Job job = Job.getInstance(new Configuration());
		job.setJarByClass(DistributedCacheDriver .class);
		job.setMapperClass(DistributedCacheMapper.class);

		// 设置为0，没有Reducer阶段处理
		job.setNumReduceTasks(0);
		// 增加缓存文件路径，map阶段从setup读取
		job.addCacheFile(new URI("file://"+INPUT+"ch13/pd.txt"));

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(NullWritable.class);

		FileInputFormat.setInputPaths(job,new Path(args[0]));
		FileOutputFormat.setOutputPath(job,new Path(args[1]));

		boolean result = job.waitForCompletion(true);
		System.exit(result ? 0 : 1);
	}
}
```

- mapper

```java
package com.stt.demo.mr.Ch13_MapJoin;

import org.apache.commons.io.Charsets;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class DistributedCacheMapper extends Mapper<LongWritable,Text,Text,NullWritable> {

	Map<String,String> pdMap = new HashMap<>();
	Text k = new Text();

	@Override
	protected void setup(Context context) throws IOException, InterruptedException {
		// 从缓存中读取pd表信息
		List<String> lines = Files.readAllLines(Paths.get(context.getCacheFiles()[0]), Charsets.UTF_8);
		for(String line : lines){
			// 01	小米
			String[] fields = line.split("\\s+");
			pdMap.put(fields[0],fields[1]);
		}
	}

	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        // 1001	01	1
		String[] fields = value.toString().split("\\s+");
		k.set(fields[0] + "\t" + pdMap.get(fields[1]) + "\t" + fields[2]);
		context.write(k, NullWritable.get());
	}
}
```





# 计数器应用

- Hadoop内置计数器，用于描述多项指标
  - 计已处理字节数，记录数
- 用户可以监控已处理的输入数据量，已生产的输出数据量

- 如`File System Counters`就是 计数器的groupName ，后面是计数器的名称

```java
16:01:07.002 [main] INFO org.apache.hadoop.mapreduce.Job - Counters: 30
	File System Counters
		FILE: Number of bytes read=1226
		FILE: Number of bytes written=584137
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=194
```



- 计数器API

  - 使用枚举方式

  ```java
  enum MyCounter{NORMAL,ERROR}
  //针对定义的枚举自定义计数器+1
  context.getCounter(MyCounter.NORMAL).increment(1);
  ```

  - 使用计数器组，计数器名称方式

  ```java
  context.getCounter("counterGroup","myCounter").increment(1);
  ```

- 一般在数据清洗等场景使用



# ETL 数据清洗

- 在运行核心业务MapReduce程序之前
- 对数据进行清洗，清理掉不符合用户要求的数据
- 清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序



**简单示例**

- 需求：去除日志中长度<=11的日志

- mapper
```java
package com.stt.demo.mr.Ch14_ETL;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class LogMapper extends Mapper<LongWritable,Text,Text,NullWritable> {

	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		if(parseLog(value)){
			context.write(value,NullWritable.get());
		}
	}
	
	private boolean parseLog(Text value) {
		return value.toString().split("\\s+").length > 11 ;
	}
}
```

- driver

```java
package com.stt.demo.mr.Ch14_ETL;


import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import static com.stt.demo.mr.Constant.*;

public class LogDriver {

	public static void main(String[] args) throws Exception {

		args = new String[]{INPUT+"ch14/log.txt", OUTPUT+"ch14/output"};

		Job job = Job.getInstance(new Configuration());
		job.setJarByClass(LogDriver.class);
		job.setMapperClass(LogMapper.class);

		// 设置为0，没有Reducer阶段处理
		job.setNumReduceTasks(0);

		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(NullWritable.class);

		FileInputFormat.setInputPaths(job,new Path(args[0]));
		FileOutputFormat.setOutputPath(job,new Path(args[1]));

		boolean result = job.waitForCompletion(true);
		System.exit(result ? 0 : 1);
	}
}
```



**复杂示例**

- 定义一个bean，用来记录日志数据中的各数据字段

```java
@Data
public class LogBean {
	private String remote_addr;// 记录客户端的ip地址
	private String remote_user;// 记录客户端用户名称,忽略属性"-"
	private String time_local;// 记录访问时间与时区
	private String request;// 记录请求的url与http协议
	private String status;// 记录请求状态；成功是200
	private String body_bytes_sent;// 记录发送给客户端文件主体内容大小
	private String http_referer;// 用来记录从那个页面链接访问过来的
	private String http_user_agent;// 记录客户浏览器的相关信息
	private boolean valid = true;// 判断数据是否合法

	@Override
	public String toString() {
		// 使用特殊字符分隔，日志上有空格属于正常字符
		StringBuilder sb = new StringBuilder(this.valid);
		sb.append("\001").append(this.remote_addr);
		sb.append("\001").append(this.remote_user);
		sb.append("\001").append(this.time_local);
		sb.append("\001").append(this.request);
		sb.append("\001").append(this.status);
		sb.append("\001").append(this.body_bytes_sent);
		sb.append("\001").append(this.http_referer);
		sb.append("\001").append(this.http_user_agent);
		return sb.toString();
	}
}
```

- mapper

```java
import java.io.IOException;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class LogMapper extends Mapper<LongWritable, Text, Text, NullWritable>{
	Text k = new Text();
	
	@Override
	protected void map(LongWritable key, Text value, Context context)	throws IOException, InterruptedException {

		// 1 获取1行
		String line = value.toString();
		// 2 解析日志是否合法
		LogBean bean = parseLog(line);
		if (!bean.isValid()) {
			return;
		}
		k.set(bean.toString());
		// 3 输出
		context.write(k, NullWritable.get());
	}

	// 解析日志
	private LogBean parseLog(String line) {
		LogBean logBean = new LogBean();
		// 1 截取
		String[] fields = line.split(" ");
		if (fields.length > 11) {
			// 2封装数据
			logBean.setRemote_addr(fields[0]);
			logBean.setRemote_user(fields[1]);
			logBean.setTime_local(fields[3].substring(1));
			logBean.setRequest(fields[6]);
			logBean.setStatus(fields[8]);
			logBean.setBody_bytes_sent(fields[9]);
			logBean.setHttp_referer(fields[10]);
			if (fields.length > 12) {
				logBean.setHttp_user_agent(fields[11] + " "+ fields[12]);
			}else {
				logBean.setHttp_user_agent(fields[11]);
			}
			// 大于400，HTTP错误
			if (Integer.parseInt(logBean.getStatus()) >= 400) {
				logBean.setValid(false);
			}
		}else {
			logBean.setValid(false);
		}
		return logBean;
	}
}
```

- driver

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class LogDriver {
	public static void main(String[] args) throws Exception {
	    //1 获取job信息
		Job job = Job.getInstance(new Configuration());
		// 2 加载jar包
		job.setJarByClass(LogDriver.class);
		// 3 关联map
		job.setMapperClass(LogMapper.class);
		// 4 设置最终输出类型
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(NullWritable.class);
		// 5 设置输入和输出路径
		FileInputFormat.setInputPaths(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		// 6 提交
		job.waitForCompletion(true);
	}
}
```




# 总结

- InputFormat 输入数据接口
  - 默认使用实现类 TextInputFormat
    - 一次读取一行文本
    - 将改行的起始偏移量作为key，行内容作为value返回
  - KeyValueTextInputFormat
    - 一行为一条记录
    - 被分隔符分隔为key，value
    - 默认分隔符为tab -- \t
  - NlineInputFormat
    - 按照指定的行数N进行分片
  - CombineTextInputFormat
    - 将多个小文件合并为一个切片进行处理
    - 提高小文件效率
  - 自定义InputFormat
- Mapper 逻辑处理接口
  - 依据业务实现3个方法
    - setup
    - map
    - cleanup
- Partitioner 分区
  - hashPartitioner 默认实现
    - 依据key的hashCode和numReduces返回一个分区号
  - 自定义分区
- Comparable 排序
  - 自定义对象作为key，需要实现WritableComparable接口
    - 重写compareTo方法
  - 部分排序，对最终的每个输出文件内部排序
  - 全排序，对所有数据进行排序，通常只有一个Reduce
  - 二次排序，排序条件多个
- Combiner 合并
  - 提高效率，在mapTask生成分区文件进行一次简单的reduce操作
- GroupingComparator 分组
  - Reduce端进行key分组
  - 接收key为bean对象时，让多个字段相同的key进入到同一个reduce方法
  - 可以实现分组排序功能
- Reducer 逻辑处理接口
  - 实现3个方法
    - setup
    - reduce
    - cleanup
- OutputFormat 输出数据接口
  - 默认实现 TextOutputFormat
    - 将每一个KV对，向目标文件输出一行
  - SequenceFileOutputFormat
    - 输出后作为后续MapReduce任务的输入
    - 格式紧凑，易于压缩
  - 自定义OutputFormat