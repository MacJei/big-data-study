# Dstream创建-数据源

- Spark Streaming原生支持一些不同的数据源
- 一些“核心”数据源已经被打包到Spark Streaming 的 Maven 工件中，其他的一些则通过 spark-streaming-kafka 等附加工件获取
- 每个接收器都以 Spark 执行器程序中一个长期运行任务的形式执行，会占据分配给应用的 CPU 核心
  - 还需要有可用的 CPU 核心来处理数据
  - 如果要运行多个接收器，就必须至少有
    - ==接收器数目相同的核心数+用来完成计算所需要的核心数==
    - 如要在流计算应用中运行 10 个接收器，那么至少需要为应用分配 11 个 CPU 核心
    - 如在本地模式运行，不要使用local[1]



## 文件数据源 [textFileStream]

- 文件数据流

  - 能够读取所有HDFS API兼容的文件系统文件，通过fileStream方法进行读取
  - Spark Streaming 将会监控 dataDirectory 目录并不断处理移动进来的文件

- 注意

  - 目前不支持嵌套目录

  - 文件需要有相同的数据格式

  - 文件进入 dataDirectory的方式需要通过移动或者重命名来实现

  - 一旦文件移动进目录，则不能再修改，即便修改了也不会读取新数据

    

示例

- 从hdfs上读取变化的文件
- 创建目录与文件

```bash
[ttshe@hadoop102 data]$ vim a.txt
[ttshe@hadoop102 data]$ vim b.txt
[ttshe@hadoop102 data]$ cp a.txt c.txt
[ttshe@hadoop102 data]$ hadoop fs -mkdir /fileStream
```

- 执行代码

```scala
package com.stt.spark.streaming

import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}

object Ch02_FileSource {
    def main(args: Array[String]): Unit = {

        var conf = new SparkConf().setMaster("local[*]").setAppName("Ch02_FileSource")
        // 5s读取一次
        val context: StreamingContext = new StreamingContext(conf,Seconds(5))
        // 监听指定的文件夹，5s读取一次数据
        val dstream: DStream[String] = context.textFileStream("hdfs://hadoop102:9000/fileStream")
        // 将读取的数据扁平化
        val wordStream: DStream[String] = dstream.flatMap(_.split(" "))

        val tupleDstream: DStream[(String, Int)] = wordStream.map(w=>(w,1))

        val result: DStream[(String, Int)] = tupleDstream.reduceByKey(_ + _)

        result.print

        // 开启接收器
        context.start()

        // main的是driver，需要一直启动，等待接收器执行
        context.awaitTermination()
    }
}
```

- 移动文件到hdfs上
  - 注意启动完spark之后在移动文件

```bash
[ttshe@hadoop102 data]$ hadoop fs -put ./a.txt /fileStream
[ttshe@hadoop102 data]$ hadoop fs -put ./b.txt /fileStream
[ttshe@hadoop102 data]$ hadoop fs -put ./c.txt /fileStream
```

- 注意
  - 文件夹所在系统的时间和程序运行的时间要一致
  - 文件夹内的文件新增更新使用流，复制和移动有时无效



## RDD队列（了解）

- 测试过程中，可以通过使用ssc.queueStream(queueOfRDDs)来创建DStream
- 每一个推送到这个队列中的RDD，都会作为一个DStream处理
- 示例

```scala
package com.stt.spark.streaming

import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.dstream.{DStream, InputDStream}

import scala.collection.mutable


object Ch03_QueueSource {
    def main(args: Array[String]): Unit = {

        var conf = new SparkConf().setMaster("local[*]").setAppName("Ch03_QueueSource")
        // 5s读取一次
        val context: StreamingContext = new StreamingContext(conf,Seconds(5))

        // 创建RDD队列
        var queueSource = new mutable.Queue[RDD[Int]]()

        // 第二个参数默认true，表示一次从队列里面取一个
        //    val dstream: InputDStream[Int] = context.queueStream(queueSource)
        // 设置false，则从队列中取出全部
        val dstream: InputDStream[Int] = context.queueStream(queueSource,false)
        // 求和
        dstream.reduce(_ + _).print()
        // 开启接收器
        context.start()

        // 循环创建并向RDD队列中放入RDD
        // 向队列中放入5个RDD[Int]
        for(i<- 1 to 5){
            // val value: RDD[Int] = context.sparkContext.makeRDD(1 to 5)
            // 等价于
            val value: RDD[Int] = context.sparkContext.makeRDD(Array(1,2,3,4,5))
            
            queueSource += value
            //      Thread.sleep(2000)
        }

        // main的是driver，需要一直启动，等待接收器执行
        // block，此处阻塞
        context.awaitTermination()
    	// 在阻塞后面添加代码，在driver结束后才会执行
    }
}
```



## 自定义数据源

- 需要继承Receiver，并实现onStart、onStop方法来自定义数据源采集



示例

- 自定义数据源，实现监控某个端口号，获取该端口号内容

```scala
package com.stt.spark.streaming

import java.io.{BufferedReader, IOException, InputStreamReader}
import java.net.Socket
import java.nio.charset.StandardCharsets

import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.dstream.ReceiverInputDStream
import org.apache.spark.streaming.receiver.Receiver
import org.apache.spark.streaming.{Seconds, StreamingContext}

import scala.util.control.Breaks

object ReceiverSource {
    def main(args: Array[String]): Unit = {
        var conf = new SparkConf().setMaster("local[*]").setAppName("Ch03_QueueSource")
        // 5s读取一次
        val context: StreamingContext = new StreamingContext(conf,Seconds(5))

        val dStream: ReceiverInputDStream[String] = context.receiverStream(new SocketReceiver("hadoop102",9999))

        dStream.flatMap(_.split(" ")).map(w=>(w,1)).reduceByKey(_ + _).print

        // 开启接收器
        context.start()

        // main的是driver，需要一直启动，等待接收器执行
        context.awaitTermination()
    }
}

class SocketReceiver(host:String,port:Int) extends Receiver[String](StorageLevel.MEMORY_ONLY){

    def receive(): Unit = {
        var runFlag = true
        // 如果服务器没有开启，需要等待
        try{
            val socket = new Socket(host,port)
            val reader = new BufferedReader(new InputStreamReader(socket.getInputStream(),StandardCharsets.UTF_8))
            var line = ""
            // 当receiver没有关闭并且输入数据不为空，则循环发送数据给Spark
            // 网络流当中 socket是没有结尾的，readLine是不会获取null的
            // 一般的处理方式为数据的传递和数据的接收要统一规范
            // 类似自定义协议，可以使用--END表示流结束
            Breaks.breakable{
                while((line=reader.readLine())!=null && !isStopped()){
                    if("--END".equals(line)){
                        Breaks.break()
                    }
                    store(line)
                }
            }
            reader.close()
            // 添加该socket.close可能会出错，等同于装饰者模式，socket已经关了
            // socket.close()

            //        restart("restart")
        }catch {
            case e: IOException => {
                runFlag = false
            }
        }

        if(runFlag == false){
            Thread.sleep(1000)
            return
        }
    }

    override def onStart(): Unit = {
        new Thread(){
            override def run(): Unit = {
                while(true){
                    receive()
                }
            }
        }.start()
    }
    override def onStop(): Unit = {
    }
}
```

- 开启netcat

```scala
[ttshe@hadoop102 data]$ nc -lk 9999
hello
you
--END
```



## Kafka数据源（重点）

- 在工程中需要引入 Maven 工件 spark- streaming-kafka_2.10
  - 包内提供的 KafkaUtils 对象可以在 StreamingContext 和 JavaStreamingContext 中以你的 Kafka 消息创建出 DStream
  - 由于 KafkaUtils 可以订阅多个主题，因此它创建出的 DStream 由成对的主题和消息组成
  - 要创建出一个流数据，需要使用 StreamingContext 实例、一个由逗号隔开的 ZooKeeper 主机列表字符串、消费者组的名字(唯一名字)，以及一个从主题到针对这个主题的接收器线程数的映射表来调用 createStream() 方法

### pom

```xml
<!--参考官方文档获取适合的jar包版本-->
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka-0-8_2.11</artifactId>
    <version>2.1.1</version>
</dependency>
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>0.11.0.2</version>
</dependency>
```



### 示例

- 通过SparkStreaming从Kafka读取数据，并将读取过来的数据做简单计算(WordCount)，最终打印到控制台
- 写法1，0.10版本

```scala
package com.atguigu

import kafka.serializer.StringDecoder
import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.dstream.ReceiverInputDStream
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}

object KafkaSparkStreaming {

  def main(args: Array[String]): Unit = {

    //1.创建SparkConf并初始化SSC
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("KafkaSparkStreaming")
    val ssc = new StreamingContext(sparkConf, Seconds(5))

    //2.定义kafka参数
    val brokers = "hadoop102:9092,hadoop103:9092,hadoop104:9092"
    val topic = "source"
    val consumerGroup = "spark"

    //3.将kafka参数映射为map
    val kafkaParam: Map[String, String] = Map[String, String](
      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -> "org.apache.kafka.common.serialization.StringDeserializer",
      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -> "org.apache.kafka.common.serialization.StringDeserializer",
      ConsumerConfig.GROUP_ID_CONFIG -> consumerGroup,
      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> brokers
    )

    //4.通过KafkaUtil创建kafkaDSteam
    val kafkaDSteam: ReceiverInputDStream[(String, String)] = KafkaUtils.createStream[String, String, StringDecoder, StringDecoder](
      ssc,
      kafkaParam,
      Set(topic),
      StorageLevel.MEMORY_ONLY
    )

    //5.对kafkaDSteam做计算（WordCount）
    kafkaDSteam.foreachRDD {
      rdd => {
        val word: RDD[String] = rdd.flatMap(_._2.split(" "))
        val wordAndOne: RDD[(String, Int)] = word.map((_, 1))
        val wordAndCount: RDD[(String, Int)] = wordAndOne.reduceByKey(_ + _)
        wordAndCount.collect().foreach(println)
      }
    }

    //6.启动SparkStreaming
    ssc.start()
    ssc.awaitTermination()
  }
}
```

- 写法2，0.8版本

```scala
package com.stt.spark.streaming

import kafka.serializer.StringDecoder
import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.dstream.ReceiverInputDStream
import org.apache.spark.streaming.kafka.KafkaUtils

object Ch05_KafkaSource {

    def main(args: Array[String]): Unit = {

        var conf = new SparkConf().setMaster("local[*]").setAppName("Ch04_ReceiverSource")

        // 5s读取一次
        val context: StreamingContext = new StreamingContext(conf,Seconds(5))

        // kafka数据源
        val dStream: ReceiverInputDStream[(String, String)] = KafkaUtils.createStream[String,String,StringDecoder,StringDecoder](
            context,
            Map(
                //        ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> "hadoop102:9092", // 0.8版本必须连接zookeeper
                "zookeeper.connect" -> "hadoop102:2181",
                ConsumerConfig.GROUP_ID_CONFIG -> "spark",
                ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -> "org.apache.kafka.common.serialization.StringDeserializer",
                ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -> "org.apache.kafka.common.serialization.StringDeserializer"

            ),
            Map(
                "spark-topic" -> 3
            ),
            StorageLevel.MEMORY_ONLY
        )

        dStream.map(t=>(t._2,1)).reduceByKey(_ + _).print()

        // 开启接收器
        context.start()

        // main的是driver，需要一直启动，等待接收器执行
        context.awaitTermination()
    }

}
```

- 创建topic

```bash
[ttshe@hadoop102 bin]$ kf.sh start
[ttshe@hadoop102 bin]$ cd /opt/module/kafka
[ttshe@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:2181 --create --topic spark-topic --partitions 2 --replication-factor 2
Created topic "spark-topic".
```

- 创建生产者

```bash
[ttshe@hadoop102 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic spark-topic
>
```

- 启动消费者