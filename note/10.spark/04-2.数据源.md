# SparkSQL 数据源



## 通用加载/保存方法

- SparkSQL的DataFrame接口支持多种数据源的操作
- 一个DataFrame可以进行RDDs方式的操作，也可被注册为临时表
  - DataFrame注册为临时表之后，可对该DataFrame执行SQL查询
- SparkSQL数据源
  - ==默认Parquet格式==
  - 修改默认格式
    - 配置项spark.sql.sources.default 修改默认数据源格式
  - 手动指定格式
    - 当文件格式不是parquet格式文件时
      - 全名指定
        - 如org.apache.spark.sql.parquet
      - 简称指定
        - 数据源格式为内置格式
        - 如json, parquet, jdbc, orc, libsvm, csv, text
  - 加载数据
    - SparkSessiond.read.load
  - 使用df.write和df.save保存数据



### 加载



#### read.load

- 通过load方式

```scala
// 直接加载json
scala> spark.read.load("file:/opt/software/person.json")
// 报错,json不是一个parquet文件
...
Caused by: java.lang.RuntimeException: file:/opt/software/person.json is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [51, 49, 125, 10]
...
// 加载spark提供的parquet文件
scala> var df = spark.read.load("file:/opt/module/spark/examples/src/main/resources/users.parquet")
df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field]

scala> df.show
19/11/05 00:50:54 WARN ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
+------+--------------+----------------+
|  name|favorite_color|favorite_numbers|
+------+--------------+----------------+
|Alyssa|          null|  [3, 9, 15, 20]|
|   Ben|           red|              []|
+------+--------------+----------------+

// 手动指定json格式读取文件
scala> spark.read.format("json").load("file:/opt/software/person.json")
res0: org.apache.spark.sql.DataFrame = [age: bigint, name: string]
```



#### spark.sql

- 通过在sql上直接加载的方式读取

```scala
// 直接读取建立视图，指定格式和sql，得到结果df
scala> var df = spark.sql("select name from json.`file:/opt/software/person.json`")
19/11/05 01:11:05 WARN ObjectStore: Failed to get database json, returning NoSuchObjectException
df: org.apache.spark.sql.DataFrame = [name: string]

scala> df.show
+--------+
|    name|
+--------+
|     stt|
|zhangsan|
|    lisi|
+--------+

// 从hdfs上读取
scala> val sqlDF = spark.sql("SELECT * FROM parquet.`hdfs:// hadoop102:9000/namesAndAges.parquet`")
```



### 保存



#### write.save

- 默认保存的是snappy压缩的parquet文件

```scala
scala> var df = spark.read.format("json").load("file:/opt/software/person.json")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]

scala> df.write.save("file:/opt/software/data/output")
```

- 保存为其他格式
  - 指定格式

```scala
scala> df.write.format("json").save("file:/opt/software/data/jsonOutput")
```



#### 保存模式 [SaveMode]

- 采用SaveMode执行存储操作，SaveMode定义了对数据的处理模式
  - 注意
    - 保存模式不使用任何锁定，不是原子操作
    - 当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除

```scala
@InterfaceStability.Stable
public enum SaveMode {
    /**
   * Append mode means that when saving a DataFrame to a data source, if data/table already exists,
   * contents of the DataFrame are expected to be appended to existing data.
   *
   * @since 1.3.0
   */
    Append,
    /**
   * Overwrite mode means that when saving a DataFrame to a data source,
   * if data/table already exists, existing data is expected to be overwritten by the contents of
   * the DataFrame.
   *
   * @since 1.3.0
   */
    Overwrite,
    /**
   * ErrorIfExists mode means that when saving a DataFrame to a data source, if data already exists,
   * an exception is expected to be thrown.
   *
   * @since 1.3.0
   */
    ErrorIfExists,
    /**
   * Ignore mode means that when saving a DataFrame to a data source, if data already exists,
   * the save operation is expected to not save the contents of the DataFrame and to not
   * change the existing data.
   *
   * @since 1.3.0
   */
    Ignore
}
```

| Scala/Java                      | Any Language | Meaning                  |
| ------------------------------- | ------------ | ------------------------ |
| SaveMode.ErrorIfExists(default) | "error"      | 若文件存在，则报错，默认 |
| SaveMode.Append                 | "append"     | 追加                     |
| SaveMode.Overwrite              | "overwrite"  | 覆写                     |
| SaveMode.Ignore                 | "ignore"     | 数据存在，则忽略         |

```scala
scala> df.write.format("json").mode("append").save("file:/opt/software/data/jsonOutput")
```



## Json文件

- Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]
- 可通过SparkSession.read.json()去加载一个JSON 文件
- ==注意==：JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串



### read.json

```scala
{"name":"Michael"}
{"name":"Andy", "age":30}
{"name":"Justin", "age":19}

// Primitive types (Int, String, etc) and Product types (case classes) encoders are
// supported by importing this when creating a Dataset.
import spark.implicits._

// A JSON dataset is pointed to by path.
// The path can be either a single text file or a directory storing text files
val path = "examples/src/main/resources/people.json"
val peopleDF = spark.read.json(path)

// The inferred schema can be visualized using the printSchema() method
peopleDF.printSchema()
// root
//  |-- age: long (nullable = true)
//  |-- name: string (nullable = true)

// Creates a temporary view using the DataFrame
peopleDF.createOrReplaceTempView("people")

// SQL statements can be run by using the sql methods provided by spark
val teenagerNamesDF = spark.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19")
teenagerNamesDF.show()
// +------+
// |  name|
// +------+
// |Justin|
// +------+

// Alternatively, a DataFrame can be created for a JSON dataset represented by
// a Dataset[String] storing one JSON object per string
val otherPeopleDataset = spark.createDataset(
    """{"name":"Yin","address":{"city":"Columbus","state":"Ohio"}}""" :: Nil)
val otherPeople = spark.read.json(otherPeopleDataset)
otherPeople.show()
// +---------------+----+
// |        address|name|
// +---------------+----+
// |[Columbus,Ohio]| Yin|
```



## Parquet文件

- Parquet是一种流行的列式存储格式，可以高效地存储具有嵌套字段的记录
- Parquet格式经常在Hadoop生态圈中被使用，它支持Spark SQL的全部数据类型



### read.parquet

```scala
import spark.implicits._

val peopleDF = spark.read.json("examples/src/main/resources/people.json")

peopleDF.write.parquet("hdfs://hadoop102:9000/people.parquet")

val parquetFileDF = spark.read.parquet("hdfs:// hadoop102:9000/people.parquet")

parquetFileDF.createOrReplaceTempView("parquetFile")

val namesDF = spark.sql("SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19")
namesDF.map(attributes => "Name: " + attributes(0)).show()
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+
```



## JDBC

- 通过JDBC从关系型数据库中读取数据的方式创建DataFrame
- 通过对DataFrame一系列的计算后，可将数据再写回关系型数据库中
- 注意:**需要将相关的数据库驱动放到spark的类路径下**

```bash
[ttshe@hadoop102 mysql-connector-java-5.1.27]$ cp mysql-connector-java-5.1.27-bin.jar /opt/module/spark/jars/
[ttshe@hadoop102 mysql-connector-java-5.1.27]$ pwd
/opt/software/mysql-libs/mysql-connector-java-5.1.27
```

- 开启spark-shell

```scala
$ bin/spark-shell
```



### 加载

- 方式1
  - dbtable 访问的表名
  - paste 解决spark-shell中的粘贴换行

```scala
:paste
val jdbcDF = spark.read.format("jdbc")
.option("url", "jdbc:mysql://hadoop102:3306/test")
.option("dbtable", "user")
.option("user", "root")
.option("password", "123456").load()
```

- 方式2

```scala
val prop = new java.util.Properties()
prop.put("user", "root")
prop.put("password", "123456")
val jdbcDF2 = spark.read.jdbc("jdbc:mysql://hadoop102:3306/test", "user", prop)

jdbcDF2: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]

scala> jdbcDF2.show
+---+----+---+
| id|name|age|
+---+----+---+
|  1| stt| 22|
+---+----+---+
```



### 写入

- 方式1

```scala
:paste
jdbcDF.write
.format("jdbc")
.option("url", "jdbc:mysql://hadoop102:3306/test")
.option("dbtable", "user")
.option("user", "root")
.option("password", "123456")
.save()
```

- 方式2

```scala
val prop = new java.util.Properties()
prop.put("user", "root")
prop.put("password", "123456")
jdbcDF2.write.jdbc("jdbc:mysql://hadoop102:3306/test", "user", prop)
// 添加模式
jdbcDF2.write.mode("append").jdbc("jdbc:mysql://hadoop102:3306/test", "user", prop)
```



## Hive

- Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可包含Hive支持
- 包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等
- 需要强调的一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持
- 若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)
- spark中有metastore_db文件夹



### 内嵌Hive应用

- sparkSQL可直接使用内嵌Hive
- 可使用hive的命令

```scala
scala> spark.sql("show tables").show()
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
+--------+---------+-----------+

scala> spark.sql("create table mytest(id int,name string)")
// 导入数据
scala> spark.sql("load data local inpath '/opt/software/2.txt' into table mytest").show
++
||
++
++
```

- 查看本地文件夹，看到数据成功导入到spark-warehouse中

```bash
[ttshe@hadoop102 mytest]$ pwd
/opt/module/spark/spark-warehouse/mytest
[ttshe@hadoop102 mytest]$ ll
总用量 4
-rwxrwxr-x 1 ttshe ttshe 13 11月  5 02:37 2.txt
[ttshe@hadoop102 mytest]$ cat 2.txt 
1 stt
2 lisi
```

- 通过添加参数初次指定数据仓库hdfs地址
  - --conf  spark.sql.warehouse.dir=hdfs://hadoop102/spark-wearhouse
- 注意：如果使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果你需要是用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到Spark conf目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题，这是需要使用HDFS，则需要将metastore删除，重启集群



### 外部Hive应用

- 连接外部已经部署好的Hive，需要通过以下几个步骤
  - 将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下
    - 注意hive-site.xml中配置了tez引擎需要拷贝后注释掉，否则报错
    - 注意带上访问==Hive元数据库==的JDBC客户端
  - 打开spark shell

```bash
[ttshe@hadoop102 spark]$ rm -rf derby.log 
[ttshe@hadoop102 spark]$ rm -rf spark-warehouse/
[ttshe@hadoop102 spark]$ rm -rf metastore_db/
[ttshe@hadoop102 spark]$ cp /opt/module/hive/conf/hive-site.xml /opt/module/spark/conf/
[ttshe@hadoop102 spark]$ bin/spark-shell
```

```scala
scala> spark.sql("show tables").show
+--------+------------------+-----------+
|database|         tableName|isTemporary|
+--------+------------------+-----------+
| default|dwd_base_event_log|      false|
| default|           student|      false|
+--------+------------------+-----------+


scala> spark.sql("select * from student").show
+----+----+
|  id|name|
+----+----+
|1000| stt|
+----+----+
```



### 运行Spark SQL CLI

- Spark SQL CLI可方便的在本地运行Hive元数据服务以及从命令行执行查询任务

```bash
./bin/spark-sql
```



### 代码中使用Hive

- pom

```xml
<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive -->
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-hive_2.11</artifactId>
    <version>2.1.1</version>
</dependency>
<!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec -->
<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-exec</artifactId>
    <version>1.2.1</version>
</dependency>
```

- 创建SparkSession时需要添加hive支持

```scala
// 使用内置Hive需要指定一个Hive仓库地址
// 若使用的是外部Hive，则需要将hive-site.xml添加到ClassPath下
val warehouseLocation: String = new File("spark-warehouse").getAbsolutePath

val spark = SparkSession
.builder()
.appName("Spark Hive Example")
.config("spark.sql.warehouse.dir", warehouseLocation)
.enableHiveSupport() // 开启hive支持
.getOrCreate()
```

