# Action 算子

- 所有的行动算子都执行runJob方法(在DAGScheduler.scala)
- runJob-->submitJob-->new ActiveJob



## reduce

- reduce(func)
- 通过func函数聚集RDD中的所有元素，先聚合分区内数据，再聚合分区间数据
- 需求：创建一个RDD，将所有元素聚合得到结果

```scala
scala> var rdd = sc.makeRDD(1 to 10)

scala> rdd.reduce(_ max _)
res16: Int = 10

scala> val rdd2 = sc.makeRDD(Array(("a",1),("a",3),("c",3),("d",5)))

scala> rdd2.reduce((x,y)=>(x._1+y._1,x._2+y._2))
res17: (String, Int) = (daac,12)
```



## collect

- 在驱动程序中，以数组的形式返回数据集的所有元素
- 需求：创建一个RDD，并将RDD内容收集到Driver端打印

```scala
scala> sc.makeRDD(1 to 10).collect
res18: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
```



## foreach

- foreach(func)
- 在数据集的每一个元素上，运行函数func进行更新

```scala
var rdd = sc.makeRDD(1 to 5)
rdd.foreach(println(_))

// map也可以打印,在一些不需要返回数据，只需要一些操作即可
// 如将数据插入HBase或者MySql时，不需要返回数据，可使用foreach
rdd.map(x=>{println(x);x}).collect
```



## foreachPartition

- 将每个分区循环遍历



## count

- 返回RDD中元素的个数

```scala
scala> sc.makeRDD(1 to 4).count
res19: Long = 4
```



## countByKey

- 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。
- 需求：创建一个PairRDD，统计每种key的个数

```scala
scala> val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)

scala> rdd.countByKey()
res1: scala.collection.Map[Int,Long] = Map(3 -> 2, 1 -> 3, 2 -> 1)
```



## first

- 返回RDD中的第一个元素

```scala
scala> sc.makeRDD(1 to 4).first
res23: Int = 1
```



## take

- take(n)
- 返回一个由RDD的前n个元素组成的数组

```scala
scala> sc.makeRDD(1 to 4).take(2)
res24: Array[Int] = Array(1, 2)
```



## takeOrdered

- takeOrdered(n)
- 返回该RDD排序后的前n个元素组成的数组
  - 先排序再取值

```scala
scala> sc.makeRDD(Array(9,3,2,4,5)).takeOrdered(3)
res25: Array[Int] = Array(2, 3, 4)
```



## aggregate

- 参数：(zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)
- aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。
- 需求：创建一个RDD，将所有元素相加得到结果

```scala
scala> val rdd = sc.makeRDD(Array("aa","bb","aa"))
scala> rdd.map((_,1)).aggregateByKey(0)(_ + _,_ + _).collect
res28: Array[(String, Int)] = Array((aa,2), (bb,1))
----------

scala> val rdd2 = sc.makeRDD(1 to 6,2)

scala> rdd2.glom.collect
res29: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6))

scala> rdd2.aggregate(0)(_+_,_+_)
res30: Int = 21

// 2个分区内初始10计算，2个分区间初始值10计算，合计30+和值
scala> rdd2.aggregate(10)(_+_,_+_)
res31: Int = 51
```

- 注意与aggregateByKey的区别
  - 初始值
    - aggregateByKey在分区内使用
    - ==aggregate在分区内使用，同时在分区间也使用==



## fold

- 折叠操作
- aggregate的简化操作，seqop和combop一样

```scala
var rdd = sc.makeRDD(1 to 6,2)
rdd.fold(10)(_+_)
res0: Int = 51
```



## saveAsTextFile

- saveAsTextFile(path)
- 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本



## saveAsSequenceFile

- saveAsSequenceFile(path)

- 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统

  

## saveAsObjectFile

- saveAsObjectFile(path)
- 用于将RDD中的元素序列化成对象，存储到文件中

