# 中文分词



## 分词

- 将文本依据某些规则转换成一系列单词的过程
- 文本分析
- Analysis
- Elasticsearch自带的分词器

| 分词器（Analyzer）     | 特点                              |
| ---------------------- | --------------------------------- |
| **standard（ES默认）** | **支持多语言，按词切分并**        |
| simple                 | 按照非字母切分                    |
| whitespace             | 按照空格来切分                    |
| stop                   | 去除语气助词，如the、an、的、这等 |
| keyword                | 不分词                            |

- 中文分词

| 分词器 | 介绍                                   | 特点                               | 地址                                                    |
| ------ | -------------------------------------- | ---------------------------------- | ------------------------------------------------------- |
| ==IK== | 实现中英文单词切分                     | 自定义词库                         | https://github.com/medcl/elasticsearch-analysis-ik      |
| Jieba  | python流行分词系统，支持分词和词性标注 | 支持繁体、自定义、并行分词         | http://github.com/sing1ee/elasticsearch-jieba-plugin    |
| Hanlp  | 由一系列模型于算法组成的java工具包     | 普及自然语言处理在生产环境中的应用 | https://github.com/hankcs/HanLP                         |
| THULAC | 清华大学中文词法分析工具包             | 具有中文分词和词性标注功能         | https://github.com/microbun/elasticsearch-thulac-plugin |



## IK分词器

elasticsearch本身自带的中文分词，就是单纯把中文一个字一个字的分开，根本没有词汇的概念。但是实际应用中，用户都是以词汇为条件，进行查询匹配的，如果能够把文章以词汇为单位切分开，那么与用户的查询条件能够更贴切的匹配上，查询速度也更加快速。



### 安装

- 地址https://github.com/medcl/elasticsearch-analysis-ik/releases?after=v6.3.2
- 解压

```bash
[ttshe@hadoop103 ~]$ mkdir -p /opt/module/elasticsearch/plugins/ik
[ttshe@hadoop103 ik]$ unzip /opt/software/elasticsearch-analysis-ik-6.3.1.zip -d /opt/module/elasticsearch/plugins/ik/
```

- 重启ES
- 测试
  - 使用 movie_index 的默认分词器分词

```bash
GET /movie_index/_analyze
{
  "text":"中华人民共和国"
}
```

- 结果

```bash
{
  "tokens": [
    {
      "token": "中",
      "start_offset": 0,
      "end_offset": 1,
      "type": "<IDEOGRAPHIC>",
      "position": 0
    },
    {
      "token": "华",
      "start_offset": 1,
      "end_offset": 2,
      "type": "<IDEOGRAPHIC>",
      "position": 1
    },
    {
      "token": "人",
      "start_offset": 2,
      "end_offset": 3,
      "type": "<IDEOGRAPHIC>",
      "position": 2
    },
    {
      "token": "民",
      "start_offset": 3,
      "end_offset": 4,
      "type": "<IDEOGRAPHIC>",
      "position": 3
    },
    {
      "token": "共",
      "start_offset": 4,
      "end_offset": 5,
      "type": "<IDEOGRAPHIC>",
      "position": 4
    },
    {
      "token": "和",
      "start_offset": 5,
      "end_offset": 6,
      "type": "<IDEOGRAPHIC>",
      "position": 5
    },
    {
      "token": "国",
      "start_offset": 6,
      "end_offset": 7,
      "type": "<IDEOGRAPHIC>",
      "position": 6
    }
  ]
}
```



### ik_smart模式

- 指定分词器 ik_smart

```json
GET /movie_index/_analyze
{
  "analyzer": "ik_smart", 
  "text":"中华人民共和国"
}
```

- 结果

```bash
{
  "tokens": [
    {
      "token": "中华人民共和国",
      "start_offset": 0,
      "end_offset": 7,
      "type": "CN_WORD",
      "position": 0
    }
  ]
}
```



### ik_max_word贪婪模式

```json
GET /movie_index/_analyze
{
  "analyzer": "ik_max_word", 
  "text":"中华人民共和国"
}
```

- 结果

```json
{
  "tokens": [
    {
      "token": "中华人民共和国",
      "start_offset": 0,
      "end_offset": 7,
      "type": "CN_WORD",
      "position": 0
    },
    {
      "token": "中华人民",
      "start_offset": 0,
      "end_offset": 4,
      "type": "CN_WORD",
      "position": 1
    },
    {
      "token": "中华",
      "start_offset": 0,
      "end_offset": 2,
      "type": "CN_WORD",
      "position": 2
    },
    {
      "token": "华人",
      "start_offset": 1,
      "end_offset": 3,
      "type": "CN_WORD",
      "position": 3
    },
    {
      "token": "人民共和国",
      "start_offset": 2,
      "end_offset": 7,
      "type": "CN_WORD",
      "position": 4
    },
    {
      "token": "人民",
      "start_offset": 2,
      "end_offset": 4,
      "type": "CN_WORD",
      "position": 5
    },
    {
      "token": "共和国",
      "start_offset": 4,
      "end_offset": 7,
      "type": "CN_WORD",
      "position": 6
    },
    {
      "token": "共和",
      "start_offset": 4,
      "end_offset": 6,
      "type": "CN_WORD",
      "position": 7
    },
    {
      "token": "国",
      "start_offset": 6,
      "end_offset": 7,
      "type": "CN_CHAR",
      "position": 8
    }
  ]
}
```

- 能够看出不同的分词器，分词有明显的区别，以后定义一个type不能再使用默认的mapping了
  - 要手工建立mapping, 因为要选择分词器



## 自定义词库

- 修改/usr/share/elasticsearch/plugins/ik/config/中的IKAnalyzer.cfg.xml
  - 本地配置
    - 在 ik 插件的 config/custom 目录下创建一个文件 xxx.dic
    - 在文件中添加词语即可， 每一个词语一行
  - 远程配置
    - 按照标红的路径利用nginx发布静态资源
    - 通过nginx配置服务即可
  - 注意词库
    - 如果是在 linux 中直接 vi 生成的， 可直接使用
    - 如果是在 windows中创建的，文件编码必须是 UTF-8 without BOM 格式
      - UTF-8 无BOM 格式

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
    <comment>IK Analyzer 扩展配置</comment>
    <!--用户可以在这里配置自己的扩展字典 -->
    <entry key="ext_dict">custom/yyy.dic;custom/xxx.dic</entry>
    <!--用户可以在这里配置自己的扩展停止词字典-->
    <entry key="ext_stopwords"></entry>
    <!--用户可以在这里配置远程扩展字典 -->
    <entry key="remote_ext_dict">http://hadoop102/fenci/myword.txt</entry>
    <!--用户可以在这里配置远程扩展停止词字典-->
    <!-- <entry key="remote_ext_stopwords">words_location</entry> -->
</properties>
```

- 示例：远程配置
- 在nginx.conf中配置

```json
 server {
  listen  80;
  server_name hadoop102;
  location /fenci/ { // 表示访问的ip/xxx 等于本地路径下 es/xxx
     root es;
  }
}
```

- 在/usr/local/nginx/中创建/es/fenci/myword.txt
- myword.txt中编写关键词，每一行代表一个词

```txt
学习个
学习个技术
```

- 重启es服务器
- 重启nginx
- 在kibana中测试

```bash
GET /movie_index/_analyze
{
  "analyzer": "ik_max_word", 
  "text":"学习个技术"
}
// 结果
{
  "tokens": [
    {
      "token": "学习个技术",
      "start_offset": 0,
      "end_offset": 5,
      "type": "CN_WORD",
      "position": 0
    },
    {
      "token": "学习个",
      "start_offset": 0,
      "end_offset": 3,
      "type": "CN_WORD",
      "position": 1
    },
    {
      "token": "学习",
      "start_offset": 0,
      "end_offset": 2,
      "type": "CN_WORD",
      "position": 2
    },
...
```

- 更新完成后，es只会对新增的数据用新词分词。历史数据是不会重新分词的。如果想要历史数据重新分词。需要执行

```bash
POST movies_index_chn/_update_by_query?conflicts=proceed
```

