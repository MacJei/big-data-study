# 简介

- Elasticsearch官网： https://www.elastic.co/products/elasticsearch



## 全文检索

- 计算机根据用户输入的关键词进行匹配，从已有的数据库中摘录出相关的记录反馈给用户
- 常见的全网搜索引擎，像百度、谷歌，除此以外，搜索技术在垂直领域也有广泛的使用，比如淘宝、京东搜索商品，万芳、知网搜索期刊，都是基于海量数据的搜索



## 如何全文检索



### 传统关系型数据库

- 弊端
  - 对于传统的关系性数据库对于关键词的查询，只能逐字逐行的匹配，性能非常差
  - 匹配方式不合理，比如搜索“小密手机” ，如果用like进行匹配， 根本匹配不到
  - 但是考虑使用者的用户体验的话，除了完全匹配的记录，还应该显示一部分近似匹配的记录，至少应该匹配到“手机”



### 专业全文索引如何处理

- 全文搜索引擎目前主流的索引技术就是==倒排索引==的方式
  - 传统的保存数据的方式
    - 记录→单词
  - 倒排索引的保存数据的方式
    - 单词→记录



## elasticSearch



### 与lucene的关系

- 基于lucene实现处理分词，构建倒排索引等
- lucene是一个提供全文搜索功能类库的核心工具包
- elasticSearch是使用lucene搭建的一个完善的服务框架
  - 类比lucene是类似于jdk，而搜索引擎软件就是tomcat 的

- 目前流行的主流搜索引擎软件
  - elasticsearch
  - solr
  - 都是基于lucene的搭建
  - 可独立部署启动的搜索引擎服务软件
  - 由于内核相同，所以两者除了服务器安装、部署、管理、集群以外，对于数据的操作，修改、添加、保存、查询等等都十分类似
    - 类比于支持sql语言的两种数据库软件
    - 只要学会其中一个另一个很容易上手

- 从实际企业使用情况来看，elasticSearch的市场份额逐步在取代solr
  - 国内百度、京东、新浪都是基于elasticSearch实现的搜索功能
  - 国外像维基百科、GitHub、Stack Overflow等等也都是基于ES的



### 使用场景

- 为用户提供按关键字查询的全文搜索功能

- ELK框架(ElasticSearch,Logstash,Kibana)
  - 实现企业海量日志的处理分析的解决方案
  - 大数据领域的重要一份子



### 基本概念

| 属性     | 解释                                                         |
| -------- | ------------------------------------------------------------ |
| cluster  | 整个elasticsearch 默认就是集群状态，整个集群是一份完整、互备的数据 |
| node     | 集群中的一个节点，一般只一个进程就是一个node                 |
| shard    | 分片，即使是一个节点中的数据也会通过hash算法，分成多个片存放，==默认是5片== |
| index    | 相当于rdbms的database, 对于用户来说是一个逻辑数据库，虽然物理上会被分多个shard存放，也可能存放在多个node中。 |
| type     | 类似于rdbms的table，但是与其说像table，其实更像面向对象中的class , 同一Json的格式的数据集合。 |
| document | 类似于rdbms的 row、面向对象里的object                        |
| field    | 相当于字段、属性                                             |



### 结构

- 结构如下（ES6.0之前）
  - ==注意：ES6.0之后，一个索引中只有一个type==
    - index代替了type的功能，type默认推荐使用固定值_doc代替
    - 发展趋势，去除type

![img](img/ELK/3.jpg)



### 与数据库类比

| 关系型数据库（比如Mysql） | 非关系型数据库（Elasticsearch） |
| ------------------------- | ------------------------------- |
| 数据库Database            | 索引Index                       |
| 表Table                   | 类型Type                        |
| 数据行Row                 | 文档Document                    |
| 数据列Column              | 字段Field                       |
| 约束 Schema               | 映射Mapping                     |

- 索引Index
  - 由具有相同字段的文档列表组成，用于定义字段名和字段值，一个集群或elasticsearch	由多个索引组成，例如可以按照日期生成多个索引，方便数据搜索
- 类型Type
  - 具有相同特征文档的集合（ES6之后一个索引中只能定义一个type）
- 文档document
  - 用户存储在ES中的数据文档
    - 元数据
      - _index
        - 文档所在索引名称
      - _type
        - 文档所在类型名称
      - _id
        - 文档唯一id
      -  _uid
        - 组合id，由 _type 和 _id组成
        - 6.x后，_type不再起作用，与 _id
      - _source
        - 文档的原始Json数据，包括每个字段的内容
      - _all
        - 将所有字段内容整合起来
        - 默认禁用
        - 用于对所有字段内容检索

- 字段Field
  - 具有相同特性数据名称

- 与mysql的对比
  - mysql在数据表建立后，手动建立索引
  - es在创建表时，所有字段默认建立索引，数据表存储量可能会扩大到原先的8倍



# 安装&配置

- 6.3.1版本
- 安装在hadoop103上
- 注意内存至少2G以上分配
  - 由于ES是注重大数据量搜索，需要使用大量内存



## 准备

- 一台服务器 centOS6.8

- 网络环境配置

- Java8环境安装

  - 删除Centos6.8自带的openJDK

  ```bash
  #查询是否有openJDK
  rpm -qa | grep java
  
  #删除自带openJDK
  rpm -qa | grep java | xargs rpm -e --nodeps
  
  #重新加载配置/etc/profile文件
  source /etc/profile
  ```

  - 安装JDK1.8，配置环境变量

- 创建非root用户
  - ==ES服务不能使用root用户启动，原因是ES有执行脚本能力，因安全问题ES禁止用root用户启动==
  - 此处使用ttshe

```bash
useradd es
passwd es
```

- 演示root启动错误

![img](img/ELK/5.jpg)



## 修改系统配置

- 为什么要修改linux配置？
  - 默认elasticsearch是单机访问模式，只能自己访问自己
  - 需要设置成允许应用服务器通过网络方式访问
    - elasticsearch会因为单机版的低端默认配置而报错，甚至无法启动
    - 需要把服务器的一些限制打开，能支持更多并发

- ==注意如下修改完后重启==



### limits.conf

- 解决问题：max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536] elasticsearch
- 原因：系统允许 Elasticsearch 打开的最大文件数需要修改成65536
- 注意：“*” 不要省略掉

```bash
[ttshe@hadoop103 root]$ sudo vi /etc/security/limits.conf
# 添加如下配置
* soft nofile 65536
* hard nofile 131072
* soft nproc 2048
* hard nproc 4096
# 与bootstrap.memory_lock: false 相对应
* hard memlock unlimited
* soft memlock unlimited
```



### 90-nproc.conf

- 解决问题：max number of threads [1024] for user [judy2] likely too low, increase to at least [2048]  （CentOS7.x  不用改）
- 原因：允许最大进程数修该成4096

```bash
[ttshe@hadoop103 elasticsearch]$sudo vi /etc/security/limits.d/90-nproc.conf 
# 修改软连接线程数 4096
*          soft    nproc     4096
root       soft    nproc     unlimited
```



### sysctl.conf

- 解决问题：max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] （CentOS7.x  不用改）
- 原因：一个进程可以拥有的虚拟内存区域的数量
- 解决：临时提高vm.max_map_count的大小
- 命令：sysctl -w vm.max_map_count=262144
- 上述方法修改之后，如果重启虚拟机将失效，修改sysctl.conf可永久修改

```bash
[ttshe@hadoop103 root]$ sudo vi /etc/sysctl.conf
# 添加如下配置
vm.max_map_count=655360
fs.file-max=655360
```

- 以上参数改完需要重启



## 单机安装

- 解压安装包

```bash
[ttshe@hadoop103 software]$ tar -zvxf elasticsearch-6.3.1.tar.gz -C /opt/module/
[ttshe@hadoop103 module]$ mv elasticsearch-6.3.1/ elasticsearch/
```



### 配置

#### elasticsearch.yml

- ES5.2.0默认bootstrap.system_call_filter为true进行检测SecComp
  - Centos6不支持SecComp
  - 导致检测失败，失败后直接导致ES不能启动
- 在elasticsearch.yml中配置bootstrap.system_call_filter为false，注意要在Memory下面
  - bootstrap.memory_lock: false
    - 默认配置true
      - 会导致硬盘频繁读，IOPS变高
      - 锁定物理内存地址，防止es内存被交换出去
      - 避免es使用swap交换分区，频繁的交换，会导致IOPS变高
  - bootstrap.system_call_filter: false

```bash
[ttshe@hadoop103 config]$ vim elasticsearch.yml
#添加修改设置
bootstrap.memory_lock: false
bootstrap.system_call_filter: false
# 修改为外部访问模式 0.0.0.0 或 hostname
network.host: hadoop102
```



#### jvm.options

- 默认是2G 
- 由于教学实验环境集群启动1G的话个人电脑配置压力较大，所以调整为256m

```bash
[ttshe@hadoop102 config]$ vim jvm.options
# 修改如下配置
- Xms256m
- Xms256m
```



### 启动

- 后台启动需要添加参数 -d 

```bash
[ttshe@hadoop102 elasticsearch]$ bin/elasticsearch
```



### 验证

```bash
[ttshe@hadoop102 ~]$ curl localhost:9200
```



## 安装Kibana

- 参考kibana安装笔记



## 集群安装



### 配置



#### elasticserach.yml

```bash
# 集群名称（不能重复）
cluster.name: my-es#必须相同 
# 节点名称，仅仅是描述名称，用于在日志中区分（自定义）,每个节点必须不同
node.name: es1
#指定了该节点可能成为 master 节点，还可以是数据节点
node.master: true
node.data: true
# 当前节点的IP地址 
network.host: 0.0.0.0
# 关闭bootstrap 自启程序
bootstrap.memory_lock: false
bootstrap.system_call_filter: false
# 数据的默认存放路径（自定义）
path.data: ../data/esData
# 日志的默认存放路径 
path.logs: ../data/esLogs
# 对外提供服务的端口
http.port: 9200 
# 9300为集群服务的端口
transport.tcp.port: 9300
# 集群各个节点IP地址，可使用域名，需要各节点能够解析 
discovery.zen.ping.unicast.hosts: ["hadoop102","hadoop103","hadoop104"] 
# 为了避免脑裂，集群节点数最少为 半数+1
discovery.zen.minimum_master_nodes: 2  
```

- 注意1：编写要有空格，关闭防火墙

- 注意2：discovery.zen.ping.unicast.hosts，

- 注意3：启动前，清空esData和esLogs数据

  

#### jvm.options

- 默认是2G 
- 由于教学实验环境集群启动1G的话个人电脑配置压力较大，所以调整为256m

```bash
[ttshe@hadoop102 config]$ vim jvm.options
# 修改如下配置
- Xms256m
- Xms256m
```



### 启动脚本

```bash
[ttshe@hadoop102 bin]$ vim es.sh
[ttshe@hadoop102 bin]$ chmod 777 es.sh 
```

- 脚本如下

```bash
#!/bin/bash 

es_home=/opt/module/elasticsearch
kibana_home=/opt/module/kibana

case $1 in
 "start") {
  for i in hadoop102 hadoop103 hadoop104
  do
    ssh $i "${es_home}/bin/elasticsearch >/dev/null 2>&1 &"
  done
  nohup ${kibana_home}/bin/kibana >kibana.log 2>&1 &
};;
"stop") {
  ps -ef|grep ${kibana_home} |grep -v grep|awk '{print $2}'|xargs kill
  for i in hadoop102 hadoop103 hadoop104
  do
    ssh $i "ps -ef|grep $es_home |grep -v grep|awk '{print \$2}'|xargs kill" >/dev/null 2>&1
  done
};;
esac
```

- 启动验证

```bash
[ttshe@hadoop102 bin]$ es.sh start
[ttshe@hadoop102 bin]$ curl -XGET 'http://hadoop102:9200/_cat/nodes?pretty'
192.168.1.103 43 54 4 0.36 0.16 0.10 mdi - es2
192.168.1.104 44 52 4 0.52 0.22 0.12 mdi - es3
192.168.1.102 49 26 4 0.66 0.35 0.17 mdi * es1
```



# 基本命令



## 启动

```bash
[ttshe@hadoop103 elasticsearch]$ bin/elasticsearch
# 后台启动
[ttshe@hadoop103 elasticsearch]$ bin/elasticsearch -d
```

- 测试

```bash
[ttshe@hadoop103 ~]$ curl localhost:9200
{
  "name" : "9oEk0A8",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "i73W9jl3Q1qticF3u0CXJQ",
  "version" : {
    "number" : "6.3.1",
    "build_flavor" : "default",
    "build_type" : "tar",
    "build_hash" : "eb782d0",
    "build_date" : "2018-06-29T21:59:26.107521Z",
    "build_snapshot" : false,
    "lucene_version" : "7.3.1",
    "minimum_wire_compatibility_version" : "5.6.0",
    "minimum_index_compatibility_version" : "5.0.0"
  },
  "tagline" : "You Know, for Search"
}
```



## 关闭

```bash
ps -ef|grep elastic
jps -l
kill xxxx
```



## 运行模式

### Development

- 默认模式
- 开发模式
- 仅内部访问



### Production

- 生产模式
- 可外部访问



### 区别

- Development模式下启动时，配置检查异常， 会提示**警告**

- Production模式下在启动时，配置检查异常，会提示**错误并抛出**



### 修改

- Development模式：network.host=localhost(默认)
- Production 模式：network.host=真实IP

```bash
[ttshe@hadoop103 elasticsearch]$ vim config/elasticsearch.yml
# 添加如下配置0.0.0.0或hostname
network.host: hadoop103
```



## 状态查看 _cat

- 网络接口查询

### 语法

```bash
ip:port/_cat/[args](?v|?format=json&pretty)
（?v表示显示字段说明,?format=json&pretty表示显示成json格式）
```



### 版本信息

```bash
curl hadoop103:9200

{
  "name" : "9oEk0A8",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "i73W9jl3Q1qticF3u0CXJQ",
  "version" : {
    "number" : "6.3.1",
    "build_flavor" : "default",
    "build_type" : "tar",
    "build_hash" : "eb782d0",
    "build_date" : "2018-06-29T21:59:26.107521Z",
    "build_snapshot" : false,
    "lucene_version" : "7.3.1",
    "minimum_wire_compatibility_version" : "5.6.0",
    "minimum_index_compatibility_version" : "5.0.0"
  },
  "tagline" : "You Know, for Search"
}
```



### 健康状态 [_cat/health?v]

- 

```http
http://hadoop103:9200/_cat/health?v

epoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1569812207 10:56:47  elasticsearch green           1         1      0   0    0    0        0             0                  -                100.0%
```

- 集群情况

```bash
GET _cat/health?v

epoch      timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1574566462 11:34:22  my-es   green           3         3     10   5    0    0        0             0                  -                100.0%
```



### 节点列表 [_cat/nodes?v]

- 查看单机

```http
http://hadoop103:9200/_cat/nodes?v

ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
192.168.1.103           23          46   0    0.00    0.00     0.00 mdi       *      9oEk0A8
```

- 查看集群

```bash
GET _cat/nodes?v

ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
192.168.1.103           44          54   0    0.12    0.15     0.10 mdi       -      es2
192.168.1.104           44          52   0    0.26    0.20     0.13 mdi       -      es3
192.168.1.102           31          26   0    0.22    0.29     0.16 mdi       *      es1
```



# 数据操作 DSL

- elasticsearch restful api

- 需要先安装kibana，在kibana中操作

  

## 数据结构

- java中的数据结构

```java
public class  Movie {
    String id;
    String name;
    Double doubanScore;
    List<Actor> actorList;
}

public class Actor{
    String id;
    String name;
}
```

- 关系型数据库：该结构会被拆成2张表保存
- elasticsearch：用一个json来表示一个document

```json
{
    “id”:”1”,
    “name”:”operation red sea”,
    “doubanScore”:”8.5”,
    “actorList”:[  // 直接存储json对象，表示actor，如果是mysql则存储主键
        {“id”:”1”,”name”:”zhangyi”},
        {“id”:”2”,”name”:”haiqing”},
        {“id”:”3”,”name”:”zhanghanyu”}
	]
}
```

- elasticSearch比Mysql快的原因
  - 空间换时间
  - 存储冗余信息
    - 倒排索引
  - 平均是1:8的存储容量比例



## 索引操作

- 索引相对于数据库中的database
- 特点，只增不改



### rest api格式

- elasticsearch REST api遵循的格式为
- -X 表示使用的协议类型

```bash
curl -X<Verb> <Node>:<Port>/<Index>/<Type>/<ID>
```



### 索引存储大小信息 [GET _cat/indices?v]

```kibana
get _cat/indices?v
```
- 或
```http
http://hadoop103:9200/_cat/indices?v

health status index   uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   .kibana CzU0nsqmReucEfP6LZIhqg   1   0          0            0       230b           230b
```

- es 中会默认存在一个名为.kibana的索引

- 表头的含义

| 字段           | 含义                                                         |
| -------------- | ------------------------------------------------------------ |
| health         | green(集群完整) yellow(==单点正常、集群不完整，在单机模式下必现==) red(单点不正常) |
| status         | 是否能使用                                                   |
| index          | 索引名                                                       |
| uuid           | 索引统一编号                                                 |
| pri            | 主节点几个，数据的分片的个数                                 |
| rep            | 从节点几个，数据的备份个数                                   |
| docs.count     | 文档个数                                                     |
| docs.deleted   | 文档被删了多少                                               |
| store.size     | 整体占空间大小，分片与副本攻占用大小                         |
| pri.store.size | 主节点占空间大小，分片总和占用大小                           |



### 增加索引 [PUT /xxx_index]

- kibana中操作

```kibana
PUT /movie_index
```

- 索引名称必须全部消息，不能以_开头

```http
curl -XPUT 'hadoop103:9200/mytest_index'

{"acknowledged":true,"shards_acknowledged":true,"index":"mytest_index"}
```



### 删除索引 [DELETE /xxx_index]

- ES 是不删除也不修改任何数据的，而是增加版本号

```kibana
DELETE /movie_index
```

- 或

```http
curl -XDELETE 'hadoop103:9200/mytest_index'

{"acknowledged":true}
```



### 查询索引 [GET /xxx_index]

```kibana
GET /movie_index
```

- 或

```http
curl -XGET 'hadoop103:9200/mytest_index'

{"mytest_index":{"aliases":{},"mappings":{},"settings":{"index":{"creation_date":"1569814068999","number_of_shards":"5","number_of_replicas":"1","uuid":"It-zQdGaQ36KZwCs0APq-g","version":{"created":"6030199"},"provided_name":"mytest_index"}}}}
```



## 文档操作



### 新增文档 [PUT /index/type/id]

- id为1，而内容中的id是属性名称
- 示例的type值是movie
- 如果之前没建过index或者type，es 会自动创建

```json
PUT /movie_index/movie/1
{ "id":1,
  "name":"operation red sea",
  "doubanScore":8.5,
  "actorList":[  
        {"id":1,"name":"zhang yi"},
        {"id":2,"name":"hai qing"},
        {"id":3,"name":"zhang han yu"}
	]
}
PUT /movie_index/movie/2
{
  "id":2,
  "name":"operation meigong river",
  "doubanScore":8.0,
  "actorList":[  
		{"id":3,"name":"zhang han yu"}
	]
}

PUT /movie_index/movie/3
{
  "id":3,
  "name":"incident red sea",
  "doubanScore":5.0,
  "actorList":[  
		{"id":4,"name":"zhang chen"}
	]
}
```

- 示例

```json
POST /ttshe_index/student/1
{
  "name":"stt",
  "age":11,
  "address":"anhui"
}
```

- 返回

```json
{
  "_index": "ttshe_index",
  "_type": "student",
  "_id": "1",
  "_version": 1,
  "result": "created",
  "_shards": {
    "total": 2,
    "successful": 1,
    "failed": 0
  },
  "_seq_no": 0,
  "_primary_term": 1
}
```



### 使用id查找 [GET /index/type/id]

```kibana
GET movie_index/movie/1
```



### 整体替换 [PUT /index/type/id]

- 和新增没有区别

```bash
PUT /movie_index/movie/3
{
  "id":"3",
  "name":"incident red sea",
  "doubanScore":"5.0",
  "actorList":[  
		{"id":"1","name":"zhang chen"}
	]
}
```



### 修改某个字段 [POST /index/type/id/_update]

- 使用POST与doc固定写法，添加_update表示修改
  - 相同的键进行替换
  - 没有的键进行增加
- ==注意POST关键字的使用==
- ==注意doc的固定写法==
- 示例1

```json
POST movie_index/movie/3/_update
{ 
  "doc": {
    "doubanScore":"7.0"
  } 
}
```

- 返回数据
  - 如果经常修改，ES会存储很多冗余数据

```json
{
  "_index": "movie_index",
  "_type": "movie",
  "_id": "3",
  "_version": 9, // 修改成功，表示当前版本
  "result": "updated",
  "_shards": {
    "total": 2,
    "successful": 1,
    "failed": 0
  },
  "_seq_no": 8,
  "_primary_term": 1
}
```



- 示例2

```json
POST ttshe_index/student/1/_update 
{
  "doc": {
    "age":31,
    "desc":"update age desc"
  }
}
```



### 删除文档 [DELETE /index/type/id]

```kibana
DELETE /ttshe_index/student/2
```



## 文档查询 [_search]



### 语法

- _search 表示查询操作

```bash
GET /_search							#查询所有索引文档
GET /my_index/_search					#查询指定索引文档
GET /my_index1,my_index2/_search		#多索引查询
GET /my_*/_search						#匹配索引查询
```



数据准备

- 可以直接创建索引和type，以及文档

```bash
POST /my_test/student/1
{
"username":"tom jack",
"job":"javaengineer",
"age":18,
"birth":"1991-12-15",
"isMarried":false
}

POST /my_test/student/2
{
"username":"jack tom",
"job":"java engineer",
"age":16,
"birth":"1991-12-15",
"isMarried":false
}

POST /my_test/student/3
{
"username":"tom",
"job":"seniorjava ist",
"age":28,
"birth":"1980-05-07",
"isMarried":true
}

POST /my_test/student/4
{
"username":"lee",
"job":"java and ruby engineer",
"age":22,
"birth":"1985-08-07",
"isMarried":false
}

POST /my_test/student/5
{
"username":"lee and tom",
"job":"ruby engineer",
"age":23,
"birth":"1986-08-07",
"isMarried":false
}
```



### 查询type的全部数据 [GET /xxx_index/xxx_type/_search]

```kibana
GET movie_index/movie/_search
```

- 结果

```json
{
  "took": 2,    //耗费时间 毫秒
  "timed_out": false, //是否超时
  "_shards": {
    "total": 5,   //发送给全部5个分片
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 3,  //命中3条数据
    "max_score": 1,   //最大评分
    "hits": [  // 结果
      {
        "_index": "movie_index",
        "_type": "movie",
        "_id": 2,
        "_score": 1, // 表示匹配度100%
        "_source": {
          "id": "2",
          "name": "operation meigong river",
          "doubanScore": 8.0,
          "actorList": [
            {
              "id": "1",
              "name": "zhang han yu"
            }
          ]
        }
...
      }
```

- 示例2

- 多个索引查询

```bash
GET /my_test,my_test2/_search
```

- 通配符查询

```bash
GET /my_*/_search
```



### 按条件查询全部 [query match_all]

```bash
GET movie_index/movie/_search
{
  "query":{
    "match_all": {}
  }
}
```



### 按分词查询 [query match]

- query中使用空格区分匹配关键字

```bash
GET movie_index/movie/_search
{
  "query":{
    "match": {"name":"red yellow"}
  }
}
```

- 将name属性中的值通过空格分隔得到的分词组进行查询
- 返回结果按照_score进行降序排列
- 示例2
  - 对字段作全文检索，最基本和常用的查询类型

```json
GET /my_test/_search
{
  "query":{
    "match": {
      "name": {
        "query":"tom jack"
       }
    }
  }
}
```

- 示例3
  - 通过operator参数可以控制单词间的匹配关系，可选项为or和and

```json
GET /my_test/_search
{
  "query":{
    "match": {
      "name": {
        "query":"tom jack",
        "operator":"and"
      }
    }
  }
}
```

- 示例4
  - 通过minimum_should_match参数可以控制需要匹配的单词数
    - minimum_should_match 最少匹配几个词

```json
GET /my_test/_search
{
  "query": {
    "match": {
      "username": {
         "query": "tom jack lee",
         "minimum_should_match":3
      }
    }
  }
}
```



### 按分词子属性查询 [query match .]

```bash
GET movie_index/movie/_search
{
  "query":{
    "match": {"actorList.name":"zhang"}
  }
}
```



### 按短语查询[query match_phrase]

- 按短语查询，不再利用分词技术，直接用短语在原始数据中匹配

```bash
GET movie_index/movie/_search
{
    "query":{
      "match_phrase": {"name":"operation red"}
    }
}

GET movie_index/movie/_search
{
    "query":{
      "match_phrase": {"actorList.name":"zhang han yu"}
    }
}
```

- name的值作为一个整体进行查询



### 校正分词查询 [_search query fuzzy]

- fuzzy 模糊
- 针对英文，当英文单词错误，可近似匹配
- 校正匹配分词，当一个单词都无法准确匹配，es通过一种算法对非常接近的单词也给与一定的评分，能够查询出来，但是消耗更多的性能
- 使用的少

```bash
GET movie_index/movie/_search
{
    "query":{
      "fuzzy": {"name":"rad"}
    }
}
```



### 分页查询 [_search from size]

- from 下标从0开始

```bash
GET movie_index/movie/_search
{
  "query": { "match_all": {} },
  "from": 1,
  "size": 1
}
```



### 指定显示的字段 [_search _source]

- 控制显示的结果信息
- 示例：只显示name和doubanScore信息

```bash
GET movie_index/movie/_search
{
  "query": { "match_all": {} },
  "_source": ["name", "doubanScore"]
}
```



### 词项查询 [_search query term]

- 词项搜索时对倒排索引中存储的词项进行==精确匹配==
- 词项级别的查询通过用于结构化数据，如数字、日期和枚举类型

```bash
GET movie_index/movie/_search
{
  "query": {
  	"term": {
  		"name":"operation red sea"
  	}
  }
}
```



### 高亮 [_search highlight]

- 使用highlight 进行高亮操作
  - fields表示需要高亮的字段
  - pre_tags 表示关键字前面的标签
  - post_tags 表示关键字后面的标签

```bash
GET movie_index/movie/_search
{
    "query":{
      "match": {"name":"red sea"}
    },
    "highlight": {
      "fields": {"name":{} },
      "pre_tags":"<span color='red'>",
      "post_tags":"</span>"
    }
}
```

- 返回的结果

```json
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 2,
    "max_score": 0.5753642,
    "hits": [
      {
        "_index": "movie_index",
        "_type": "movie",
        "_id": "1",
        "_score": 0.5753642,
        "_source": {
          "id": 1,
          "name": "operation red sea",
          "doubanScore": 8.5,
          "actorList": [
            {
              "id": 1,
              "name": "zhang yi"
            },
            {
              "id": 2,
              "name": "hai qing"
            },
            {
              "id": 3,
              "name": "zhang han yu"
            }
          ]
        },
        "highlight": {
          "name": [
            "operation <span color='red'>red</span> <span color='red'>sea</span>"
          ]
        }
      },
      {
        "_index": "movie_index",
        "_type": "movie",
        "_id": "3",
        "_score": 0.5753642,
        "_source": {
          "id": 3,
          "name": "incident red sea",
          "doubanScore": "7.0",
          "actorList": [
            {
              "id": 4,
              "name": "zhang chen"
            }
          ]
        },
        "highlight": {
          "name": [
            "incident <span color='red'>red</span> <span color='red'>sea</span>"
          ]
        }
      }
    ]
  }
}
```



### 聚合 [_search aggs]

- size 为 0 表示只显示聚合后的结果
- 示例
  - 取出每个演员共参演了多少部电影

```bash
GET movie_index/movie/_search
{ 
  "size":0,
  "aggs": {
    "count_by_actor": { // 自定义操作名称
      "terms": { // 使用terms 表示查询该field的聚合
        "field": "actorList.name.keyword"  
      }
    }
  }
}
```

- 返回结果

```json
{
    "took": 0,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": 3,
        "max_score": 0,
        "hits": []
    },
    "aggregations": {
        "count_by_actor": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
                {
                    "key": "zhang han yu",
                    "doc_count": 2
                },
                {
                    "key": "hai qing",
                    "doc_count": 1
                },
                {
                    "key": "zhang chen",
                    "doc_count": 1
                },
                {
                    "key": "zhang yi",
                    "doc_count": 1
                }
            ]
        }
    }
}
```

- 分组后求平均
  - 每个演员参演电影的平均分是多少，并按评分排序

```json
GET movie_index/movie/_search
{ 
    "size":0,
    "aggs": {
        "groupby_actor_id": { 
            "terms": { 
                "field": "actorList.name.keyword",
                "order": {
                    "avg_score": "asc"
                }
            },
            "aggs":{
                "avg_score":{
                    "avg":{
                        "field":"doubanScore"
                    }
                }
            }
        }
    }
}
```





## 文档过滤[_search filter]



### 查询后过滤 [post_filter]

- term 表示词条
- post_filter 后置过滤

```bash
GET movie_index/movie/_search
{
    "query":{
      "match": {"name":"red"}
    },
    "post_filter":{
      "term": {
        "actorList.id": 3
      }
    }
}
```

- 示例2
  - keyword关键字
    - 添加，表示`actorList.name` 这个值的==短语==和 `zhang chen` 进行匹配
    - 不添加，表示表示`actorList.name` 这个值的==分词==和 `zhang chen` 进行匹配

```bash
GET movie_index/movie/_search
{
    "query":{
      "match": {"name":"red"}
    },
    "post_filter":{
      "term": {
        "actorList.name.keyword": "zhang chen"
      }
    }
}
```



### 查询前过滤 [bool filter term]（推荐）

- must 必须
- should 可有可无
- must_not 必须不匹配
- 上述主要用于关注评分_score

```bash
GET movie_index/movie/_search
{ 
    "query":{
        "bool":{
          "filter":[ {"term": {  "actorList.id": "1"  }},
                     {"term": {  "actorList.id": "3"  }}
           ],
           "must":[{"match":{"name":"red"}}] #匹配模式must
         }
    }
}
```

- 判断是否存在

```bash
GET recommender/_search
{
  "query":{
    "bool":{
      "must":[{"exists":{"field":"tags"}}]
    }
  }
}
```



### 按范围过滤 [bool filter range]

```bash
GET movie_index/movie/_search
{
   "query": {
     "bool": {
       "filter": {
         "range": {
            "doubanScore": {"gte": 8,"lte":20}
         }
       },
       "must":[{"match":{"name":"red"}}] 
     }
   }
}
```

| 操作 | 含义     |
| ---- | -------- |
| gt   | 大于     |
| lt   | 小于     |
| gte  | 大于等于 |
| lte  | 小于等于 |



## 文档排序 [_search sort]

- 使用sort关键字
  - 属性是要排序的字段
  - order 表示要排序的规则

```bash
GET movie_index/movie/_search
{
  "query":{
    "match": {"name":"red sea"}
  }, 
  "sort": [
    {
      "doubanScore": {
        "order": "desc"
      }
    }
  ]
}
```



## 执行计划[_search profile]

```bash
GET /my_test/_search?q=tom
{
  "profile":true
}
```



## URI 风格操作



#### 泛查询

- q=tom 不指定字段查询，相当于单词查询
  - q，指定查询语句，例如q=aa或q=user:aa
  - df，q中不指定字段默认查询的字段，如果不指定，ES会查询所有字段

```bash
GET my_test/_search?q=tom
```

- 结果
  - 将所有含有tom匹配上的字段的文档记录查询出来

```bash
{
  "took": 25,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 4,
    "max_score": 0.6099695,
    "hits": [
      {
        "_index": "my_test",
        "_type": "student",
        "_id": "2",
        "_score": 0.6099695,
        "_source": {
          "username": "jack tom",
          "job": "java engineer",
          "age": 16,
          "birth": "1991-12-15",
          "isMarried": false
        }
      },
      {
        "_index": "my_test",
        "_type": "student",
        "_id": "5",
        "_score": 0.2876821,
        "_source": {
          "username": "lee and tom",
          "job": "ruby engineer",
          "age": 23,
          "birth": "1986-08-07",
          "isMarried": false
        }
      },
      {
        "_index": "my_test",
        "_type": "student",
        "_id": "1",
        "_score": 0.2876821,
        "_source": {
          "username": "tom jack",
          "job": "javaengineer",
          "age": 18,
          "birth": "1991-12-15",
          "isMarried": false
        }
      },
      {
        "_index": "my_test",
        "_type": "student",
        "_id": "3",
        "_score": 0.2876821,
        "_source": {
          "username": "tom",
          "job": "seniorjava ist",
          "age": 28,
          "birth": "1980-05-07",
          "isMarried": true
        }
      }
    ]
  }
}
```

- 指定匹配的字段
  - 指定job中匹配上tom的文档记录

```bash
GET /my_test/_search?q=tom&df=job

{
  "took": 0,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 0,
    "max_score": null,
    "hits": []
  }
}
```



#### 指定字段

```bash
GET /my_test/_search?q=username:tom
```



#### term查询

- 使用空格，表示or的关系
- 只要包含任意一个词就匹配

```bash
GET /my_test/_search?q=tom jack
```



#### phrase查询

- 使用“”表示and的关系

```bash
GET /my_test/_search?q=username:"tom jack"
```



#### 排序

- 使用sort字段
  - asc 升序
  - desc 降序
- 注意sort必须指定字段，按照指定字段排序

```bash
GET /my_test/_search?q=username:tom&sort=age:asc
```



#### 超时时间设置

- 查询时指定超时时间
  - 默认不超时

```bash
GET /my_test/_search?q=tom&df=username&timeout=1s
```



#### 操作符



##### 逻辑操作符

- AND(&&),OR(||),NOT(!)

```bash
( NOT lee)	
(way AND lee)
(way OR lee)
```

```bash
GET /my_test/_search?q=username:(NOT tom)
```

- +、-分别对应must和must_not

```bash
name:(+lee -alfred)
# 表示name字段中，一定包含lee，一定不包含alfred，可以包含tom
```

```bash
GET /my_test/_search?q=username:(+lee -alfred)
```



##### 范围

- 闭区间
  - []

```bash
age:[1 TO 10]
```

```bash
GET /my_test/_search?q=age:[1 TO 20]
```



##### 通配符

```bash
?:1个字符  le? lee
*:0或多个字符 le* leeeeee

例如：name:t?m   tam tom tim
  name:tom*   tomcat
  name:t*m    tm
  
注意：通配符匹配执行效率低，且占用较多内存，不建议使用，如无特殊要求，不要将?或*放在最前面
```

```bash
GET /my_test/_search?q=username:le?
```



##### 正则

```bash
aee  /.ee/
name:/.ee/
# 表示 ee 前可以使任意的字符
```

```bash
GET /my_test/_search?q=username:/.ee/
```



#### 括号调整优先级

- 将查询条件分组，举例（tom OR jack）AND  lee
- ==多个条件可以使用括号进行分组==

```bash
GET /my_test/_search?q=username:tom OR jack AND lee
# 使用 () 对条件进行分组，逻辑上的优先级
GET /my_test/_search?q=username:(tom OR jack) AND lee
```





# 中文分词



## 分词

- 将文本依据某些规则转换成一系列单词的过程
- 文本分析
- Analysis
- Elasticsearch自带的分词器

| 分词器（Analyzer）     | 特点                              |
| ---------------------- | --------------------------------- |
| **standard（ES默认）** | **支持多语言，按词切分并**        |
| simple                 | 按照非字母切分                    |
| whitespace             | 按照空格来切分                    |
| stop                   | 去除语气助词，如the、an、的、这等 |
| keyword                | 不分词                            |

- 中文分词

| 分词器 | 介绍                                   | 特点                               | 地址                                                    |
| ------ | -------------------------------------- | ---------------------------------- | ------------------------------------------------------- |
| ==IK== | 实现中英文单词切分                     | 自定义词库                         | https://github.com/medcl/elasticsearch-analysis-ik      |
| Jieba  | python流行分词系统，支持分词和词性标注 | 支持繁体、自定义、并行分词         | http://github.com/sing1ee/elasticsearch-jieba-plugin    |
| Hanlp  | 由一系列模型于算法组成的java工具包     | 普及自然语言处理在生产环境中的应用 | https://github.com/hankcs/HanLP                         |
| THULAC | 清华大学中文词法分析工具包             | 具有中文分词和词性标注功能         | https://github.com/microbun/elasticsearch-thulac-plugin |



## IK分词器

elasticsearch本身自带的中文分词，就是单纯把中文一个字一个字的分开，根本没有词汇的概念。但是实际应用中，用户都是以词汇为条件，进行查询匹配的，如果能够把文章以词汇为单位切分开，那么与用户的查询条件能够更贴切的匹配上，查询速度也更加快速。



### 安装

- 地址https://github.com/medcl/elasticsearch-analysis-ik/releases?after=v6.3.2
- 解压

```bash
[ttshe@hadoop103 ~]$ mkdir -p /opt/module/elasticsearch/plugins/ik
[ttshe@hadoop103 ik]$ unzip /opt/software/elasticsearch-analysis-ik-6.3.1.zip -d /opt/module/elasticsearch/plugins/ik/
```

- 重启ES

- 测试
  - 使用 movie_index 的默认分词器分词

```bash
GET /movie_index/_analyze
{
  "text":"中华人民共和国"
}
```

- 结果

```bash
{
  "tokens": [
    {
      "token": "中",
      "start_offset": 0,
      "end_offset": 1,
      "type": "<IDEOGRAPHIC>",
      "position": 0
    },
    {
      "token": "华",
      "start_offset": 1,
      "end_offset": 2,
      "type": "<IDEOGRAPHIC>",
      "position": 1
    },
    {
      "token": "人",
      "start_offset": 2,
      "end_offset": 3,
      "type": "<IDEOGRAPHIC>",
      "position": 2
    },
    {
      "token": "民",
      "start_offset": 3,
      "end_offset": 4,
      "type": "<IDEOGRAPHIC>",
      "position": 3
    },
    {
      "token": "共",
      "start_offset": 4,
      "end_offset": 5,
      "type": "<IDEOGRAPHIC>",
      "position": 4
    },
    {
      "token": "和",
      "start_offset": 5,
      "end_offset": 6,
      "type": "<IDEOGRAPHIC>",
      "position": 5
    },
    {
      "token": "国",
      "start_offset": 6,
      "end_offset": 7,
      "type": "<IDEOGRAPHIC>",
      "position": 6
    }
  ]
}
```



### ik_smart模式

- 指定分词器 ik_smart

```json
GET /movie_index/_analyze
{
  "analyzer": "ik_smart", 
  "text":"中华人民共和国"
}
```

- 结果

```bash
{
  "tokens": [
    {
      "token": "中华人民共和国",
      "start_offset": 0,
      "end_offset": 7,
      "type": "CN_WORD",
      "position": 0
    }
  ]
}
```



### ik_max_word贪婪模式

```json
GET /movie_index/_analyze
{
  "analyzer": "ik_max_word", 
  "text":"中华人民共和国"
}
```

- 结果

```json
{
  "tokens": [
    {
      "token": "中华人民共和国",
      "start_offset": 0,
      "end_offset": 7,
      "type": "CN_WORD",
      "position": 0
    },
    {
      "token": "中华人民",
      "start_offset": 0,
      "end_offset": 4,
      "type": "CN_WORD",
      "position": 1
    },
    {
      "token": "中华",
      "start_offset": 0,
      "end_offset": 2,
      "type": "CN_WORD",
      "position": 2
    },
    {
      "token": "华人",
      "start_offset": 1,
      "end_offset": 3,
      "type": "CN_WORD",
      "position": 3
    },
    {
      "token": "人民共和国",
      "start_offset": 2,
      "end_offset": 7,
      "type": "CN_WORD",
      "position": 4
    },
    {
      "token": "人民",
      "start_offset": 2,
      "end_offset": 4,
      "type": "CN_WORD",
      "position": 5
    },
    {
      "token": "共和国",
      "start_offset": 4,
      "end_offset": 7,
      "type": "CN_WORD",
      "position": 6
    },
    {
      "token": "共和",
      "start_offset": 4,
      "end_offset": 6,
      "type": "CN_WORD",
      "position": 7
    },
    {
      "token": "国",
      "start_offset": 6,
      "end_offset": 7,
      "type": "CN_CHAR",
      "position": 8
    }
  ]
}
```

- 能够看出不同的分词器，分词有明显的区别，以后定义一个type不能再使用默认的mapping了
  - 要手工建立mapping, 因为要选择分词器



## 自定义词库

- 修改/usr/share/elasticsearch/plugins/ik/config/中的IKAnalyzer.cfg.xml
  - 本地配置
    - 在 ik 插件的 config/custom 目录下创建一个文件 xxx.dic
    - 在文件中添加词语即可， 每一个词语一行
  - 远程配置
    - 按照标红的路径利用nginx发布静态资源
    - 通过nginx配置服务即可
  - 注意词库
    - 如果是在 linux 中直接 vi 生成的， 可直接使用
    - 如果是在 windows中创建的，文件编码必须是 UTF-8 without BOM 格式
      -  UTF-8 无BOM 格式

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
    <comment>IK Analyzer 扩展配置</comment>
    <!--用户可以在这里配置自己的扩展字典 -->
    <entry key="ext_dict">custom/yyy.dic;custom/xxx.dic</entry>
    <!--用户可以在这里配置自己的扩展停止词字典-->
    <entry key="ext_stopwords"></entry>
    <!--用户可以在这里配置远程扩展字典 -->
    <entry key="remote_ext_dict">http://hadoop102/fenci/myword.txt</entry>
    <!--用户可以在这里配置远程扩展停止词字典-->
    <!-- <entry key="remote_ext_stopwords">words_location</entry> -->
</properties>
```

- 示例：远程配置
- 在nginx.conf中配置

```json
 server {
  listen  80;
  server_name hadoop102;
  location /fenci/ { // 表示访问的ip/xxx 等于本地路径下 es/xxx
     root es;
  }
}
```

- 在/usr/local/nginx/中创建/es/fenci/myword.txt
- myword.txt中编写关键词，每一行代表一个词

```txt
学习个
学习个技术
```

- 重启es服务器
- 重启nginx

- 在kibana中测试

```bash
GET /movie_index/_analyze
{
  "analyzer": "ik_max_word", 
  "text":"学习个技术"
}
// 结果
{
  "tokens": [
    {
      "token": "学习个技术",
      "start_offset": 0,
      "end_offset": 5,
      "type": "CN_WORD",
      "position": 0
    },
    {
      "token": "学习个",
      "start_offset": 0,
      "end_offset": 3,
      "type": "CN_WORD",
      "position": 1
    },
    {
      "token": "学习",
      "start_offset": 0,
      "end_offset": 2,
      "type": "CN_WORD",
      "position": 2
    },
...
```

- 更新完成后，es只会对新增的数据用新词分词。历史数据是不会重新分词的。如果想要历史数据重新分词。需要执行

```bash
POST movies_index_chn/_update_by_query?conflicts=proceed
```



# Mapping （等价于schema）

- 定义数据库中的表的结构的定义，通过mapping来控制索引存储数据的设置
  - 定义Index下的字段名（Field Name）
  - 定义字段的类型，比如数值型、字符串型、布尔型等
  - 定义倒排索引相关的配置，比如documentId、记录position、打分等

- 实际上每个type中的字段是什么数据类型，由mapping定义
- 如果没有设定mapping系统会自动，根据一条数据的格式来推断出应该的数据格式
  - true/false → boolean
  - 1020  →  long
  - 20.1 → double
  - “2018-02-01” → date
  - “hello world” → text +keyword
- 默认只有text会进行分词，keyword是不会分词的字符串
- mapping除了自动定义，还可以手动定义
  - 只能对新加的、没有数据的字段进行定义
  - 一旦有数据就无法再做修改
- 注意：虽然每个Field的数据放在不同的type下,但是同一个名字的Field在一个index下只能有一种mapping定义



## 获取索引的mapping

- 示例

```bash
GET /movie_index/_mapping/movie
```

- 结果

```bash
{
  "movie_index": { #索引名称
    "mappings": { #mapping设置
      "movie": { #type名称 推荐使用_doc, 在ES6上，使用index表示type的功能，type弱化，使用_doc代替
        "properties": { #字段属性
          "actorList": {
            "properties": {
              "id": {
                "type": "long"
              },
              "name": {
                "type": "text", #字段类型，字符串默认类型，进行分词
                "fields": { #子字段属性设置
                  "keyword": { #分词类型（不分词）如job.keyword匹配就不分词，直接job匹配就分词
                    "type": "keyword",
                    "ignore_above": 256 #超过256就不建立索引，其他字段都建立索引，其他类型都建立索引
                  }
                }
              }
            }
          },
          "doubanScore": {
            "type": "float"
            "index": true # 默认true ，表示需要建立索引，有些字段只用于显示，不进行查询可以设置为false，减少性能消耗
          },
          "id": {
            "type": "long"
          },
          "name": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          }
        }
      }
    }
  }
}
```





## 数据类型

| 类型       | 关键字                                    |
| ---------- | ----------------------------------------- |
| 字符串型   | text、keyword                             |
| 数值型     | long、integer、short、byte、double、float |
| 日期类型   | date                                      |
| 布尔类型   | boolean                                   |
| 二进制类型 | binary                                    |



## 创建 mapping [put mappings]

```bash
PUT my_class_index	
{
  "mappings": {
    "student":{					#类型名称
    	"dynamic":false,
      	"properties":{
        	"username":{"type": "text"}#字段类型
        }
      }
    }
  }
}
```

- 创建文档

```bash
PUT my_index1/student/1
{
  "username":"zhangsan"
}
```

- 示例

```bash
PUT movie_chn
{
  "mappings": {
    "movie":{
      "properties": {
        "id":{
          "type": "long"
        },
        "name":{
          "type": "text"
          , "analyzer": "ik_smart" # 定义分词器，不指定则默认是standard
        },
        "doubanScore":{
          "type": "double"
        },
        "actorList":{
          "properties": {
            "id":{
              "type":"long"
            },
            "name":{
              "type":"keyword" # 设置为keyword不进行分词，可以节省空间
            }
          }
        }
      }
    }
  }
```

- 插入数据

```json
PUT /movie_chn/movie/1
{ "id":1,
 "name":"红海行动",
 "doubanScore":8.5,
 "actorList":[  
     {"id":1,"name":"张译"},
     {"id":2,"name":"海清"},
     {"id":3,"name":"张涵予"}
 ]
}
PUT /movie_chn/movie/2
{
    "id":2,
    "name":"湄公河行动",
    "doubanScore":8.0,
    "actorList":[  
        {"id":3,"name":"张涵予"}
    ]
}

PUT /movie_chn/movie/3
{
    "id":3,
    "name":"红海事件",
    "doubanScore":5.0,
    "actorList":[  
        {"id":4,"name":"张晨"}
    ]
}
```

- 测试

```bash
GET /movie_chn/movie/_search
{
  "query": {
    "match": {
      "name": "红海战役" # 对name进行ik分词，然后进行匹配搜索，如果搜索 '海行' 就搜索不到
    }
  }
}

GET /movie_chn/movie/_search
{
  "query": {
    "term": {
      "actorList.name": "张译"
    }
  }
}
```



# 特性

- 分词表
  - 在ES的数据库中存储有分词表
  - 用于存储倒排索引
  - 表id 是分词，值是文档id
  - 只查询一次，将所有关联的文档查询出来

## 索引



### 正排索引

- 在mysql中为id建立的索引是正排索引
- 记录文档Id到文档内容的关联关系

- 查询机制
  - 通过id查询时候，会先查询索引库找到id，进而找到文档内容



### 倒排索引（重点）

- 倒排索引（Posting List）
  - 记录单词到**内容列表**的关联信息

- 单词词典（Term DicTionary）
  - 记录所有 文档的单词，一般比较大

![1569856861408](img/ELK/8.png)

- 通过内容关键字找到文档的位置
- 每个文档都有自己的多个倒排索引，每个文档有一个正排索引

- 通过给文档记录中的内容分词，将分出的词保存到分词表中，最后查询的时候通过分词表中的词倒推出文档的位置，从而获取文档



### ES数据存储和搜索

![1569858226029](img/ELK/9.png)

- 注意在ES6.0以后只能有一个type



# java 调用



- 目前市面上有两类客户端
  - TransportClient 为代表的ES原生客户端，不能执行原生dsl语句必须使用它的Java api方法
  - 以Rest Api为主的missing client，最典型的就是jest
    - 这种客户端可以直接使用dsl语句拼成的字符串，直接传给服务端，然后返回json字符串再解析
  - 两种方式各有优劣，但最近elasticsearch官网宣布计划在7.0以后的版本中废除TransportClient
  - 以RestClient为主



## pom

```xml
<dependency>
   <groupId>org.springframework.boot</groupId>
   <artifactId>spring-boot-starter-data-elasticsearch</artifactId>
</dependency>

<!-- https://mvnrepository.com/artifact/io.searchbox/jest -->
<dependency>
   <groupId>io.searchbox</groupId>
   <artifactId>jest</artifactId>
   <version>5.3.3</version>
</dependency>

<!-- https://mvnrepository.com/artifact/net.java.dev.jna/jna -->
<dependency>
   <groupId>net.java.dev.jna</groupId>
   <artifactId>jna</artifactId>
   <version>4.5.1</version>
</dependency>
```



## application.properties

```properties
spring.elasticsearch.jest.uris=http://hadoop102:9200
```



## 代码

```java
@Autowired
JestClient jestClient;

@Test
public void testEs() throws IOException {
    String query="{\n" +
        "  \"query\": {\n" +
        "    \"match\": {\n" +
        "      \"actorList.name\": \"张译\"\n" +
        "    }\n" +
        "  }\n" +
        "}";
    Search search = new Search.Builder(query).addIndex("movie_chn").addType("movie").build();

    SearchResult result = jestClient.execute(search);

    List<SearchResult.Hit<HashMap, Void>> hits = result.getHits(HashMap.class);

    for (SearchResult.Hit<HashMap, Void> hit : hits) {
        HashMap source = hit.source;
        System.err.println("source = " + source);
    }

}
```



# 问题处理



## 关于seccomp

- 启动过程中容器出现的错误如下

```bash
[ttshe@hadoop103 elasticsearch]$ bin/elasticsearch
[2019-09-29T21:54:38,970][WARN ][o.e.b.JNANatives         ] unable to install syscall filter: 
java.lang.UnsupportedOperationException: seccomp unavailable: CONFIG_SECCOMP not compiled into kernel, CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER are needed
	at org.elasticsearch.bootstrap.SystemCallFilter.linuxImpl(SystemCallFilter.java:341) ~[elasticsearch-6.3.1.jar:6.3.1]
...
```

- seccomp是linux kernel从2.6.23版本开始所支持的一种安全机制
  - 详见：https://en.wikipedia.org/wiki/Seccomp
- 主机操作系统是CentOS release 6.8 (Final)，没有支持seccomp
- 然而ES默认将利用内核的seccomp机制，所以报错

- 解决
  - ES是通过配置参数bootstrap.system_call_filter: true
    - 使用内核seccomp机制的
  - 在开发环境下可以将该参数值设为false

```bash
[es@localhost]$ vim config/elasticsearch.yml
# 默认该参数值不在配置文件中，添加并设置为false即可

bootstrap.system_call_filter: false
```



## 启动后有三个警告

```bash
[ttshe@hadoop103 elasticsearch]$ bin/elasticsearch

[2019-09-29T21:58:50,618][WARN ][o.e.b.BootstrapChecks    ] [9oEk0A8] max file descriptors [4096] for elast too low, increase to at least [65536]
[2019-09-29T21:58:50,618][WARN ][o.e.b.BootstrapChecks    ] [9oEk0A8] max number of threads [1024] for user, increase to at least [4096]
[2019-09-29T21:58:50,618][WARN ][o.e.b.BootstrapChecks    ] [9oEk0A8] max virtual memory areas vm.max_map_c low, increase to at least [262144]
```

- 最大文件描述符配置过低，至少65536

```bash
切换到root用户，编辑limits.conf 添加类似如下内容
vi /etc/security/limits.conf 
添加如下内容:
* soft nofile 65536
* hard nofile 131072
* soft nproc 2048
* hard nproc 4096
```

- 系统配置的线程数过低，至少4096

```bash
切换到root用户，进入limits.d目录下修改配置文件。
vi /etc/security/limits.d/90-nproc.conf 
修改如下内容：
* soft nproc 1024
# 软链接线程数 修改为
* soft nproc 4096
```

- 虚拟内存过低，至少262144

```bash
需要切换到root用户修改配置sysctl.conf
vi /etc/sysctl.conf 
添加下面配置：
vm.max_map_count=655360
fs.file-max=655360
```



