# Table API

- Table API 是流处理和批处理通用的关系型 API
- Table API 基于流输入或者批输入来运行而不需要进行任何修改
- Table API 是 SQL 语言的超集并专门为 Apache Flink 设计的，Table API 是 Scala 和 Java 语言集成式的 API
- 与常规 SQL 语言中将查询指定为字符串不同，Table API 查询是以 Java 或 Scala 中的语言嵌入样式来定义
  的



## pom

```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table_2.11</artifactId>
    <version>1.7.2</version>
</dependency>
```



## 创建

```scala
def main(args: Array[String]): Unit = {
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

    val myKafkaConsumer: FlinkKafkaConsumer011[String] = MyKafkaUtil.getConsumer("GMALL_STARTUP")
    val dstream: DataStream[String] = env.addSource(myKafkaConsumer)

    val tableEnv: StreamTableEnvironment = TableEnvironment.getTableEnvironment(env)

    val startupLogDstream: DataStream[StartupLog] = dstream.map{ jsonString =>JSON.parseObject(jsonString,classOf[StartupLog]) }

    val startupLogTable: Table = tableEnv.fromDataStream(startupLogDstream)

    val table: Table = startupLogTable.select("mid,ch").filter("ch ='appstore'")

    val midchDataStream: DataStream[(String, String)] = table.toAppendStream[(String,String)]

    midchDataStream.print()
    env.execute()
}
```



## 动态表

- 如果流中的数据类型是case class可以直接根据case class的结构生成table

```scala
tableEnv.fromDataStream(startupLogDstream)  
```

- 或者根据字段顺序单独命名

```scala
tableEnv.fromDataStream(startupLogDstream,’mid,’uid  .......)  
```

- 最后的动态表可以转换为流进行输出

```scala
table.toAppendStream[(String,String)]
```



## 字段

- 用一个单引放到字段前面 来标识字段名, 如 ‘name , ‘mid ,’amount 等



## 示例

```scala
//每10秒中渠道为appstore的个数
def main(args: Array[String]): Unit = {
    //sparkcontext
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

    //时间特性改为eventTime
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)

    val myKafkaConsumer: FlinkKafkaConsumer011[String] = MyKafkaUtil.getConsumer("GMALL_STARTUP")
    val dstream: DataStream[String] = env.addSource(myKafkaConsumer)

    val startupLogDstream: DataStream[StartupLog] = dstream.map{ jsonString =>JSON.parseObject(jsonString,classOf[StartupLog]) }
    //告知watermark 和 eventTime如何提取
    val startupLogWithEventTimeDStream: DataStream[StartupLog] = startupLogDstream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[StartupLog](Time.seconds(0L)) {
        override def extractTimestamp(element: StartupLog): Long = {
            element.ts
        }
    }).setParallelism(1)

    //SparkSession
    val tableEnv: StreamTableEnvironment = TableEnvironment.getTableEnvironment(env)

    //把数据流转化成Table
    val startupTable: Table = tableEnv.fromDataStream(startupLogWithEventTimeDStream , 'mid,'uid,'appid,'area,'os,'ch,'logType,'vs,'logDate,'logHour,'logHourMinute,'ts.rowtime)//rowtime 表示事件时间

    //通过table api 进行操作
    // 每10秒 统计一次各个渠道的个数 table api 解决
    //1 groupby  2 要用 window   3 用eventtime来确定开窗时间
    val resultTable: Table = startupTable.window(Tumble over 10000.millis on 'ts as 'tt).groupBy('ch,'tt ).select( 'ch, 'ch.count)



    //把Table转化成数据流
    //val appstoreDStream: DataStream[(String, String, Long)] = appstoreTable.toAppendStream[(String,String,Long)]
    val resultDstream: DataStream[(Boolean, (String, Long))] = resultSQLTable.toRetractStream[(String,Long)]

    resultDstream.filter(_._1).print()

    env.execute()

}
```



## groupby

- 如果使用 groupby table转换为流的时候只能用toRetractDstream 

```scala
val rDstream: DataStream[(Boolean, (String, Long))] = table.toRetractStream[(String,Long)]
```

- toRetractDstream 得到的第一个boolean型字段标识 true就是最新的数据，false表示过期老数据

```scala
val rDstream: DataStream[(Boolean, (String, Long))] = table.toRetractStream[(String,Long)]
rDstream.filter(_._1).print()
```

- 如果使用的api包括时间窗口，那么时间的字段必须，包含在group by中

```scala
val table: Table = startupLogTable.filter("ch ='appstore'").window(Tumble over 10000.millis on 'ts as 'tt).groupBy('ch ,'tt).select("ch,ch.count ")
```



## 时间窗口

- 用到时间窗口，必须提前声明时间字段，如果是processTime直接在创建动态表时进行追加就可以

```scala
val startupLogTable: Table = tableEnv.fromDataStream(startupLogWithEtDstream,'mid,'uid,'appid,'area,'os,'ch,'logType,'vs,'logDate,'logHour,'logHourMinute,'ts.rowtime)
```

- 如果是EventTime要在创建动态表时声明

```scala
val startupLogTable: Table = tableEnv.fromDataStream(startupLogWithEtDstream,'mid,'uid,'appid,'area,'os,'ch,'logType,'vs,'logDate,'logHour,'logHourMinute,'ps.processtime)
```

- 滚动窗口可以使用Tumble over 10000.millis on

```scala
val table: Table = startupLogTable.filter("ch ='appstore'").window(Tumble over 10000.millis on 'ts as 'tt).groupBy('ch ,'tt).select("ch,ch.count ")
```



# SQL

```scala
def main(args: Array[String]): Unit = {
    //sparkcontext
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

    //时间特性改为eventTime
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)

    val myKafkaConsumer: FlinkKafkaConsumer011[String] = MyKafkaUtil.getConsumer("GMALL_STARTUP")
    val dstream: DataStream[String] = env.addSource(myKafkaConsumer)

    val startupLogDstream: DataStream[StartupLog] = dstream.map{ jsonString =>JSON.parseObject(jsonString,classOf[StartupLog]) }
    //告知watermark 和 eventTime如何提取
    val startupLogWithEventTimeDStream: DataStream[StartupLog] = startupLogDstream.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[StartupLog](Time.seconds(0L)) {
        override def extractTimestamp(element: StartupLog): Long = {
            element.ts
        }
    }).setParallelism(1)

    //SparkSession
    val tableEnv: StreamTableEnvironment = TableEnvironment.getTableEnvironment(env)

    //把数据流转化成Table
    val startupTable: Table = tableEnv.fromDataStream(startupLogWithEventTimeDStream , 'mid,'uid,'appid,'area,'os,'ch,'logType,'vs,'logDate,'logHour,'logHourMinute,'ts.rowtime)

    //通过table api 进行操作
    // 每10秒 统计一次各个渠道的个数 table api 解决
    //1 groupby  2 要用 window   3 用eventtime来确定开窗时间
    val resultTable: Table = startupTable.window(Tumble over 10000.millis on 'ts as 'tt).groupBy('ch,'tt ).select( 'ch, 'ch.count)
    // 通过sql 进行操作

    val resultSQLTable : Table = tableEnv.sqlQuery( "select ch ,count(ch)   from "+startupTable+"  group by ch   ,Tumble(ts,interval '10' SECOND )")

    //把Table转化成数据流
    //val appstoreDStream: DataStream[(String, String, Long)] = appstoreTable.toAppendStream[(String,String,Long)]
    val resultDstream: DataStream[(Boolean, (String, Long))] = resultSQLTable.toRetractStream[(String,Long)]

    resultDstream.filter(_._1).print()

    env.execute()

}
```

