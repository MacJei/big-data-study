# cut

> 在文本中负责剪切数据，从文件的每一行剪切字节，字符，字段，并输出

基本语法

- 默认分隔符是制表符 tab
- 选项参数说明
  - -f 列号，提取第几列
  - -d 分隔符，按照指定分割符分割列

```shell
cut [选项参数] filename
```

示例：按照空格split，然后取得第1列和第11列

```shell
[root@hadoop100 sh-demo]# ifconfig | grep eth | cut -d " " -f 1,11
eth0 00:0C:29:D1:82:07
```



# sed

> 一种流式编辑器，一次处理一行内容，处理时把当前处理的行存储在临时缓冲区中（模式空间），然后用sed命令处理缓冲区的内容，处理完成后将缓冲区的内容推送屏幕，接着处理下一行，不断重复，直到文件结尾，**文件内容没有改变**，除非使用重定向存储输出

基本用法

```shell
sed [选项] 命令 filename
选项：
-e	直接在指令模式上进行sed的动作编辑
命令：
a 	新增，a的后面可以接字串，在下一行出现
d 	删除
s	查找并替换
```

示例

```shell
# 数据准备
[root@hadoop100 sh-demo]# cat sed.txt 
dong shen
guan zhen
wo wo
lai lai

le le
# 将 mei nv 插入到第二行下
[root@hadoop100 sh-demo]# sed '2a mei nv' sed.txt 
dong shen
guan zhen
mei nv
wo wo
lai lai

le le
# 在dong行下插入123
[root@hadoop100 sh-demo]# sed '/dong/a 123' sed.txt 
dong shen
123
guan zhen
wo wo
lai lai

le le
# 注意文件并没有改变
[root@hadoop100 sh-demo]# cat sed.txt 
dong shen
guan zhen
wo wo
lai lai

le le
# 删除sed.txt文件所包含的所有wo的行
[root@hadoop100 sh-demo]# sed '/wo/d' sed.txt
dong shen
guan zhen
lai lai

le le
# 将wo替换为ni
[root@hadoop100 sh-demo]# sed 's/wo/ni/g' sed.txt
dong shen
guan zhen
ni ni
lai lai

le le
# g表示全部
# 将sed.txt 文件中的第二行删除，并将wo替换为ni
[root@hadoop100 sh-demo]# sed -e '2d' -e 's/wo/ni/g' sed.txt
dong shen
ni ni
lai lai

le le
```



# awk

> 文本分析工具，把文件逐行读取，用空格作为默认分隔符进行切片，对每个切片进行分析处理

基本用法

- 只有匹配了pattern，才会执行action

```shell
awk [选项参数] ‘pattern1{action1} pattern2{action2} ...’ filename
说明：
	pattern		表示AWK在数据中查找的内容，匹配模式，正则表达式/.../
	action		找到匹配内容时进行的命令
		BEGIN	在所有数据读取之前执行
		END		在所有数据执行之后执行
选项：
	-F			指定输入文件拆分的分隔符
	-v			赋值一个自定义变量
内置变量：
	FILENAME	文件名
	NR			已读的记录数
	NF			浏览记录的域的个数，切割后，列的个数
```

 示例

```shell
# 1.数据准备 拷贝passwd文件到当前目录
[root@hadoop100 sh-demo]# sudo cp /etc/passwd ./
[root@hadoop100 sh-demo]# cat passwd 
root:x:0:0:root:/root:/bin/bash
bin:x:1:1:bin:/bin:/sbin/nologin
daemon:x:2:2:daemon:/sbin:/sbin/nologin
adm:x:3:4:adm:/var/adm:/sbin/nologin
lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
...
# 打印第1列和第7列
[root@hadoop100 sh-demo]# awk -F: '{print $1,$7}' passwd 
root /bin/bash
bin /sbin/nologin
daemon /sbin/nologin
...
# 搜索passwd 文件以root关键字开头的所有行，并输出该行的第7列
[root@hadoop100 sh-demo]# awk -F: '/^root/{print $7}' passwd 
/bin/bash

# 多个模式下的搜索 如果2个模式都满足，那么都输出
[root@hadoop100 sh-demo]# awk -F: '/^root/{print $1} /^a/{print $1}' passwd 
root
adm
avahi-autoipd
abrt
apache

# 搜索passwd 文件以root关机字开头的所有航，输出第1列和第7列，中间以逗号分隔
[root@hadoop100 sh-demo]# awk -F: '/^root/{print $1","$7}' passwd 
root,/bin/bash

# 在输出的数据上添加开始和结束信息
[root@hadoop100 sh-demo]# awk -F: 'BEGIN{print "....start..."} /^root/{print $1","$7} END{print "...end..."}' passwd 
....start...
root,/bin/bash
...end...

# 将passwd文件中用户id 增加数值1并输出
[root@hadoop100 sh-demo]#  awk -v i=1 -F : '{print $3+i}' passwd 
1
2
3
4
# 对用户id进行累加输出
[root@hadoop100 sh-demo]#  awk -F : 'BEGIN{sum=0} {print $3;sum+=$3} END{print "sum="sum}' passwd 
0
1
2
3
...
501
sum=69322
```



# sort

> 将文件进行排序，将排序结果进行标准输出

基本语法

```shell
sort(选项)(参数)
选项：
	-n 	依照数值的大小排序
	-r	以相反的顺序排序，默认从小到大排列
	-t	设置排序时的分隔符
	-k	指定需要排序的列
参数：
	指定代排序的文件列表
```

示例

```shell
# 数据准备
[root@hadoop100 sh-demo]# cat sort.txt 
bb:40:5.4
dd:20:4.2
xz:50:2.3
cls:10:3.5
ss:30:1.6
# 按照:分隔的第三列倒叙排列
[root@hadoop100 sh-demo]# sort -t : -nrk 3 sort.txt 
bb:40:5.4
dd:20:4.2
cls:10:3.5
xz:50:2.3
ss:30:1.6
# 写法灵活
[root@hadoop100 sh-demo]# sort -nrt : -k 3 sort.txt 
```

