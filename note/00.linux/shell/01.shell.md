


## 运算符

基本语法

- $((运算式)) 或者 $[运算式]

- expr
  - `+` 加 
  - `-` 减
  - `\*` 乘
  - `/` 除
  - `%` 取余
  - 注意：expr 运算符之间要有空格
  - 使用的频率比较低

```shell
示例：
[root@hadoop100 sh-demo]# A=1
[root@hadoop100 sh-demo]# C=$[$A+2]
[root@hadoop100 sh-demo]# echo $C
3
[root@hadoop100 sh-demo]# D=$A+3
[root@hadoop100 sh-demo]# echo $D
1+3
# 使用$[]本质上将字符串解析为运算公式，计算出来
[root@hadoop100 sh-demo]# echo $[$D]
4

示例：
[root@hadoop100 sh-demo]# expr 2 + 3
5
[root@hadoop100 sh-demo]# expr 2 / 3
0
[root@hadoop100 sh-demo]# expr 2 % 3
2
[root@hadoop100 sh-demo]# expr 2 \* 3
6

示例：复杂计算，一般使用$[]
[root@hadoop100 sh-demo]# expr `expr 2 + 3` \* 4
20
[root@hadoop100 sh-demo]# S=$[(2+3)*4]
[root@hadoop100 sh-demo]# echo $S
20
```


## 条件判断

基本语法

[ condition ] 

- condition左右要有空格，每个元素之间都要有空格
- 条件非空就是true，[ test ] 返回true,[] 返回false

常用判断条件

- 2个整数之间的比较
  - = 字符串比较
  - -lt 小于 less than
  - -le 小于等于 less equal
  - -eq 等于 equal
  - -gt 大于 greater than
  - -ge 大于等于 greater equal
  - -ne 不等于 not equal
- 按照文件的权限进行判断
  - -r 有读的权限 read
  - -w 有写得权限 write
  - -x 有执行的权限 execute
- 按照文件类型进行判断
  - -f 文件存在并且是一个常规文件 file
  - -e 文件存在 existence
  - -d 文件存在且是一个目录 directory

```shell
示例：23 >= 22 ?
[root@hadoop100 sh-demo]# [ 23 -ge 22 ]
[root@hadoop100 sh-demo]# echo $?
0
[root@hadoop100 sh-demo]# [ 23 -ge 26 ]
[root@hadoop100 sh-demo]# echo $?
1
示例：
[root@hadoop100 sh-demo]# A=1
[root@hadoop100 sh-demo]# [ $A -eq 1 ]
[root@hadoop100 sh-demo]# echo $?
0
[root@hadoop100 sh-demo]# [ $A -eq 2 ]
[root@hadoop100 sh-demo]# echo $?
1
示例：文件权限判断
[root@hadoop100 sh-demo]# [ -x paramter.sh ]
[root@hadoop100 sh-demo]# echo $?
0
示例：判断文件类型
[root@hadoop100 sh-demo]# [ -d /home/sh-demo/paramter.sh ]
[root@hadoop100 sh-demo]# echo $?
1
```


## 流程控制

### if 判断

基本语法

- [ 条件表达式 ]，括号和条件表达式之间必须要有空格
- if后要有空格

```shell
# 写法1
if [ 条件表达式 ];then
	...
elif [ 条件表达式2 ]；then
	...
else 
	...
fi
# 写法2
if [ 条件表达式 ]
	then 
		...
elif [ 条件表达式2 ]
	then
		...
else 
	...
fi
```

示例

 ```shell
[root@hadoop100 sh-demo]# chmod 777 if.sh 
[root@hadoop100 sh-demo]# cat if.sh 
#!/bin/bash
if [ $1 -eq 1 ];then
	echo success
elif [ $1 -eq 2 ];then
	echo success2
else
	echo fail
fi
[root@hadoop100 sh-demo]# ./if.sh 1
success
[root@hadoop100 sh-demo]# ./if.sh 2
success2
[root@hadoop100 sh-demo]# ./if.sh 3
fail
 ```

###  case 语句

基本语法

- case 尾行必须in单词结尾，每一个模式匹配必须使用 ） 结束
- 双分号 ;; 表示命令序列结束，相当于java中的break
- 最后的 *） 表示默认模式，相当于java中的default

```shell
case $变量名 in
	"值1"){
		...
		};;
	"值2"){
		...
	};;
	*){
			#如果变量都不满足
	};;
esac
```

示例

```shell
[root@hadoop100 sh-demo]# cat case.sh 
#!/bin/bash
case $1 in
	"1")
		echo success1
		;;
	"2")	
		echo success2
		;;	
	*)
		echo default
		;;
esac
[root@hadoop100 sh-demo]# chmod 777 case.sh 
[root@hadoop100 sh-demo]# ./case.sh 1
success1
[root@hadoop100 sh-demo]# ./case.sh 2
success2
[root@hadoop100 sh-demo]# ./case.sh 3
default
```

### for 循环

基本语法

```shell
for ((初始值；循环控制条件；变量变化))
	do
		...
done
#写法2,使用；可以避免换行，将do放在for一行上
for ((初始值；循环控制条件；变量变化));do
		...
done
#用法2：值使用空格连接 那么$*也可以遍历
for 变量 in 值1 值2 值3 ...
	do
		...
done
```

示例

```shell
[root@hadoop100 sh-demo]# cat for.sh 
#!/bin/bash
for ((i=1;i<5;i++));do
	echo $i
done
[root@hadoop100 sh-demo]# ./for.sh 
1
2
3
4
# 使用用法2的方式
[root@hadoop100 sh-demo]# cat for2.sh 
#!/bin/bash
for i in $*;do
	echo $i
done
echo ======
for i in $@;do
	echo $i
done
[root@hadoop100 sh-demo]# ./for2.sh 1 2 3 4 5
1
2
3
4
5
======
1
2
3
4
5
# 注意 $* 和 $@ 的区别
[root@hadoop100 sh-demo]# cat for2.sh 
#!/bin/bash
for i in "$*";do
	echo $i
done
echo ======
for i in "$@";do
	echo $i
done
[root@hadoop100 sh-demo]# ./for2.sh 1 2 3 4 5
1 2 3 4 5
======
1
2
3
4
5
```

### while  循环

基本语法

```shell
while [ 条件表达式 ]
do
	...
done
```

示例

```shell
[root@hadoop100 sh-demo]# cat while.sh 
#!/bin/bash
i=1
sum=0
while [ $i -lt 101 ]
do
	sum=$[$i+$sum]
	i=$[$i+1]
done
echo 'sum='$sum
[root@hadoop100 sh-demo]# ./while.sh 
sum=5050
```



## read读取控制台输入

> 用于用户和控制台进行交互使用，类似于java中的System.in.read()

基本语法

```shell
read(选项)(参数)
选项：
	-p 	指定读取值时的提示符
	-t 	指定读取值时等待的时间，单位s
参数：
	变量名  指定读取值的变量名
```

示例

```shell
[root@hadoop100 sh-demo]# cat read.sh 
#!/bin/bash
read -t 5 -p "请输入任意文本" wenben
echo $wenben

[root@hadoop100 sh-demo]# ./read.sh
请输入任意文本test
test
```



## 函数

### 系统函数

#### basename

基本语法

- basename命令会删除所有前缀包括==最后一个 /== 字符，然后将字符串显示出来
- suffix如果被指定了，basename会将pathname或string中的suffix去除

```shell
basename [string/pathname] [suffix]
```

示例：截取文件名称

```shell
[root@hadoop100 sh-demo]# basename /home/sh-demo/paramter.sh .sh
paramter
[root@hadoop100 sh-demo]# basename /home/sh-demo/paramter.sh
paramter.sh
```

#### dirname

基本语法

- 从给定的包含绝对路径的文件名称中去除文件名，返回剩下的路径

```shell
dirname [文件的绝对路径]
```

示例

```shell
[root@hadoop100 sh-demo]# dirname /home/sh-demo/paramter.sh 
/home/sh-demo
```



### 自定义函数

> 解释型语言，先声明后使用
> 函数返回值，只能通过$?系统变量获取，可以通过return返回，如果没有return，则通过最后一条命令返回运行结果作为返回值，return 后面添加数值n(0-255)

```shell
[function] func_name[()]
{
    ...
    [return int;]
}
# 调用 
func_name
```

示例：注意入参使用$调用

```shell
[root@hadoop100 sh-demo]# cat fun.sh 
#!/bin/bash
function add()
{
	return $[$1+$2]
}
add 1 2
echo $?

[root@hadoop100 sh-demo]# ./fun.sh 
3
```



## Shell工具

### cut

> 在文本中负责剪切数据，从文件的每一行剪切字节，字符，字段，并输出

基本语法

- 默认分隔符是制表符 tab
- 选项参数说明
  - -f 列号，提取第几列
  - -d 分隔符，按照指定分割符分割列

```shell
cut [选项参数] filename
```

示例：按照空格split，然后取得第1列和第11列

```shell
[root@hadoop100 sh-demo]# ifconfig | grep eth | cut -d " " -f 1,11
eth0 00:0C:29:D1:82:07
```



### sed

> 一种流式编辑器，一次处理一行内容，处理时把当前处理的行存储在临时缓冲区中（模式空间），然后用sed命令处理缓冲区的内容，处理完成后将缓冲区的内容推送屏幕，接着处理下一行，不断重复，直到文件结尾，**文件内容没有改变**，除非使用重定向存储输出

基本用法

```shell
sed [选项] 命令 filename
选项：
-e	直接在指令模式上进行sed的动作编辑
命令：
a 	新增，a的后面可以接字串，在下一行出现
d 	删除
s	查找并替换
```

示例

```shell
# 数据准备
[root@hadoop100 sh-demo]# cat sed.txt 
dong shen
guan zhen
wo wo
lai lai

le le
# 将 mei nv 插入到第二行下
[root@hadoop100 sh-demo]# sed '2a mei nv' sed.txt 
dong shen
guan zhen
mei nv
wo wo
lai lai

le le
# 在dong行下插入123
[root@hadoop100 sh-demo]# sed '/dong/a 123' sed.txt 
dong shen
123
guan zhen
wo wo
lai lai

le le
# 注意文件并没有改变
[root@hadoop100 sh-demo]# cat sed.txt 
dong shen
guan zhen
wo wo
lai lai

le le
# 删除sed.txt文件所包含的所有wo的行
[root@hadoop100 sh-demo]# sed '/wo/d' sed.txt
dong shen
guan zhen
lai lai

le le
# 将wo替换为ni
[root@hadoop100 sh-demo]# sed 's/wo/ni/g' sed.txt
dong shen
guan zhen
ni ni
lai lai

le le
# g表示全部
# 将sed.txt 文件中的第二行删除，并将wo替换为ni
[root@hadoop100 sh-demo]# sed -e '2d' -e 's/wo/ni/g' sed.txt
dong shen
ni ni
lai lai

le le
```



### awk

> 文本分析工具，把文件逐行读取，用空格作为默认分隔符进行切片，对每个切片进行分析处理

基本用法

- 只有匹配了pattern，才会执行action

```shell
awk [选项参数] ‘pattern1{action1} pattern2{action2} ...’ filename
说明：
	pattern		表示AWK在数据中查找的内容，匹配模式，正则表达式/.../
	action		找到匹配内容时进行的命令
		BEGIN	在所有数据读取之前执行
		END		在所有数据执行之后执行
选项：
	-F			指定输入文件拆分的分隔符
	-v			赋值一个自定义变量
内置变量：
	FILENAME	文件名
	NR			已读的记录数
	NF			浏览记录的域的个数，切割后，列的个数
```

 示例

```shell
# 1.数据准备 拷贝passwd文件到当前目录
[root@hadoop100 sh-demo]# sudo cp /etc/passwd ./
[root@hadoop100 sh-demo]# cat passwd 
root:x:0:0:root:/root:/bin/bash
bin:x:1:1:bin:/bin:/sbin/nologin
daemon:x:2:2:daemon:/sbin:/sbin/nologin
adm:x:3:4:adm:/var/adm:/sbin/nologin
lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
...
# 打印第1列和第7列
[root@hadoop100 sh-demo]# awk -F: '{print $1,$7}' passwd 
root /bin/bash
bin /sbin/nologin
daemon /sbin/nologin
...
# 搜索passwd 文件以root关键字开头的所有行，并输出该行的第7列
[root@hadoop100 sh-demo]# awk -F: '/^root/{print $7}' passwd 
/bin/bash

# 多个模式下的搜索 如果2个模式都满足，那么都输出
[root@hadoop100 sh-demo]# awk -F: '/^root/{print $1} /^a/{print $1}' passwd 
root
adm
avahi-autoipd
abrt
apache

# 搜索passwd 文件以root关机字开头的所有航，输出第1列和第7列，中间以逗号分隔
[root@hadoop100 sh-demo]# awk -F: '/^root/{print $1","$7}' passwd 
root,/bin/bash

# 在输出的数据上添加开始和结束信息
[root@hadoop100 sh-demo]# awk -F: 'BEGIN{print "....start..."} /^root/{print $1","$7} END{print "...end..."}' passwd 
....start...
root,/bin/bash
...end...

# 将passwd文件中用户id 增加数值1并输出
[root@hadoop100 sh-demo]#  awk -v i=1 -F : '{print $3+i}' passwd 
1
2
3
4
# 对用户id进行累加输出
[root@hadoop100 sh-demo]#  awk -F : 'BEGIN{sum=0} {print $3;sum+=$3} END{print "sum="sum}' passwd 
0
1
2
3
...
501
sum=69322
```

### sort

> 将文件进行排序，将排序结果进行标准输出

基本语法

```shell
sort(选项)(参数)
选项：
	-n 	依照数值的大小排序
	-r	以相反的顺序排序，默认从小到大排列
	-t	设置排序时的分隔符
	-k	指定需要排序的列
参数：
	指定代排序的文件列表
```

示例

```shell
# 数据准备
[root@hadoop100 sh-demo]# cat sort.txt 
bb:40:5.4
dd:20:4.2
xz:50:2.3
cls:10:3.5
ss:30:1.6
# 按照:分隔的第三列倒叙排列
[root@hadoop100 sh-demo]# sort -t : -nrk 3 sort.txt 
bb:40:5.4
dd:20:4.2
cls:10:3.5
xz:50:2.3
ss:30:1.6
# 写法灵活
[root@hadoop100 sh-demo]# sort -nrt : -k 3 sort.txt 
```

