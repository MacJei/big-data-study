# MapReduce学习



## 简述

### 定义

- MapReduce是==一个分布式运算程序的编程框架==
- 用于基于Hadoop的数据分析引用的核心框架
- 将==用户编写的业务代码==和==自带默认组件==整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上

### 优点

- 易于编程
  - 简单的实现一些接口，完成一个分布式程序
  - 分布式程序可以部署到大量廉价的机器上
  - 编写的程序形式和一个串行的程序一致
- 良好的扩展性
  - 计算资源不满足，可以通过简单的增加机器来扩展计算能力
- 高容错性
  - 如果其中一台机器宕机，可以将上面的==计算任务转移==到另一个节点上运行，整个过程不需要人工参与，系统自动完成
- 适合PB级以上的海量数据的离线处理
  - 上千台服务器集群并发工作，提供数据处理能力

### 缺点

- 不擅长实时计算
  - MapReduce无法像MySql一样，在ms内返回结果
- 不擅长流式计算
  - 流式计算的输入数据是动态的，而MapReduce的==输入数据集是静态的==，不能动态变化
  - 数据源必须是静态的
- 不擅长DAG（有向图）计算
  - DAG：多个应用程序存在依赖关系，后一个程序的输入是前一个程序的输出
  - MapReduce可以使用DAG计算，但是==每个MapReduce的作业结果都会输出到磁盘，会造成大量的IO，性能非常低下==



### 核心思想

![1557036126179](img\hadoop\04.mr01.png)



- 分布式的运算程序至少分为2个阶段
  - 第一个阶段：mapTask并发实例，完全并行运行，互不相干
  - 第二个阶段：reduceTask并发实例，互不相干，但是数据依赖于上一个阶段的所有mapTask并发实例的输出
- MapReduce模型只能包含一个map阶段和一个reduce阶段，如果用户的逻辑非常复杂，只能多个MapReduce程序串行运行



### 编写程序

#### MapReduce进程

一个完整的MapReduce程序在分布式运行时有3个进程

- MrAppMaster：负责整个程序的过程调度和状态协调

- MapTask：负责Map阶段的整个数据处理流程

- ReduceTask：负责Reduce阶段的整个数据处理流程

  

#### 官方WordCount源码反编译

- 01_jar包\02_win7下编译过的hadoop jar包\hadoop-2.7.2\share\hadoop\mapreduce\hadoop-mapreduce-examples-2.7.2.jar
- 对官方的Demo进行反编译，发现WordCount案例有Map类，Reduce类和驱动类
- 且数据类型是Hadoop自身封装的序列化类型



#### 常用数据序列化类型

一个包装类，用于Hadoop进行序列化操作

| **Java类型** | **Hadoop Writable类型** |
| ------------ | ----------------------- |
| boolean      | BooleanWritable         |
| byte         | ByteWritable            |
| int          | IntWritable             |
| float        | FloatWritable           |
| long         | LongWritable            |
| double       | DoubleWritable          |
| String       | Text                    |
| map          | MapWritable             |
| array        | ArrayWritable           |



#### 编程规范

##### Mapper阶段

- 用户定义的Mapper需要继承自己的父类
- 输入数据是K1V1对的形式
- K和V类型可以自定义
- 业务逻辑写在map()中
- 输出数据是K2V2对的形式
- map方法（MapTask进程）对每一个<K,V>调用一次



##### Reducer阶段

- 用户自定义Reducer要继承自己的父类
- Reducer的输入数据类型对应Mapper的输出数据类型，也是KV类型
- 业务逻辑写在reduce()中
- ReduceTask进程对每一组相同k的<K,V>组调用一次reduce()方法



##### Dirver阶段

- 相当于YARN集群的客户端，用于提交整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象



#### 编写WordCount

- 需求：给定的文本中统计输出每一个单词出现的总次数
- 输入数据：hello.txt

```shell
atguigu atguigu
ss ss
cls cls
jiao
banzhang
xue
hadoop
```

- 期望输出：

```shell
atguigu	2
banzhang	1
cls	2
hadoop	1
jiao	1
ss	2
xue	1
```

- 分析
  - Mapper阶段
    - 将MapTask传给我们的文本内容先转换为String
    - 根据空格将这一行切分成单词
    - 将单词输出<单词，1>
  - Reducer阶段
    - 汇总各个key的个数
    - 输出该key的总次数
  - Driver
    - 获取配置信息，获取job对象实例
    - 指定本程序的jar包所在的本地路径
    - 关联Mapper/Reducer业务类
    - 指定Mapper输出数据的KV类型
    - 指定最终输出的数据的KV类型
    - 指定job的输入原始文件所在的目录
    - 指定job的输出结果所在的目录
    - 提交作业

##### 创建项目

创建一个maven项目，添加如下pom，如果是springboot项目，可以不用添加log4j

```xml
<dependencies>
		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
			<version>RELEASE</version>
		</dependency>
		<dependency>
			<groupId>org.apache.logging.log4j</groupId>
			<artifactId>log4j-core</artifactId>
			<version>2.8.2</version>
		</dependency>
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-common</artifactId>
			<version>2.7.2</version>
		</dependency>
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-client</artifactId>
			<version>2.7.2</version>
		</dependency>
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-hdfs</artifactId>
			<version>2.7.2</version>
		</dependency>
</dependencies>
```

如果是springBoot项目下，不用添加如下日志配置(在项目的src/main/resources目录下，新建一个文件，命名为log4j.properties)

```properties
log4j.rootLogger=INFO, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
log4j.appender.logfile=org.apache.log4j.FileAppender
log4j.appender.logfile.File=target/spring.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
```



##### 编写程序

- 编写Mapper类

```java
package com.stt.demo.mr.wordCount;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * KEYIN:输入数据Key
 * VALUEIN:输入数据value
 * KEYOUT:输出数据key
 * VALUEOUT：输出数据value
 * Created by Administrator on 2019/5/5.
 */
public class WordCountMapper extends Mapper<LongWritable,Text,Text,IntWritable>{

	// 这里使用属性变量的意义在于节省内存
	// 定义输出的key对象
	Text k = new Text();
	// 定义输出的值,值都是1,匹配到一个单词就放入context中
	IntWritable v = new IntWritable(1);

	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		// 读取一行数据
		String line = value.toString();
		// 对这一行进行空格分隔
		String[] words = line.split("\\s+");
		// 输出
		for(String word : words){
			k.set(word);
			context.write(k,v);
		}
	}
}
```



- 编写Reducer类

```java
package com.stt.demo.mr.wordCount;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

/**
 * 输入的是单词text和1
 * 输出的是单词text和具体的个数
 * Created by Administrator on 2019/5/5.
 */
public class WordCountReducer extends Reducer<Text,IntWritable,Text,IntWritable> {


	int sum = 0;
	IntWritable v = new IntWritable();

	// 每次会获取一个key，value的list作为输入
	@Override
	protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
		sum = 0;
		for(IntWritable count : values){
			sum += count.get();
		}
		// 输出
		v.set(sum);
		context.write(key,v);
	}
}
```



- 编写Driver类

```java
package com.stt.demo.mr.wordCount;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

/**
 * 驱动类
 * Created by Administrator on 2019/5/5.
 */
public class WordCountDriver {

	public static void main(String[] args) throws Exception {

		// 获取相应的配置服务
		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf);
		// 设置jar加载路径
		job.setJarByClass(WordCountDriver.class);
		// 设置map和reduce类
		job.setMapperClass(WordCountMapper.class);
		job.setReducerClass(WordCountReducer.class);
		// 设置map输出
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);
		// 设置最终输出的类型
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		// 设置输入和输出路径
		FileInputFormat.setInputPaths(job,new Path(args[0]));
		FileOutputFormat.setOutputPath(job,new Path(args[1]));
		// 提交:查看源码，true表示监控job的运行情况，并打印
		boolean result = job.waitForCompletion(true);
		System.exit(result ? 0 : 1);
	}
}
```



##### 测试

- 环境准备

  如果电脑系统是win7的就将win7的hadoop jar包解压到非中文路径，并在Windows环境上配置HADOOP_HOME环境变量。如果是电脑win10操作系统，就解压win10的hadoop jar包，并配置HADOOP_HOME环境变量

  注意：win8电脑和win10家庭版操作系统可能有问题，需要重新编译源码或者更改操作系统

- 在idea上测试

  需要配置参数后运行
  ![1557069313234](img/hadoop/04.mr02.png)

- 在集群上测试

  - 使用maven进行打包，需要配置插件，注意刷新reimport

  ```xml
  <build>
      <plugins>
          <plugin>
              <artifactId>maven-compiler-plugin</artifactId>
              <version>2.3.2</version>
              <configuration>
                  <source>1.8</source>
                  <target>1.8</target>
              </configuration>
          </plugin>
          <plugin>
              <artifactId>maven-assembly-plugin </artifactId>
              <configuration>
                  <descriptorRefs>
                      <descriptorRef>jar-with-dependencies</descriptorRef>
                  </descriptorRefs>
                  <archive>
                      <manifest>
                          <!--这里需要替换为入口类-->
                          <mainClass>
                              com.stt.demo.mr.wordCount.WordCountDriver
                          </mainClass>
                      </manifest>
                  </archive>
              </configuration>
              <executions>
                  <execution>
                      <id>make-assembly</id>
                      <phase>package</phase>
                      <goals>
                          <goal>single</goal>
                      </goals>
                  </execution>
              </executions>
          </plugin>
      </plugins>
  </build>
  ```

  - 将项目打包成jar，run as -> maven install 

    等待编译完成就会在项目的target文件夹中生成jar包。如果看不到。在项目上右键 -> Refresh，即可看到。修改不带依赖的jar包名称（不含jar-with-dependencies）为wc.jar，并拷贝该jar包到Hadoop集群

  - 启动集群，执行wordCount

    - 注意读取的文件不能太多，太大，否则执行会空间不足而失败

  ```shell
  [atguigu@hadoop102 software]$ hadoop jar wc.jar
  com.stt.demo.mr.wordCount.WordCountDriver /user/ttshe/input/wc.input /user/ttshe/output
  ```



## Hadoop序列化



### 序列化概述

#### 什么是序列化

- 序列化：将内存中的对象转换为==字节序列==（或其他数据传输协议，如json），以便存储到磁盘或者传输

- 反序列化：将收到的字节序列（或者其他格式）转换为==内存中的对象==

  

#### 为什么要序列化

- 一般对象存储在内存中，一旦断电就没有了，同时该对象只能本地进程使用，不能通过网络发送给另一个进程使用

- 序列化之后，该对象可以用于传输给另一进程使用

  

#### 为什么不使用Java的序列化

- java序列化是一个重量级序列化框架（Serializable），一个对象序列化之后会附带很多额外的信息，各种校验信息，header，继承体系，不便于在网络中传输，Hadoop自己开发了一套序列化机制（Writable）

- Hadoop序列化的特点
  - 紧凑：高效的使用存储空间
  - 快速：读写数据的额外开销小
  - 可扩展：随着通信协议的升级而升级
  - 互操作：支持多语言的交互



### 自定义bean序列化

#### 如何编写

当基本类型的Writable对象不能满足需求时，需要自定义序列化操作，如在Hadoop内部传输一个bean对象

- 实现Writable接口
- 反序列化时，需要反射调用空参构造函数，必须要有一个空参构造器

```java
public FlowBean(){super();}
```

- 重写序列化方法

```java
@Override
public void write(DataOutput out) throws IOException {
	out.writeLong(upFlow);
	out.writeLong(downFlow);
	out.writeLong(sumFlow);
}
```

- 重写反序列化方法

```java
@Override
public void readFields(DataInput in) throws IOException {
	upFlow = in.readLong();
	downFlow = in.readLong();
	sumFlow = in.readLong();
}
```

- 注意：==反序列化的顺序和序列化的顺序完全一致==
- 可以重写toString方法，用于显示在日志文件中等
- 如果需要将自定义bean放在Key中传输，==需要实现 Comparable 接口==，因为MapReduce框中的==Shuffle过程要求对Key必须能排序==

```java
@Override
public int compareTo(FlowBean o) {
	// 倒序排列，从大到小
	return this.sumFlow > o.getSumFlow() ? -1 : 1;
}
```

#### 示例

- 需求：统计一个手机号耗费的总上行流量，下行流量，总流量
- 输入数据

```xml
1	13736230513	192.196.100.1	www.atguigu.com	2481	24681	200
2	13846544121	192.196.100.2			264	0	200
3 	13956435636	192.196.100.3			132	1512	200
4 	13966251146	192.168.100.1			240	0	404
5 	18271575951	192.168.100.2	www.atguigu.com	1527	2106	200
6 	84188413	192.168.100.3	www.atguigu.com	4116	1432	200
7 	13590439668	192.168.100.4			1116	954	200
8 	15910133277	192.168.100.5	www.hao123.com	3156	2936	200
9 	13729199489	192.168.100.6			240	0	200
10 	13630577991	192.168.100.7	www.shouhu.com	6960	690	200
11 	15043685818	192.168.100.8	www.baidu.com	3659	3538	200
12 	15959002129	192.168.100.9	www.atguigu.com	1938	180	500
13 	13560439638	192.168.100.10			918	4938	200
14 	13470253144	192.168.100.11			180	180	200
15 	13682846555	192.168.100.12	www.qq.com	1938	2910	200
16 	13992314666	192.168.100.13	www.gaga.com	3008	3720	200
17 	13509468723	192.168.100.14	www.qinghua.com	7335	110349	404
18 	18390173782	192.168.100.15	www.sogou.com	9531	2412	200
19 	13975057813	192.168.100.16	www.baidu.com	11058	48243	200
20 	13768778790	192.168.100.17			120	120	200
21 	13568436656	192.168.100.18	www.alibaba.com	2481	24681	200
22 	13568436656	192.168.100.19			1116	954	200
```

- 输入数据格式

```xml
7 	13560436666	120.196.100.99		1116		 954			200
id	手机号码		网络ip			上行流量     下行流量     网络状态码
```

- 期望输出

```xml
13560436666 		1116		  954 			2070
手机号码		    上行流量        下行流量		总流量
```

- 分析



- 编写



## MapReduce框架原理



## 数据压缩



## YARN资源调度



## Hadoop企业优化



## MapReduce扩展



## 问题处理