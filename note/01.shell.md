# Shell学习

> Shell 就是一个命令行**解释器**，接收应用程序和用户命令，调用操作系统内核。
> 也是功能强大的编程语言，易编写，易调试，灵活性强

![1553786470973](img\linux\7.shell1.png)



## Shell解释器

- Linux提供的shell解释器

```shell
[root@hadoop100 ~]# cat /etc/shells
/bin/sh
/bin/bash
/sbin/nologin
/bin/dash
/bin/tcsh
/bin/csh
```

- bash 和 sh 的关系

```shell
[root@hadoop100 ~]# ll /bin/sh
lrwxrwxrwx. 1 root root 4 3月  27 23:21 /bin/sh -> bash
```

sh 和 bash本质上是一个，Linux**默认是bash**，是在sh的基础上的升级，其他的sh不常用。

- CentOS 默认是bash

```shell
[root@hadoop100 bin]# echo $SHELL
/bin/bash
# 也可以通过env查看
[root@hadoop100 bin]# env | grep SHELL
SHELL=/bin/bash
```



## Shell脚本入门

> 使用 shell 编写的脚本程序，作为命令语言互动式的解释和执行用户的输入命令
> 可以用于程序设计，提供定义变量，参数的手段，丰富的程序控制
> 类似于Dos系统中的批处理文件

### 脚本格式

脚本以 **#!/bin/bash** 开头，用于指定解释器

### helloworld

创建一个脚本，输出helloworld

```shell
[root@hadoop100 sh-demo]# touch helloworld.sh
[root@hadoop100 sh-demo]# vi helloworld.sh 
[root@hadoop100 sh-demo]# cat helloworld.sh 
#!/bin/bash
echo "helloword"
[root@hadoop100 sh-demo]# sh helloworld.sh 
helloword
```

脚本常用执行方式

- 使用bash或sh+脚本的相对路径或绝对路径，不用赋予脚本+x权限
- sh+脚本的相对路径

```shell
[root@hadoop100 sh-demo]# sh helloworld.sh 
```

- 赋予脚本权限，直接执行脚本

```shell
[root@hadoop100 sh-demo]# ll
-rw-r--r--. 1 root root 29 3月  28 23:44 helloworld.sh
[root@hadoop100 sh-demo]# chmod +x helloworld.sh 
[root@hadoop100 sh-demo]# ll
-rwxr-xr-x. 1 root root 29 3月  28 23:44 helloworld.sh
[root@hadoop100 sh-demo]# ./helloworld.sh 
helloword
```



## Shell变量

### 系统变量

> 系统内置的环境变量 
> $HOME 
> $PWD 
> $SHELL 
> $USER

```shell
# 当前用户的home目录
[root@hadoop100 ~]# echo $HOME
/root
# 当前目录
[root@hadoop100 test01]# echo $PWD
/home/test01
# 当前使用的shell解释器
[root@hadoop100 test01]# echo $SHELL
/bin/bash
# 当前用户
[root@hadoop100 test01]# echo $USER
root

#显示当前shell中的所有变量
[root@hadoop100 test01]# set
BASH=/bin/bash
BASHOPTS=checkwinsize:cmdhist:expand_aliases:extquote:force_fignore:hostcomplete:interactive_comments:login_shell:progcomp:promptvars:sourcepath
BASH_ALIASES=()
BASH_ARGC=()
...
```



### 自定义变量

- 基本语法
  - 定义语法： 变量=值
  - 撤销变量：unset变量
  - 声明静态变量：readonly 变量，注意：不能unset

- 变量定义规则
  - 变量名称可以由字母，数字，下划线组成，不能使用数字开头
  - **==环境变量名大写==**
  - **==等号两侧不能有空格==**
  - 在bash中，变量的**默认类型**都是**字符串**类型，无法直接进行数值运算
  - 变量的值如果有空格，需要使用双引号或单引号括起来
    - 单引号：字符串中的所有字符进行转义(包括特殊字符)
    - 双引号：不会对字符串的内容进行转义

```shell
示例：定义变量A 
[root@hadoop100 test01]# A=5
[root@hadoop100 test01]# echo $A
5
# 如果有空格
[root@hadoop100 test01]# A = 6
-bash: A: command not found

示例：撤销变量
[root@hadoop100 test01]# echo $A
6
[root@hadoop100 test01]# unset A
[root@hadoop100 test01]# echo $A
# 此时打印为空

示例：声明只读变量 一般很少使用readonly
[root@hadoop100 test01]# readonly B=3
[root@hadoop100 test01]# echo $B
3
# 再次修改
[root@hadoop100 test01]# B=5
-bash: B: readonly variable
# 撤销变量
[root@hadoop100 test01]# unset B
-bash: unset: B: cannot unset: readonly variable

示例：关于shell中的值都是字符串，如果使用+，没有数值运算符的效果，整个C是一个字符串值
[root@hadoop100 test01]# A=5
[root@hadoop100 test01]# C=$A+3
[root@hadoop100 test01]# echo $C
5+3

示例：值含空格的变量，必须使用单引号或双引号括起来
[root@hadoop100 test01]# A="I am a variable"
[root@hadoop100 test01]# echo $A
I am a variable

示例：单双引号的区别，!在shell中有特殊含义，是特殊字符
[root@hadoop100 test01]# A="this is !"
-bash: !": event not found
# 需要使用单引号
[root@hadoop100 test01]# A='this is !'
[root@hadoop100 test01]# echo $A
this is !
```



### 特殊变量 $n

> 接收入参，表示脚本入参的占位符

基本语法：n 为数字，$0表示该脚本的名称，$1-9表示第一个到第九个参数，十个以上的参数使用大括号括起来表示${10}

```shell
示例：输出脚本的入参
[root@hadoop100 sh-demo]# touch paramter.sh
[root@hadoop100 sh-demo]# vi paramter.sh 
[root@hadoop100 sh-demo]# cat paramter.sh 
#!/bin/bash
echo $0 $1 $2
[root@hadoop100 sh-demo]# chmod 777 paramter.sh 
[root@hadoop100 sh-demo]# ./paramter.sh param1 param2
./paramter.sh param1 param2

示例：对脚本进行修改，加深对单引号和双引号的认识
[root@hadoop100 sh-demo]# cat paramter.sh 
#!/bin/bash
echo $0 $1 $2
echo '$0='$0
echo '$1='$1
echo '$2='"$2"
# 单引号里面的$输出的就是$字符串，而在双引号中的$则依然表示命令
[root@hadoop100 sh-demo]# ./paramter.sh param1 param2
./paramter.sh param1 param2
$0=./paramter.sh
$1=param1
$2=param2
```



### 特殊变量 $#

> 获取输入参数的个数，等价于args.length 那么$1等价于args[1]
> 通常用于循环

```shell
[root@hadoop100 sh-demo]# cat paramter.sh 
#!/bin/bash
echo $0 $1 $2
echo $#
[root@hadoop100 sh-demo]# ./paramter.sh param1 param2
./paramter.sh param1 param2
2
```

### 特殊变量 $* ，$@

>  $*  表示命令行入参的所有参数，将所有参数看成一个整体，类似于args.toString()，参数之间以空格连接
> $@  表示命令行入参的所有参数，等于输入参数的列表对象，等价于args的引用

```shell
[root@hadoop100 sh-demo]# cat paramter.sh 
#!/bin/bash
echo $0 $1 $2
echo $#
echo '$*='$*
echo '$@='$@
# $* 和 $@ 打印的效果是一样的
[root@hadoop100 sh-demo]# ./paramter.sh param1 param2
./paramter.sh param1 param2
2
$*=param1 param2
$@=param1 param2
```

### 特殊变量 $?

判断上一个操作是否成功，返回0表示成功，其他表示失败


## 运算符

基本语法

- $((运算式)) 或者 $[运算式]

- expr
  - `+` 加 
  - `-` 减
  - `\*` 乘
  - `/` 除
  - `%` 取余
  - 注意：expr 运算符之间要有空格
  - 使用的频率比较低

```shell
示例：
[root@hadoop100 sh-demo]# A=1
[root@hadoop100 sh-demo]# C=$[$A+2]
[root@hadoop100 sh-demo]# echo $C
3
[root@hadoop100 sh-demo]# D=$A+3
[root@hadoop100 sh-demo]# echo $D
1+3
# 使用$[]本质上将字符串解析为运算公式，计算出来
[root@hadoop100 sh-demo]# echo $[$D]
4

示例：
[root@hadoop100 sh-demo]# expr 2 + 3
5
[root@hadoop100 sh-demo]# expr 2 / 3
0
[root@hadoop100 sh-demo]# expr 2 % 3
2
[root@hadoop100 sh-demo]# expr 2 \* 3
6

示例：复杂计算，一般使用$[]
[root@hadoop100 sh-demo]# expr `expr 2 + 3` \* 4
20
[root@hadoop100 sh-demo]# S=$[(2+3)*4]
[root@hadoop100 sh-demo]# echo $S
20
```


## 条件判断

基本语法

[ condition ] 

- condition左右要有空格，每个元素之间都要有空格
- 条件非空就是true，[ test ] 返回true,[] 返回false

常用判断条件

- 2个整数之间的比较
  - = 字符串比较
  - -lt 小于 less than
  - -le 小于等于 less equal
  - -eq 等于 equal
  - -gt 大于 greater than
  - -ge 大于等于 greater equal
  - -ne 不等于 not equal
- 按照文件的权限进行判断
  - -r 有读的权限 read
  - -w 有写得权限 write
  - -x 有执行的权限 execute
- 按照文件类型进行判断
  - -f 文件存在并且是一个常规文件 file
  - -e 文件存在 existence
  - -d 文件存在且是一个目录 directory

```shell
示例：23 >= 22 ?
[root@hadoop100 sh-demo]# [ 23 -ge 22 ]
[root@hadoop100 sh-demo]# echo $?
0
[root@hadoop100 sh-demo]# [ 23 -ge 26 ]
[root@hadoop100 sh-demo]# echo $?
1
示例：
[root@hadoop100 sh-demo]# A=1
[root@hadoop100 sh-demo]# [ $A -eq 1 ]
[root@hadoop100 sh-demo]# echo $?
0
[root@hadoop100 sh-demo]# [ $A -eq 2 ]
[root@hadoop100 sh-demo]# echo $?
1
示例：文件权限判断
[root@hadoop100 sh-demo]# [ -x paramter.sh ]
[root@hadoop100 sh-demo]# echo $?
0
示例：判断文件类型
[root@hadoop100 sh-demo]# [ -d /home/sh-demo/paramter.sh ]
[root@hadoop100 sh-demo]# echo $?
1
```


## 流程控制

### if 判断

基本语法

- [ 条件表达式 ]，括号和条件表达式之间必须要有空格
- if后要有空格

```shell
# 写法1
if [ 条件表达式 ];then
	...
elif [ 条件表达式2 ]；then
	...
else 
	...
fi
# 写法2
if [ 条件表达式 ]
	then 
		...
elif [ 条件表达式2 ]
	then
		...
else 
	...
fi
```

示例

 ```shell
[root@hadoop100 sh-demo]# chmod 777 if.sh 
[root@hadoop100 sh-demo]# cat if.sh 
#!/bin/bash
if [ $1 -eq 1 ];then
	echo success
elif [ $1 -eq 2 ];then
	echo success2
else
	echo fail
fi
[root@hadoop100 sh-demo]# ./if.sh 1
success
[root@hadoop100 sh-demo]# ./if.sh 2
success2
[root@hadoop100 sh-demo]# ./if.sh 3
fail
 ```

###  case 语句

基本语法

- case 尾行必须in单词结尾，每一个模式匹配必须使用 ） 结束
- 双分号 ;; 表示命令序列结束，相当于java中的break
- 最后的 *） 表示默认模式，相当于java中的default

```shell
case $变量名 in
	"值1")
		...
		;;
	"值2")
		...
		;;
	*)
		#如果变量都不满足
		;;
esac
```

示例

```shell
[root@hadoop100 sh-demo]# cat case.sh 
#!/bin/bash
case $1 in
	"1")
		echo success1
		;;
	"2")	
		echo success2
		;;	
	*)
		echo default
		;;
esac
[root@hadoop100 sh-demo]# chmod 777 case.sh 
[root@hadoop100 sh-demo]# ./case.sh 1
success1
[root@hadoop100 sh-demo]# ./case.sh 2
success2
[root@hadoop100 sh-demo]# ./case.sh 3
default
```

### for 循环

基本语法

```shell
for ((初始值；循环控制条件；变量变化))
	do
		...
done
#写法2,使用；可以避免换行，将do放在for一行上
for ((初始值；循环控制条件；变量变化));do
		...
done
#用法2：值使用空格连接 那么$*也可以遍历
for 变量 in 值1 值2 值3 ...
	do
		...
done
```

示例

```shell
[root@hadoop100 sh-demo]# cat for.sh 
#!/bin/bash
for ((i=1;i<5;i++));do
	echo $i
done
[root@hadoop100 sh-demo]# ./for.sh 
1
2
3
4
# 使用用法2的方式
[root@hadoop100 sh-demo]# cat for2.sh 
#!/bin/bash
for i in $*;do
	echo $i
done
echo ======
for i in $@;do
	echo $i
done
[root@hadoop100 sh-demo]# ./for2.sh 1 2 3 4 5
1
2
3
4
5
======
1
2
3
4
5
# 注意 $* 和 $@ 的区别
[root@hadoop100 sh-demo]# cat for2.sh 
#!/bin/bash
for i in "$*";do
	echo $i
done
echo ======
for i in "$@";do
	echo $i
done
[root@hadoop100 sh-demo]# ./for2.sh 1 2 3 4 5
1 2 3 4 5
======
1
2
3
4
5
```

### while  循环

基本语法

```shell
while [ 条件表达式 ]
do
	...
done
```

示例

```shell
[root@hadoop100 sh-demo]# cat while.sh 
#!/bin/bash
i=1
sum=0
while [ $i -lt 101 ]
do
	sum=$[$i+$sum]
	i=$[$i+1]
done
echo 'sum='$sum
[root@hadoop100 sh-demo]# ./while.sh 
sum=5050
```

## read读取控制台输入

> 用于用户和控制台进行交互使用，类似于java中的System.in.read()

基本语法

```shell
read(选项)(参数)
选项：
	-p 	指定读取值时的提示符
	-t 	指定读取值时等待的时间，单位s
参数：
	变量名  指定读取值的变量名
```

示例

```shell
[root@hadoop100 sh-demo]# cat read.sh 
#!/bin/bash
read -t 5 -p "请输入任意文本" wenben
echo $wenben

[root@hadoop100 sh-demo]# ./read.sh
请输入任意文本test
test
```



## 函数

### 系统函数

#### basename

基本语法

- basename命令会删除所有前缀包括==最后一个 /== 字符，然后将字符串显示出来
- suffix如果被指定了，basename会将pathname或string中的suffix去除

```shell
basename [string/pathname] [suffix]
```

示例：截取文件名称

```shell
[root@hadoop100 sh-demo]# basename /home/sh-demo/paramter.sh .sh
paramter
[root@hadoop100 sh-demo]# basename /home/sh-demo/paramter.sh
paramter.sh
```

#### dirname

基本语法

- 从给定的包含绝对路径的文件名称中去除文件名，返回剩下的路径

```shell
dirname [文件的绝对路径]
```

示例

```shell
[root@hadoop100 sh-demo]# dirname /home/sh-demo/paramter.sh 
/home/sh-demo
```



### 自定义函数

> 解释型语言，先声明后使用
> 函数返回值，只能通过$?系统变量获取，可以通过return返回，如果没有return，则通过最后一条命令返回运行结果作为返回值，return 后面添加数值n(0-255)

```shell
[function] func_name[()]
{
    ...
    [return int;]
}
# 调用 
func_name
```

示例：注意入参使用$调用

```shell
[root@hadoop100 sh-demo]# cat fun.sh 
#!/bin/bash
function add()
{
	return $[$1+$2]
}
add 1 2
echo $?

[root@hadoop100 sh-demo]# ./fun.sh 
3
```



## Shell工具

### cut

> 在文本中负责剪切数据，从文件的每一行剪切字节，字符，字段，并输出

基本语法

- 默认分隔符是制表符 tab
- 选项参数说明
  - -f 列号，提取第几列
  - -d 分隔符，按照指定分割符分割列

```shell
cut [选项参数] filename
```

示例：按照空格split，然后取得第1列和第11列

```shell
[root@hadoop100 sh-demo]# ifconfig | grep eth | cut -d " " -f 1,11
eth0 00:0C:29:D1:82:07
```



### sed

> 一种流式编辑器，一次处理一行内容，处理时把当前处理的行存储在临时缓冲区中（模式空间），然后用sed命令处理缓冲区的内容，处理完成后将缓冲区的内容推送屏幕，接着处理下一行，不断重复，直到文件结尾，**文件内容没有改变**，除非使用重定向存储输出

基本用法

```shell
sed [选项] 命令 filename
选项：
-e	直接在指令模式上进行sed的动作编辑
命令：
a 	新增，a的后面可以接字串，在下一行出现
d 	删除
s	查找并替换
```

示例

```shell
# 数据准备
[root@hadoop100 sh-demo]# cat sed.txt 
dong shen
guan zhen
wo wo
lai lai

le le
# 将 mei nv 插入到第二行下
[root@hadoop100 sh-demo]# sed '2a mei nv' sed.txt 
dong shen
guan zhen
mei nv
wo wo
lai lai

le le
# 在dong行下插入123
[root@hadoop100 sh-demo]# sed '/dong/a 123' sed.txt 
dong shen
123
guan zhen
wo wo
lai lai

le le
# 注意文件并没有改变
[root@hadoop100 sh-demo]# cat sed.txt 
dong shen
guan zhen
wo wo
lai lai

le le
# 删除sed.txt文件所包含的所有wo的行
[root@hadoop100 sh-demo]# sed '/wo/d' sed.txt
dong shen
guan zhen
lai lai

le le
# 将wo替换为ni
[root@hadoop100 sh-demo]# sed 's/wo/ni/g' sed.txt
dong shen
guan zhen
ni ni
lai lai

le le
# g表示全部
# 将sed.txt 文件中的第二行删除，并将wo替换为ni
[root@hadoop100 sh-demo]# sed -e '2d' -e 's/wo/ni/g' sed.txt
dong shen
ni ni
lai lai

le le
```



### awk

> 文本分析工具，把文件逐行读取，用空格作为默认分隔符进行切片，对每个切片进行分析处理

基本用法

- 只有匹配了pattern，才会执行action

```shell
awk [选项参数] ‘pattern1{action1} pattern2{action2} ...’ filename
说明：
	pattern		表示AWK在数据中查找的内容，匹配模式，正则表达式/.../
	action		找到匹配内容时进行的命令
		BEGIN	在所有数据读取之前执行
		END		在所有数据执行之后执行
选项：
	-F			指定输入文件拆分的分隔符
	-v			赋值一个自定义变量
内置变量：
	FILENAME	文件名
	NR			已读的记录数
	NF			浏览记录的域的个数，切割后，列的个数
```

 示例

```shell
# 1.数据准备 拷贝passwd文件到当前目录
[root@hadoop100 sh-demo]# sudo cp /etc/passwd ./
[root@hadoop100 sh-demo]# cat passwd 
root:x:0:0:root:/root:/bin/bash
bin:x:1:1:bin:/bin:/sbin/nologin
daemon:x:2:2:daemon:/sbin:/sbin/nologin
adm:x:3:4:adm:/var/adm:/sbin/nologin
lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
...
# 打印第1列和第7列
[root@hadoop100 sh-demo]# awk -F: '{print $1,$7}' passwd 
root /bin/bash
bin /sbin/nologin
daemon /sbin/nologin
...
# 搜索passwd 文件以root关键字开头的所有行，并输出该行的第7列
[root@hadoop100 sh-demo]# awk -F: '/^root/{print $7}' passwd 
/bin/bash

# 多个模式下的搜索 如果2个模式都满足，那么都输出
[root@hadoop100 sh-demo]# awk -F: '/^root/{print $1} /^a/{print $1}' passwd 
root
adm
avahi-autoipd
abrt
apache

# 搜索passwd 文件以root关机字开头的所有航，输出第1列和第7列，中间以逗号分隔
[root@hadoop100 sh-demo]# awk -F: '/^root/{print $1","$7}' passwd 
root,/bin/bash

# 在输出的数据上添加开始和结束信息
[root@hadoop100 sh-demo]# awk -F: 'BEGIN{print "....start..."} /^root/{print $1","$7} END{print "...end..."}' passwd 
....start...
root,/bin/bash
...end...

# 将passwd文件中用户id 增加数值1并输出
[root@hadoop100 sh-demo]#  awk -v i=1 -F : '{print $3+i}' passwd 
1
2
3
4
# 对用户id进行累加输出
[root@hadoop100 sh-demo]#  awk -F : 'BEGIN{sum=0} {print $3;sum+=$3} END{print "sum="sum}' passwd 
0
1
2
3
...
501
sum=69322
```

### sort

> 将文件进行排序，将排序结果进行标准输出

基本语法

```shell
sort(选项)(参数)
选项：
	-n 	依照数值的大小排序
	-r	以相反的顺序排序，默认从小到大排列
	-t	设置排序时的分隔符
	-k	指定需要排序的列
参数：
	指定代排序的文件列表
```

示例

```shell
# 数据准备
[root@hadoop100 sh-demo]# cat sort.txt 
bb:40:5.4
dd:20:4.2
xz:50:2.3
cls:10:3.5
ss:30:1.6
# 按照:分隔的第三列倒叙排列
[root@hadoop100 sh-demo]# sort -t : -nrk 3 sort.txt 
bb:40:5.4
dd:20:4.2
cls:10:3.5
xz:50:2.3
ss:30:1.6
# 写法灵活
[root@hadoop100 sh-demo]# sort -nrt : -k 3 sort.txt 
```

